---
title: "Causal Inference Final Project Code"
author: "Katherine Rose Wolf"
date: "March 27, 2020"
output: html_document
---

```{r setup load libraries}

library(extrafont)

# # loads fonts 
font_import(pattern = "Raleway")

library(praise)
library(tidyverse)
library(tableone)
library(kableExtra)
library(furniture)
library(RColorBrewer)
library(psych)
library(SuperLearner)
library(ltmle)
library(arm)

# # loads fonts 
# font_import()

# stop r from abbreviating doubles in school ids
options("scipen" = 40)

praise()  # self-esteem boost

```


```{r load datasets}

load(file = "dataset_full.rdata")

load(file = "dataset_complete_cases.rdata")

load(file = "dataset_complete_cases_forget_funding.rdata") # no per cap variable so N goes to ~9000

```


# BELOW LIES KATIE'S PLAYGROUND

```{r KATIE code pre-SuperLearner}

dataset_complete_cases_working <- 
  dataset_complete_cases %>% 
  mutate(math_proportion = math_standard_met/100,
         pm25_12_plus = ifelse(pm25_12_plus == "yes", 1, 0), 
         air_basin = factor(air_basin))

# histogram of math percentage (weird, lots of zeroes)
hist(dataset_complete_cases_working$math_standard_met, 
     breaks = 20)

# simple mean: areas with pm above 12 lose 2.03 percentage points of passage
mean(dataset_complete_cases_working$math_proportion[
  dataset_complete_cases_working$pm25_12_plus == 1
  ] - 
    dataset_complete_cases_working$math_proportion[
      dataset_complete_cases_working$pm25_12_plus == 0])

# this code just fits the exposure linearly by the outcome
plain_glm_one_variable <- glm(math_standard_met ~ pm25_12_plus, 
                              data = dataset_complete_cases_working)
summary(plain_glm_one_variable)

# full linear glm no interactions
plain_glm <- glm(math_standard_met ~ 
                   pm25_12_plus + 
                   air_basin + 
                   afr_am_percent + 
                   asian_filipino_percent + 
                   two_none_aian_pi_percent + 
                   hisp_latinx_percent + 
                   english_learner_percent + 
                   frpm_percent + 
                   district_cost_per_pupil + 
                   student_teacher_ratio + 
                   school_k_12_enrollment +
                   cred_percent, 
                 data = dataset_complete_cases_working)

summary(plain_glm)

# full linear glm drop weirds
plain_glm_drop_per_pupil <- glm(math_standard_met ~ 
                   pm25_12_plus + 
                   air_basin + 
                   afr_am_percent + 
                   asian_filipino_percent + 
                   two_none_aian_pi_percent + 
                   hisp_latinx_percent + 
                   english_learner_percent + 
                   frpm_percent + 
                   cred_percent +
                   school_k_12_enrollment, 
                 data = dataset_complete_cases_working)

summary(plain_glm_drop_per_pupil)

summary(dataset_complete_cases_working$student_teacher_ratio)

# make working dataset without funding data
dataset_complete_cases_forget_funding_working <- 
  dataset_complete_cases_forget_funding

# simple glm without funding data
glm_forget_funding <- 
  glm(math_standard_met ~ 
                   pm25_12_plus + 
                   air_basin + 
                   afr_am_percent + 
                   asian_filipino_percent + 
                   two_none_aian_pi_percent + 
                   hisp_latinx_percent + 
                   english_learner_percent + 
                   frpm_percent + 
                   student_teacher_ratio + 
                   cred_percent + 
                   school_k_12_enrollment, 
                 data = dataset_complete_cases_forget_funding_working)

summary(glm_forget_funding)


```


```{r plots for slides}

dataset_complete_cases_working <- 
  dataset_complete_cases %>% 
  mutate(math_proportion = math_standard_met/100,
         pm25_12_plus = factor(pm25_12_plus), 
         air_district = factor(air_district), 
         air_basin = factor(air_basin))

simulated_full_only_complete_cases <- 
  dataset_full %>% 
  filter(!is.na(math_standard_met)) %>% 
  filter(!is.na(english_learner_percent)) %>% 
  filter(!is.na(student_teacher_ratio)) %>% 
  filter(!is.na(air_basin)) %>% 
  filter(!is.na(district_cost_per_pupil))

# # count observations by air basin
# dataset_complete_cases_working %>% 
#   group_by(air_basin) %>% 
#   summarize(count = n()) %>% 
#   View()

#------------------------------------------
# Exposure-Outcome
#------------------------------------------

# only if actually plotting
windowsFonts(Times=windowsFont("Raleway"))

# make basic scatterplot with loess line through it
ggplot(data = simulated_full_only_complete_cases,
       aes(x = modeled_air_at_school,
           y = math_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth(method=loess,
              se=TRUE,
              color="darkred") +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("Mean Percentage of Students Who Met or Exceeded \nMath Standard in 2018-19") +
  theme(text=element_text(family="Raleway"))

# make basic scatterplot with plain line through it
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = modeled_air_at_school, 
           y = math_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_abline() +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("Mean Percentage of Students Who Met or Exceeded \nMath Standard in 2018-19") +
  ggtitle("Complete Cases Only")

# make basic scatterplot with loess line through it
ggplot(data = dataset_full,
       aes(x = modeled_air_at_school,
           y = math_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth(method=loess,
              se=TRUE,
              color="darkred") +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("Mean Percentage of Students Who Met or Exceeded \nMath Standard in 2018-19") +
  ggtitle("Full Data")

# make basic scatterplot with plain line through it
ggplot(data = dataset_full, 
       aes(x = modeled_air_at_school, 
           y = math_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_abline() +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("Mean Percentage of Students Who Met or Exceeded \nMath Standard in 2018-19") +
  ggtitle("Full Data")

# box plot PM2.5 versus math score
ggplot(data = dataset_complete_cases_working, 
       aes(x = pm25_12_plus, 
           y = math_standard_met)) +
  geom_boxplot() +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("Mean Percentage of Students Who Met or Exceeded \nMath Standard in 2018-19") +
  ggtitle("Complete Cases Only")

colnames(dataset_complete_cases)

#------------------------------------------
# Enrollment
#------------------------------------------

# scatterplot enrollment and pm2.5
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = modeled_air_at_school, 
           y = school_k_12_enrollment)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth() +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("School Enrollment 2018-19") +
  ggtitle("Complete Cases Only")

# scatterplot enrollment and test scores
ggplot(data = dataset_complete_cases, 
       aes(x = school_k_12_enrollment, 
           y = math_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth() +
  xlab(expression(paste("School Enrollment 2018-19"))) +
  ylab("Mean Percentage of Students Who Met or Exceeded \nMath Standard in 2018-19") +
  ggtitle("Complete Cases Only")

# box plot PM2.5 versus enrollment
ggplot(data = dataset_complete_cases_working, 
       aes(x = pm25_12_plus, 
           y = school_k_12_enrollment)) +
  geom_boxplot() +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("School Enrollment 2018-19") +
  ggtitle("Complete Cases Only")

#------------------------------------------
# FRPM
#------------------------------------------

# scatterplot frpm air pollution
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = frpm_percent, 
           y = modeled_air_at_school)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth() +
  xlab("Students with Free/Reduced-Price Lunches (%)") +
  ylab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ggtitle("Complete Cases Only")

# boxplot frpm pm
ggplot(data = dataset_complete_cases_working, 
       aes(x = pm25_12_plus, 
           y = frpm_percent)) +
  geom_boxplot() +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("Students with Free/Reduced-Price Lunches (%)") +
  ggtitle("Complete Cases Only")

# scatterplot frpm math standard met
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = frpm_percent, 
           y = math_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth() +
  xlab("Students with Free/Reduced-Price Lunches (%)") +
  ylab("Math Standard Met") +
  ggtitle("Complete Cases Only")

#------------------------------------------
# Per-pupil funding
#------------------------------------------

# scatterplot cost per pupil air pollution
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = district_cost_per_pupil, 
           y = modeled_air_at_school)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth() +
  xlab("District Cost Per Pupil ($)") +
  ylab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ggtitle("Complete Cases Only")

# boxplot cost per pupil pm
ggplot(data = dataset_complete_cases_working, 
       aes(x = pm25_12_plus, 
           y = district_cost_per_pupil)) +
  geom_boxplot() +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("District Cost Per Pupil ($)") +
  ggtitle("Complete Cases Only") +
  ylim(0, 150000)

# scatterplot district cost per pupil math standard met
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = district_cost_per_pupil, 
           y = math_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth() +
  xlab("District Cost Per Pupil ($)") +
  ylab("Math Standard Met") +
  ggtitle("Complete Cases Only") +
  xlim(0, 40000) +
  ylim(0, 100)

#------------------------------------------
# Student-teacher ratio
#------------------------------------------

# scatterplot student-teacher ratio air pollution
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = student_teacher_ratio, 
           y = modeled_air_at_school)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth() +
  xlab("Student-teacher ratio") +
  ylab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ggtitle("Complete Cases Only")

# boxplot student-teacher ratio pm
ggplot(data = dataset_complete_cases_working, 
       aes(x = pm25_12_plus, 
           y = student_teacher_ratio)) +
  geom_boxplot() +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("Student-teacher ratio") +
  ggtitle("Complete Cases Only")

# scatterplot district student-teacher ratio math standard met
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = student_teacher_ratio, 
           y = math_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth(method = "loess") +
  xlab("Student-teacher ratio") +
  ylab("Math standard met (%)") +
  ggtitle("Complete Cases Only")

#------------------------------------------
# Fully-credentialed teachers
#------------------------------------------

# scatterplot credentials / air pollution
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = cred_percent, 
           y = modeled_air_at_school)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth() +
  xlab("Fully credentialed teachers (%)") +
  ylab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ggtitle("Complete Cases Only")

# boxplot credentials / pm
ggplot(data = dataset_complete_cases_working, 
       aes(x = pm25_12_plus, 
           y = cred_percent)) +
  geom_boxplot() +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("Fully credentialed teachers (%)") +
  ggtitle("Complete Cases Only")

# scatterplot credentials / math standard met
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = cred_percent, 
           y = math_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth(method = "loess") +
  xlab("Fully credentialed teachers (%)") +
  ylab("Math standard met (%)") +
  ggtitle("Complete Cases Only")

#------------------------------------------
# Air basin
#------------------------------------------

#------------------------------------------
# Percent white
#------------------------------------------

#------------------------------------------
# Percent Hispanic/Latinx
#------------------------------------------

#------------------------------------------
# Percent Asian/Filipino
#------------------------------------------

#------------------------------------------
# Percent African American
#------------------------------------------

#------------------------------------------
# Percent other
#------------------------------------------

# save plot as pdf
ggsave(device = "bmp", 
       filename = file.path("output_for_writeups", 
                            "scatterplot.bmp"), 
       height = 5, 
       width = 5)

```


```{r KATIE tabling the ancestors}

# load data
load(file = file.path("dataset_full.rdata"))

simulated_full_only_complete_cases <- 
  dataset_full %>% 
  filter(!is.na(math_standard_met)) %>% 
  filter(!is.na(english_learner_percent)) %>% 
  filter(!is.na(student_teacher_ratio)) %>% 
  filter(!is.na(air_basin)) %>% 
  filter(!is.na(district_cost_per_pupil))

data_for_table_one <- 
  simulated_full_only_complete_cases

colnames(data_for_table_one)

# relabel variables
data_for_little_table_one <- 
  data_for_table_one %>% 
  mutate("PM25" = factor(pm25_12_plus), 
         "School enrollment (n)" = school_k_12_enrollment, 
         "English learners (%)" = round(english_learner_percent, 1),
         "Free/reduced-price meal eligibility (%)" = frpm_percent, 
         "Student-teacher ratio" = student_teacher_ratio,
         "Fully credentialed teachers" = cred_percent,
         "Cost per pupil" = district_cost_per_pupil,
         "Air basin group" = air_basin_binned, 
         "Air basin" = air_basin,
         "asian_filipino_percent" = round(asian_filipino_percent, 1),
         "Asian (%)" = round(asian_percent, 1),
         "Filipino (%)" = round(filipino_percent, 1),
         "Hispanic/Latino (any race) (%)" = round(hisp_latinx_percent, 1),
         "White (%)" = round(white_percent, 1),
         "two_none_aian_pi_af_am_percent" = round(two_none_aian_pi_af_am_percent, 1),
         "African American (%)" = round(afr_am_percent, 1),
         "American Indian or Alaska Native (%)" = round(aian_percent, 1),
         "Pacific Islander (%)" = round(pacific_islander_percent, 1),
         "Two or more races (not Hispanic/Latino) (%)" = round(two_plus_percent, 1),
         "Race/ethnicity not reported (%)" = round(not_reported_percent, 1),
         "Math standard met or exceeded (%)" = math_standard_met) %>% 
  dplyr::select(`PM25`, 
         `School enrollment (n)`, 
         `English learners (%)`,
         `Free/reduced-price meal eligibility (%)`, 
         `Student-teacher ratio`,
         `Fully credentialed teachers`,
         `Cost per pupil`,
         `Air basin group`, 
         `Air basin`,
         `asian_filipino_percent`,
         `Asian (%)`,
         `Filipino (%)`,
         `Hispanic/Latino (any race) (%)`,
         `White (%)`,
         `two_none_aian_pi_af_am_percent`,
         `African American (%)`,
         `American Indian or Alaska Native (%)`,
         `Pacific Islander (%)`,
         `Two or more races (not Hispanic/Latino) (%)`,
         `Race/ethnicity not reported (%)`,
         `Math standard met or exceeded (%)`) %>% 
  as.data.frame()



# create a list of variables for the table
# (not including the stratification variable)
little_table_one_variables <- 
  c(     "PM25", 
         "School enrollment (n)", 
         "English learners (%)",
         "Free/reduced-price meal eligibility (%)", 
         "Student-teacher ratio",
         "Fully credentialed teachers",
         "Cost per pupil",
         "Air basin group", 
         "Air basin",
         "asian_filipino_percent",
         "Asian (%)",
         "Filipino (%)",
         "Hispanic/Latino (any race) (%)",
         "White (%)",
         "two_none_aian_pi_af_am_percent",
         "African American (%)",
         "American Indian or Alaska Native (%)",
         "Pacific Islander (%)",
         "Two or more races (not Hispanic/Latino) (%)",
         "Race/ethnicity not reported (%)",
         "Math standard met or exceeded (%)")

table_one <- CreateTableOne(vars = little_table_one_variables,
                            # factorVars = factor_variables,
                            strata = "PM25",
                            data = data_for_little_table_one,
                            test = FALSE,
                            includeNA = TRUE)

save(table_one, 
     file = "table_one.rdata")

# Creates a formatted table, using kable from the knitr package
# Would want to clean this up for publication purposes:
hi <- kable(print(table_one,
                  showAllLevels = TRUE,
                  printToggle = FALSE,
                  noSpaces = TRUE,
                  catDigits = 1,
                  contDigits = 3),
            col.names = c("", "Mean PM2.5 <= 12 ug/m3", "Mean PM2.5 > 12 ug/m3"),
            caption=paste(".  Descriptive statistics for California public schools in 2018-19, stratified by 2018 mean PM2.5 concentration.")) %>% 
  kable_styling("striped")

hi

table_total <- CreateTableOne(vars = little_table_one_variables,
                            # factorVars = factor_variables,
                            data = data_for_little_table_one,
                            test = FALSE,
                            includeNA = TRUE)

save(table_total, 
     file = "table_one.rdata")

hi2 <- kable(print(table_total,
                  showAllLevels = TRUE,
                  printToggle = FALSE,
                  noSpaces = TRUE,
                  catDigits = 1,
                  contDigits = 3),
            col.names = c("", "Total"),
            caption=paste(".  Descriptive statistics for California public schools in 2018-19, stratified by 2018 mean PM2.5 concentration.")) %>% 
  kable_styling("striped")

hi2

```



```{r KATIE attempt at simple substitution}

# set up parallelization (my computer has 8 cores and this uses 7 of them)
# (cluster = parallel::makeCluster(7))
# parallel::clusterEvalQ(cluster, library(SuperLearner))

dataset_complete_cases_working <- 
  dataset_complete_cases %>% 
  mutate(math_proportion = math_standard_met/100,
         pm25_12_plus = ifelse(pm25_12_plus == "yes", 1, 0), 
         air_district = factor(air_district), 
         air_basin_binned = factor(air_basin_binned))

ObsData <- 
  dataset_complete_cases_working %>% 
  mutate(id_cds = as.numeric(as.factor(cds_code))) %>% 
  dplyr::select(Y = math_proportion,
                A = pm25_12_plus, 
                air_basin_binned, 
                afr_am_percent, 
                asian_filipino_percent, 
                two_none_aian_pi_percent, 
                hisp_latinx_percent, 
                english_learner_percent, 
                frpm_percent, 
                district_cost_per_pupil, 
                student_teacher_ratio, 
                cred_percent,
                school_k_12_enrollment)

ObsData <- as.data.frame(ObsData)

set.seed(252)

# reduce runs of biglasso from 100 to 5 to save time
new.algorithm.1 = create.Learner("SL.biglasso", 
                                 params = list(nlambda = 5))

new.algorithm.2 = create.Learner("SL.randomForest", 
                                params = list(ntrees = 10))

# specify SL library
listWrappers()
katie.SL.library <- c("SL.mean", 
                      "SL.glm", 
                      "SL.glm.interaction", 
                      "SL.bayesglm" 
                      # "SL.biglasso"
                      # "SL.extraTrees"
                      # "SL.glmnet"
                      # new.algorithm.2$names
                      # "SL.rpartPrune" # this one worked
                      )

#------------------------------------------
# 2. Estimate Qbar_0(A,W) with Super Learner
#------------------------------------------
# dataframe X with baseline covariates and exposure
X<-subset(ObsData, 
          select=c(A, 
                   air_basin_binned, 
                   afr_am_percent, 
                   asian_filipino_percent, 
                   two_none_aian_pi_percent, 
                   hisp_latinx_percent, 
                   english_learner_percent, 
                   frpm_percent, 
                   district_cost_per_pupil, 
                   student_teacher_ratio, 
                   cred_percent,
                   school_k_12_enrollment))

# set the exposure=1 in X1 and the exposure=0 in X0
X1 <- X0 <- X
X1$A <- 1 # under exposure
X0$A <- 0 # under control

# call Super Learner
QbarSL <- SuperLearner(Y = ObsData$Y, 
                           X = X, 
                           SL.library = katie.SL.library,
                           # cluster = cluster, 
                           family = "quasibinomial")
QbarSL

summary(QbarSL$SL.predict)
length(QbarSL$SL.predict)

# get the expected PM2.5 , given the observed exposure and covariates
QbarAW <- predict(QbarSL, newdata=ObsData)$pred
# expected injury severity, given A=1 and covariates
Qbar1W <- predict(QbarSL, newdata=X1)$pred
# expected injury severity, given A=0 and covariates
Qbar0W <- predict(QbarSL, newdata=X0)$pred
# the fitted value at the observed exposure should equal the fitted value
# under when A=a
tail(data.frame(A=ObsData$A, QbarAW, Qbar1W, Qbar0W))

# note the simple substitution estimator would be
PsiHat.SS <- mean(Qbar1W - Qbar0W)
PsiHat.SS

#------------------------------------------
# Playing with LTMLE
#------------------------------------------

# run with ltmle for comparison
ltmle_data <- 
  dataset_complete_cases_working %>% 
  mutate(id_cds = as.numeric(as.factor(cds_code))) %>% 
  dplyr::select(air_basin_binned, 
                afr_am_percent, 
                asian_filipino_percent, 
                two_none_aian_pi_percent, 
                hisp_latinx_percent, 
                english_learner_percent, 
                frpm_percent, 
                district_cost_per_pupil, 
                student_teacher_ratio, 
                cred_percent,
                school_k_12_enrollment, 
                Y = math_proportion,
                A = pm25_12_plus)

ltmle_data <- as.data.frame(ltmle_data)

ltmle_result <- ltmle(data = ltmle_data, 
                Anodes = 'A', 
                Ynodes = 'Y', 
                abar = list(1, 0),
                SL.library = katie.SL.library)

summary(ltmle_result) # Stephen: OMG!!!!! good to see TMLE does the weird "basically equal means but stat sig diff" thing for both of us 

ltmle_result$fit

#################
# EVALUATING POSITIVITY
#######################

# call Super Learner for the exposure mechanism
gHatSL <- SuperLearner(Y=ObsData$A, 
                       X=subset(ObsData, 
                                select= -c(Y, A)),
                       SL.library = katie.SL.library, 
                       family="quasibinomial")

# generate predicted prob being exposed, given baseline covariates
gHat1W <- gHatSL$SL.predict

# predicted prob of not being exposed, given baseline covariates
gHat0W <- 1- gHat1W

# # predicted prob of observed exposure, given baseline cov
gHatAW <- rep(NA, n = nrow(ObsData))
gHatAW[ObsData$A==1] <- gHat1W[ObsData$A==1]
gHatAW[ObsData$A==0] <- gHat0W[ObsData$A==0]

# this gives a little summary of the distribution
summary(gHatAW)

# combine gHat1W with original data
data_with_probability_of_exposure <- 
  dataset_complete_cases_working %>% 
  bind_cols(tibble(gHat1W))

# probability of exposure by African American
data_with_probability_of_exposure %>% 
  ggplot(aes(x = afr_am_percent, y = gHat1W)) +
  geom_point()

# probabilty of exposure by region
data_with_probability_of_exposure %>% 
  ggplot(aes(x = air_basin_binned, y = gHat1W)) +
  geom_boxplot()

# african american proportion by bin

# calculate weights
wt <- 1/gHatAW

summary(wt)

#######################################
## plotting g in propensity score chart
#######################################


## distribution of propensity scores
p1 <- data.frame(gHat1W[ObsData$A==1])
colnames(p1) <- "ps1"
p1$type <- 1
p0 <- data.frame(gHat1W[ObsData$A==0]) 
colnames(p0) <- "ps1"
p0$type <- 0 

plot_df <- data.frame(rbind(p1,p0))
ggplot(data=plot_df) + geom_histogram(aes(ps1), bins=30, stat = "bin") + facet_wrap(~type) + coord_flip() + theme_bw()




```



```{r KATIE attempt at full code set from lab 6}

# run.tmle <- function(ObsData, SL.library, id = NULL){
# 
# #------------------------------------------
# # Estimate the conditional mean outcome Qbar(A,W)
# #------------------------------------------
# 
# # dataframe X with baseline covariates and exposure
# X <- subset(ObsData, 
#             select = c(pm25_12_plus, 
#                        air_basin, 
#                        afr_am_percent, 
#                        asian_filipino_percent, 
#                        two_none_aian_pi_percent, 
#                        hisp_latinx_percent, 
#                        english_learner_percent, 
#                        frpm_percent, 
#                        district_cost_per_pupil, 
#                        student_teacher_ratio))
# 
# # set the A=1 in X1 and the A=0 in X0
# X1 <- X0 <- X
# X1$pm25_12_plus <- 1 # under exposure
# X0$pm25_12_plus <- 0 # under control
# 
# # call Super Learner for estimation of QbarAW
# QbarSL<- SuperLearner(Y = ObsData$math_standard_met, 
#                       X = X, 
#                       SL.library = SL.library, 
#                       family = "gaussian", 
#                       id = id)
# # QbarSL
# 
# # initial estimates of the outcome, given the observed exposure & covariates
# QbarAW <- predict(QbarSL, newdata=ObsData)$pred
# # estimates of the outcome, given A=1 and covariates
# Qbar1W<- predict(QbarSL, newdata=X1)$pred
# # estimates of the outcome, given A=0 and covariates
# Qbar0W<- predict(QbarSL, newdata=X0)$pred
# 
# # simple substitution estimator:
# PsiHat.SS<-mean(Qbar1W - Qbar0W)
# 
# #------------------------------------------
# # Estimate the exposure mechanism g(pm25_12_plus|W)
# #------------------------------------------
# 
# # call Super Learner for the exposure mechanism
# gHatSL<- SuperLearner(Y = ObsData$pm25_12_plus, 
#                       X = subset(ObsData, select= -c(pm25_12_plus,
#                                                      math_standard_met,
#                                                      id)),
# SL.library = SL.library, family="binomial", id=id)
# # generate predicted prob being exposed, given baseline covariates
# gHat1W <- gHatSL$SL.predict
# # predicted prob of not being exposed, given baseline covariates
# gHat0W <- 1- gHat1W
# 
# # # predicted prob of observed exposure, given baseline cov
# # gHatAW<- rep(NA, n)
# # gHatAW[ObsData$A==1]<- gHat1W[ObsData$A==1]
# # gHatAW[ObsData$A==0]<- gHat0W[ObsData$A==0]
# 
# #-------------------------------------------------
# # Clever covariate H(A,W) for each subject
# #-------------------------------------------------
# H.AW<- as.numeric(ObsData$pm25_12_plus==1)/gHat1W - as.numeric(ObsData$pm25_12_plus==0)/gHat0W
# 
# # also want to evaluate the clever covariates at A=1 and A=0 for all subjects
# H.1W<- 1/gHat1W
# H.0W<- -1/gHat0W
# 
# 
# #IPTW estimator of the G-computation formula:
# PsiHat.IPTW <-mean( H.AW*ObsData$Y)
# 
# #------------------------------------------
# # Update the initial estimator of Qbar_0(A,W)
# #------------------------------------------
# logitUpdate<- glm(ObsData$Y ~ -1 +offset(qlogis(QbarAW)) + H.AW, family='binomial')
# epsilon <- logitUpdate$coef
# 
# QbarAW.star<- plogis(qlogis(QbarAW)+ epsilon*H.AW)
# Qbar1W.star<- plogis(qlogis(Qbar1W)+ epsilon*H.1W)
# Qbar0W.star<- plogis(qlogis(Qbar0W)+ epsilon*H.0W)
# 
# #------------------------------------------
# # Estimate Psi(P_0)
# #------------------------------------------
# 
# PsiHat.TMLE <- mean(Qbar1W.star - Qbar0W.star)
# 
# #------------------------------------------
# # Return point estimates, targeted estimates of Qbar_0(A,W),
# # and thevector of clever covariates
# #------------------------------------------
# 
# estimates <- data.frame(cbind(PsiHat.SS=PsiHat.SS, PsiHat.IPTW, PsiHat.TMLE))
# Qbar.star <- data.frame(cbind(QbarAW.star, Qbar1W.star, Qbar0W.star))
# names(Qbar.star)<- c('QbarAW.star', 'Qbar1W.star', 'Qbar0W.star')
# 
# list(estimates=estimates, Qbar.star=Qbar.star, H.AW=H.AW)
# }
# 
# out <- run.tmle(ObsData = ObsData, 
#                 SL.library = katie.SL.library)
# 
# est <- out$estimates
# est*100



```



```{r g-computation by KATIE, eval=FALSE, include=FALSE}

set.seed(252)
# specify SL library
  listWrappers()
  SL.library <- c("SL.mean","SL.glm")
  other_options <- c("SL.glm.interaction","SL.randomForest","SL.polymars","SL.rpartPrune","SL.biglasso")

```


# BELOW LIES STEPHEN'S CODE


```{r preliminary data analysis BY STEPHEN, eval=FALSE, include=FALSE}
# make into a dataframe
  dataset_complete_cases <- as.data.frame(dataset_complete_cases)
# basic bits about data
  names(dataset_complete_cases)
  (n <- nrow(dataset_complete_cases)) # checks out with Katie's description -> yay, using correct dataset
  describe(dataset_complete_cases)
    unique(dataset_complete_cases$air_district)
    unique(dataset_complete_cases$pm25_12_plus)
    unique(dataset_complete_cases$cds_code)
# descriptives for numeric columns    
  num_cols <- c("math_standard_met","modeled_air_at_school","afr_am_percent","asian_filipino_percent","two_none_aian_pi_percent","hisp_latinx_percent","white_percent","english_learner_percent","frpm_percent","district_cost_per_pupil","student_teacher_ratio","cred_percent","school_k_12_enrollment")
  str(dataset_complete_cases[,num_cols])
  describe(dataset_complete_cases[,num_cols])
  by(dataset_complete_cases[,num_cols],list(dataset_complete_cases$pm25_12_plus),describe)
  aggregate(dataset_complete_cases[,num_cols],list(dataset_complete_cases$pm25_12_plus),
            function(x) {round(c(length(x),mean(x,na.rm=T),sd(x,na.rm=T)),2)})
    
  for (i in 1:length(num_cols)) {hist(dataset_complete_cases[,num_cols[i]],main=num_cols[i],breaks=20)}  
    # looks like there are some outlier for student-teacher ratio and cost per pupil (one school spending $346k per student?)
    dataset_complete_cases[dataset_complete_cases$district_cost_per_pupil>50000,] # ok so the Bay Area has $$$$
    unique(dataset_complete_cases[dataset_complete_cases$district_cost_per_pupil>50000,c("air_district","district_cost_per_pupil")])
    cost_hist <- hist(dataset_complete_cases$district_cost_per_pupil,breaks=20,plot=FALSE)
    plot(cost_hist$counts,log="x",type="h",lwd=10,lend=2)
    
    dataset_complete_cases[dataset_complete_cases$student_teacher_ratio>50,]
    table(dataset_complete_cases$air_district) # 70 schools in Feather River. why STR so high??
    
  round(prop.table(table(dataset_complete_cases$air_district)),3)
  round(prop.table(table(dataset_complete_cases$air_basin)),3)
  round(prop.table(table(dataset_complete_cases$air_basin_binned)),3) # much better
  round(prop.table(table(dataset_complete_cases$air_basin_4_bins)),3)
# make A variable into numeric & Y variable proportion
  dataset_complete_cases$pm25_12_plus <- ifelse(dataset_complete_cases$pm25_12_plus=="yes",1,0)
  dataset_complete_cases$math_standard_met <- dataset_complete_cases$math_standard_met/100
# correlations between predictors
  predictor_corrs <- Hmisc::rcorr(as.matrix(dataset_complete_cases[,c("pm25_12_plus",num_cols)]))$r
  #openxlsx::write.xlsx(list(predictor_corrs),"~/Desktop/PH252D/cifp/output_for_writeups/predictor_correlations.xlsx",colNames=T,rowNames=T)
  
```

```{r iptw & tmle BY STEPHEN, eval=FALSE, include=FALSE}
set.seed(252)
# specify SL library
  listWrappers()
  
  SL.interaction.LunchRace <- function(Y, X, newX, family, obsWeights, model = TRUE, ...) {
    if (is.matrix(X)) {
        X = as.data.frame(X)
    }
    fit.glm <- glm(Y ~ . #+ frpm_percent:two_none_aian_pi_af_am_percent 
                         #+ frpm_percent:asian_filipino_percent 
                         + frpm_percent:hisp_latinx_percent,
                   , data = X, family = family, weights = obsWeights, model = model)
    if (is.matrix(newX)) {
        newX = as.data.frame(newX)
    }
    pred <- predict(fit.glm, newdata = newX, type = "response")
    fit <- list(object = fit.glm)
    class(fit) <- "SL.interaction.LunchRace"
    out <- list(pred = pred, fit = fit)
    return(out)
  }
  
  SL.interaction.CostRace <- function(Y, X, newX, family, obsWeights, model = TRUE, ...) {
    if (is.matrix(X)) {
        X = as.data.frame(X)
    }
    fit.glm <- glm(Y ~ . #+ district_cost_per_pupil:two_none_aian_pi_af_am_percent 
                         #+ district_cost_per_pupil:asian_filipino_percent 
                         + district_cost_per_pupil:hisp_latinx_percent,
                   , data = X, family = family, weights = obsWeights, model = model)
    if (is.matrix(newX)) {
        newX = as.data.frame(newX)
    }
    pred <- predict(fit.glm, newdata = newX, type = "response")
    fit <- list(object = fit.glm)
    class(fit) <- "SL.interaction.CostRace"
    out <- list(pred = pred, fit = fit)
    return(out)
  }
  
  SL.interaction.CredRace <- function(Y, X, newX, family, obsWeights, model = TRUE, ...) {
    if (is.matrix(X)) {
        X = as.data.frame(X)
    }
    fit.glm <- glm(Y ~ . #+ cred_percent:two_none_aian_pi_af_am_percent 
                         #+ cred_percent:asian_filipino_percent 
                         + cred_percent:hisp_latinx_percent,
                   , data = X, family = family, weights = obsWeights, model = model)
    if (is.matrix(newX)) {
        newX = as.data.frame(newX)
    }
    pred <- predict(fit.glm, newdata = newX, type = "response")
    fit <- list(object = fit.glm)
    class(fit) <- "SL.interaction.CredRace"
    out <- list(pred = pred, fit = fit)
    return(out)
  }
  
  SL.library <- c("SL.mean","SL.glm","SL.glm.interaction","SL.bayesglm"
                  #,"SL.interaction.LunchRace","SL.interaction.CostRace","SL.interaction.CredRace"
                  )
  #other_options <- c("SL.glm.interaction","SL.xgboost","SL.randomForest","SL.polymars","SL.rpartPrune")
# SS, IPTW, & TMLE function
  run.tmle <- function(Y,A,ObsData, SL.library, id=NULL) {
   
    #------------------------------------------
    # Estimate the conditional mean outcome Qbar(A,W)
    #------------------------------------------
   
      # dataframe X with baseline covariates and exposure
        X <- ObsData[,names(ObsData) != Y]
      # settheA=1inX1andtheA=0inX0
        X1 <- X0<-X
        X1[,A] <- 1 # under exposure
        X0[,A] <- 0 # under control
    
       # call Super Learner for estimation of QbarAW
        QbarSL<- SuperLearner(Y=ObsData[,Y], X=X, SL.library=SL.library, family="quasibinomial", id=id)
        # QbarSL
        
       # initial estimates of the outcome, given the observed exposure & covariates
        QbarAW <- predict(QbarSL, newdata=ObsData)$pred
       # estimates of the outcome, given A=1 and covariates
        Qbar1W<- predict(QbarSL, newdata=X1)$pred
       # estimates of the outcome, given A=0 and covariates
        Qbar0W<- predict(QbarSL, newdata=X0)$pred
    
       # simple substitution estimator:
        PsiHat.SS<-mean(Qbar1W - Qbar0W)
    
    #------------------------------------------
    # Estimate the exposure mechanism g(A|W)
    #------------------------------------------
  
     # call Super Learner for the exposure mechanism
      gHatSL<- SuperLearner(Y=ObsData[,A], X=ObsData[,!(names(ObsData) %in% c(A,Y,id))],SL.library=SL.library, family="binomial", id=id)
     # generate predicted prob being exposed, given baseline covariates
      gHat1W<- gHatSL$SL.predict
     # predicted prob of not being exposed, given baseline covariates
      gHat0W<- 1- gHat1W
  
     # predicted prob of observed exposure, given baseline cov
      gHatAW<- rep(NA, n)
      gHatAW[ObsData[,A]==1]<- gHat1W[ObsData[,A]==1]
      gHatAW[ObsData[,A]==0]<- gHat0W[ObsData[,A]==0]
      wt <- 1/gHatAW
  
    #-------------------------------------------------
    # Clever covariate H(A,W) for each subject
    #-------------------------------------------------
      H.AW<- as.numeric(ObsData[,A]==1)/gHat1W - as.numeric(ObsData[,A]==0)/gHat0W
  
     # also want to evaluate the clever covariates at A=1 and A=0 for all subjects
       H.1W<- 1/gHat1W
       H.0W<- -1/gHat0W
       
     # IPTW estimator of the G-computation formula: -> INCLUDE HORVITZ-THOMPSON 
      PsiHat.IPTW <- mean(H.AW*ObsData[,Y])
      PsiHat.IPTW <- mean(wt*as.numeric(ObsData[,A]==1)*ObsData[,Y]) -
                     mean(wt*as.numeric(ObsData[,A]==0)*ObsData[,Y])
      
     # IPTW estimator of G-comp w Horvitz-Thompson adjusmtent
      PsiHat.IPTW_HT <- mean(wt*as.numeric(ObsData[,A]==1)*ObsData[,Y])/mean(wt*as.numeric(ObsData[,A]==1)) - 
                        mean(wt*as.numeric(ObsData[,A]==0)*ObsData[,Y])/mean(wt*as.numeric(ObsData[,A]==0))
  
    #------------------------------------------
    # Update the initial estimator of Qbar_0(A,W)
    #------------------------------------------
      logitUpdate<- glm(ObsData[,Y] ~ -1 +offset(qlogis(QbarAW)) + H.AW, family='quasibinomial')
      epsilon <- logitUpdate$coef
  
      QbarAW.star<- plogis(qlogis(QbarAW)+ epsilon*H.AW)
      Qbar1W.star<- plogis(qlogis(Qbar1W)+ epsilon*H.1W)
      Qbar0W.star<- plogis(qlogis(Qbar0W)+ epsilon*H.0W)
  
    #------------------------------------------
    # Estimate Psi(P_0)
    #------------------------------------------
  
      PsiHat.TMLE <- mean(Qbar1W.star - Qbar0W.star)
  
    #------------------------------------------
    # Return point estimates, targeted estimates of Qbar_0(A,W),
    # and thevector of clever covariates
    #------------------------------------------
  
      estimates <- data.frame(cbind(PsiHat.SS=PsiHat.SS, PsiHat.IPTW, PsiHat.IPTW_HT, PsiHat.TMLE))
      Qbar.star <- data.frame(cbind(QbarAW.star, Qbar1W.star, Qbar0W.star))
      names(Qbar.star)<- c('QbarAW.star', 'Qbar1W.star', 'Qbar0W.star')
      list(estimates=estimates, Qbar.star=Qbar.star, H.AW=H.AW)
  }
# practice run of function
  # subset of rows
    sub_data <- dataset_complete_cases[sample(1:n,1000,replace=F),]
    nrow(sub_data)
    duplicated(sub_data$cds_code)
    table(sub_data$pm25_12_plus)
  # run function
    names(sub_data)
    include_vars_analysis <- c("math_standard_met","pm25_12_plus","air_basin_binned","asian_filipino_percent","two_none_aian_pi_af_am_percent","hisp_latinx_percent",
                               "english_learner_percent","frpm_percent","district_cost_per_pupil","student_teacher_ratio","cred_percent","school_k_12_enrollment")
    out <- run.tmle(Y="math_standard_met", A="pm25_12_plus", ObsData=sub_data[,include_vars_analysis], SL.library=SL.library) 
      # generating a ton of errors. removing algorithms from SuperLearner
      # think interaction algorithms will be super problematic bc number of covariates is really high from all the districts
      # FIGURED OUT ISSUE: When we resample, not all districts are in each sample and predictions are generated for non-existent districts and not generated for extisting districts
    est <- out$estimates
    est*100
  # run with ltmle for comparison
    sub_data2 <- sub_data[,include_vars_analysis]
    names(sub_data2)
    sub_data2 <- sub_data2[,c(names(sub_data2[3:12]),"math_standard_met","pm25_12_plus")]
    ltmle.SL<- ltmle(data=sub_data2, Anodes='pm25_12_plus', Ynodes='math_standard_met', abar=list(1,0),SL.library=SL.library)
    summary(ltmle.SL)    
    
# the real thing
  out <- run.tmle(Y="math_standard_met", A="pm25_12_plus", ObsData=dataset_complete_cases[,include_vars_analysis], SL.library=SL.library) 
  est <- out$estimates
  est*100
# real thing with ltmle for comparison  
  dataset_complete_cases_ltmle <- dataset_complete_cases[,include_vars_analysis]
  names(dataset_complete_cases_ltmle)
  dataset_complete_cases_ltmle <- dataset_complete_cases_ltmle[,c(3:12,1:2)]
  ltmle.SL<- ltmle(data=dataset_complete_cases_ltmle, Anodes='pm25_12_plus', Ynodes='math_standard_met', abar=list(1,0),SL.library=SL.library)  
  summary(ltmle.SL)  
   


```

```{r, Non-Parametric Bootstrap!!!! Runs just really slowly, eval=FALSE, include=FALSE}
# how many cores
  parallel::detectCores(all.tests = FALSE, logical = TRUE)
# non-parametric bootstrap
nonparam_boot <- function(ObsData,A,Y,id,SL.library,num_boots) {
  # make output matrix
    estimates <- matrix(NA,nrow=num_boots,ncol=4)
  # bootstrap data
    for (i in 1:num_boots) {
      # make data
        n <- nrow(ObsData)
        ObsData[,id] <- 1:n
        bootIndices<- sample(1:n, replace=T)
        bootData<- ObsData[bootIndices,]
      # estimate target parameters
        # simple sub
          X <- bootData[,names(ObsData) != Y]
          X1 <- X; X1[,A] <- 1
          X0 <- X; X0[,A] <- 0
          QbarSL<- SuperLearner(Y=bootData[,Y], X=X, SL.library=SL.library, family="binomial", id=bootData[,id])
          QbarAW <- predict(QbarSL, newdata=bootData)$pred
          Qbar1W<- predict(QbarSL, newdata=X1)$pred
          Qbar0W<- predict(QbarSL, newdata=X0)$pred
          simple_sub <- mean(Qbar1W-Qbar0W)
        # IPTW
          gHatSL<- SuperLearner(Y=bootData[,A], X=bootData[,!(names(bootData) %in% c(A,Y,id))], SL.library=SL.library, family="binomial", id=bootData[,id])
          gHat1W <- gHatSL$SL.predict
          gHat0W <- 1-gHatSL$SL.predict
          gHatAW <- ifelse(bootData[,A]==1,gHat1W,gHat0W)
          wt <- 1/gHatAW
          H.AW <- as.numeric(bootData[,A]==1)/gHat1W - as.numeric(bootData[,A]==0)/gHat0W
          H.1W <- as.numeric(X1[,A]==1)/gHat1W - as.numeric(X1[,A]==0)/gHat0W
          H.0W <- as.numeric(X0[,A]==1)/gHat1W - as.numeric(X0[,A]==0)/gHat0W
          iptw <- mean(H.AW*bootData[,Y])
          iptw_ht <- mean(wt*as.numeric(bootData[,A]==1)*bootData[,Y])/mean(wt*as.numeric(bootData[,A]==1)) - 
                     mean(wt*as.numeric(bootData[,A]==0)*bootData[,Y])/mean(wt*as.numeric(bootData[,A]==0))
        # TMLE
          logitUpdate <- glm(bootData[,Y] ~ -1 +offset(qlogis(QbarAW)) + H.AW,family='binomial')
          epsilon <- logitUpdate$coef
          QbarAW.star <- plogis(qlogis(QbarAW)+ epsilon*H.AW)
          Qbar1W.star <- plogis( qlogis(Qbar1W)+ epsilon*H.1W)
          Qbar0W.star <- plogis( qlogis(Qbar0W)+ epsilon*H.0W)
          PsiHat.TMLE <- mean(Qbar1W.star- Qbar0W.star)
      # save results
        estimates[i,] <- c(simple_sub,iptw,iptw_ht,PsiHat.TMLE)
      # periodically print i
        if (i%%5==0) (print(i))
    }
    return(estimates)
}  
npb_estimates <- nonparam_boot(ObsData=dataset_complete_cases[,c("cds_code",include_vars_analysis)],
                           A="pm25_12_plus",Y="math_standard_met",id="cds_code",SL.library=SL.library,num_boots=1) 
summary(npb_estimates)  
apply(npb_estimates,2,hist)

create.CI <- function(pt, boot, alpha=0.05){
   Zquant <- qnorm(alpha/2, lower.tail=F)
   CI.normal <- c(pt - Zquant*sd(boot), pt + Zquant*sd(boot) )
   CI.quant  <- quantile(boot, prob=c(0.025,0.975) )
   out<- data.frame(rbind(CI.normal, NA))*100
 colnames(out)<- c('CI.lo', 'CI.hi')
 out
}
apply(npb_estimates,2,function(x) create.CI(pt=mean(x,na.rm=T),boot=x))
apply(npb_estimates,2,function(x) quantile(x,probs=c(.025,.975),na.rm=T))



```

```{r, Influence Curves!!!, eval=FALSE, include=FALSE}
# TMLE inference -> p-value + CI assuming normal distribution
  IC <- out$H.AW*(dataset_complete_cases$math_standard_met - out$Qbar.star$QbarAW.star) + out$Qbar.star$Qbar1W.star - out$Qbar.star$Qbar0W.star - out$estimates$PsiHat.TMLE
  var(IC)
  (IC_se <- sqrt(var(IC)/n))
  (Psy.LL <- out$estimates$PsiHat.TMLE-1.96*IC_se)
  (Psy.UL <- out$estimates$PsiHat.TMLE+1.96*IC_se)
  2*pnorm(abs(out$estimates$PsiHat.TMLE/IC_se),lower.tail=F)
# IPTW inference -> p-value + CI assuming normal distribution
  IC <- out$H.AW*(dataset_complete_cases$math_standard_met - out$Qbar.star$QbarAW.star) + out$Qbar.star$Qbar1W.star - out$Qbar.star$Qbar0W.star - out$estimates$PsiHat.IPTW
  var(IC)
  (IC_se <- sqrt(var(IC)/n))
  (Psy.LL <- out$estimates$PsiHat.TMLE-1.96*IC_se)
  (Psy.UL <- out$estimates$PsiHat.TMLE+1.96*IC_se)
  2*pnorm(abs(out$estimates$PsiHat.TMLE/IC_se),lower.tail=F)  
# IPTW_HT inference -> p-value + CI assuming normal distribution
  IC <- out$H.AW*(dataset_complete_cases$math_standard_met - out$Qbar.star$QbarAW.star) + out$Qbar.star$Qbar1W.star - out$Qbar.star$Qbar0W.star - out$estimates$PsiHat.IPTW_HT
  var(IC)
  (IC_se <- sqrt(var(IC)/n))
  (Psy.LL <- out$estimates$PsiHat.TMLE-1.96*IC_se)
  (Psy.UL <- out$estimates$PsiHat.TMLE+1.96*IC_se)
  2*pnorm(abs(out$estimates$PsiHat.TMLE/IC_se),lower.tail=F)  
  
  
  
  
# CI coverage and Type 1 error rates -> NOTE TO KATIE: I DIDN'T EDIT THIS PART BECAUSE I DON'T THINK WE HAVE TO INCLUDE IT. IT'S NOT MENTIONED IN THE GUIDELINES +, MORE IMPORTANTLY, DOING IT DEPENDS ON KNOWING THE TRUE VALUE OF THE TARGET CAUSAL PARAMETER, WHICH WE DON'T KNOW
  # extra functions
  generateY<- function(W1, W2, W3, W4, A, U.Y){
    prob <- plogis(-1.5+A-2*W3+0.5*W4+5*W1*W2*W4)
    as.numeric(U.Y < prob)
  }
  generateData<- function(n, effect=T, get.psi.F=F){
    W1 <- rbinom(n, size=1, prob=0.5)
    W2 <- rbinom(n, size=1, prob=0.5)
    W3 <- runif(n, min=0, max=1)
    W4 <- runif(n, min=0, max=5)
    pscore <- plogis(1+2*W1*W2-W4)
    A<- rbinom(n, size=1, prob=pscore)
    U.Y<- runif(n,0,1)
    # generate the counterfactual outcome with A=0
    Y.0<- generateY(W1=W1, W2=W2, W3=W3, W4=W4, A=0, U.Y=U.Y)
    if(!effect){ # if there is no effect, the counterfactual under txt =
      # the counterfactual under the control
      Y.1<- Y.0 } else { 
        # otherwise, generated the counterfactual outcome with A=1
        Y.1<- generateY(W1=W1, W2=W2, W3=W3, W4=W4, A=1, U.Y=U.Y)
      }
    # assign the observed outcome based on the observed exposure
    Y<- rep(NA, n)
    Y[A==1]<- Y.1[A==1]
    Y[A==0]<- Y.0[A==0]
    data<- data.frame(W1, W2, W3, W4, A, Y, Y.1, Y.0)
    if(!get.psi.F){
      data <- subset(data,select=c(W1,W2,W3,W4,A,Y))
    }
    data
  }
  
  Psi.0 <- 0
  n <- nrow(dataset_complete_cases)
  R <- 500

  pt.est <- rep(NA,R)
  ci.cov <- rep(NA,R)
  reject <- rep(NA,R)

  for (i in 1:R) {
    # make data
      NewData<- generateData(n, effect=F, get.psi.F=F)
    # TMLE
      X <- NewData[,1:5]
      X1 <- X; X1$A <- 1
      X0 <- X; X0$A <- 0
      QbarSL <- SuperLearner(Y=NewData$Y, X=X, SL.library=SL.library, family="binomial")
      QbarAW <- predict(QbarSL, newdata=NewData)$pred
      Qbar1W<- predict(QbarSL, newdata=X1)$pred
      Qbar0W<- predict(QbarSL, newdata=X0)$pred
      
      gHatSL<- SuperLearner(Y=NewData$A, X=subset(NewData, select= -c(A,Y)),SL.library=SL.library, family="binomial")
      gHat1W <- gHatSL$SL.predict
      gHat0W <- 1-gHatSL$SL.predict
      gHatAW <- ifelse(NewData$A==1,gHat1W,gHat0W)
      H.AW <- as.numeric(NewData$A==1)/gHat1W - as.numeric(NewData$A==0)/gHat0W
      H.1W <- as.numeric(X1$A==1)/gHat1W - as.numeric(X1$A==0)/gHat0W
      H.0W <- as.numeric(X0$A==1)/gHat1W - as.numeric(X0$A==0)/gHat0W
      
      logitUpdate <- glm(NewData$Y ~ -1 +offset(qlogis(QbarAW)) + H.AW,family='binomial')
      epsilon <- logitUpdate$coef
      QbarAW.star <- plogis(qlogis(QbarAW)+ epsilon*H.AW)
      Qbar1W.star <- plogis( qlogis(Qbar1W)+ epsilon*H.1W)
      Qbar0W.star <- plogis( qlogis(Qbar0W)+ epsilon*H.0W)
      PsiHat.TMLE <- mean(Qbar1W.star- Qbar0W.star)
      
      IC <- H.AW*(NewData$Y - QbarAW.star) + Qbar1W.star - Qbar0W.star - PsiHat.TMLE
      IC_se <- sqrt(var(IC)/n)
      Psi.LL <- PsiHat.TMLE-1.96*IC_se
      Psi.UL <- PsiHat.TMLE+1.96*IC_se
      p.val <- 2*pnorm(abs(PsiHat.TMLE/IC_se),lower.tail=F)
    # save output
      pt.est[i] <- PsiHat.TMLE
      ci.cov[i] <- Psi.0 >= Psi.LL & Psi.0 <= Psi.UL
      reject[i] <- p.val < .05
  }

  hist(pt.est) # looks distributed around zero
  mean(ci.cov) # 93% contain the true value (pretty close to what it should be)
  mean(reject) # 0% were falsely rejected


```



```{r, KATIE PARALLEL Inference Curve!!!! DOES NOT RUN YET, eval=FALSE, include=FALSE}

# set up parallelization (my computer has 8 cores and this uses 7 of them)
(cluster = parallel::makeCluster(3))
parallel::clusterEvalQ(cluster, library(SuperLearner))

# load dataset
load(file = file.path("dataset_complete_cases.rdata"))

# non-parametric bootstrap
nonparam_boot_parallel <- function(ObsData,A,Y,id,SL.library,num_boots) {
  # make output matrix
    estimates <- matrix(NA,nrow=num_boots,ncol=4)
  # bootstrap data
    for (i in 1:num_boots) {
      # make data
        n <- nrow(ObsData)
        ObsData[,id] <- 1:n
        bootIndices<- sample(1:n, replace=T)
        bootData<- ObsData[bootIndices,]
      # estimate target parameters
        # simple sub
          X <- bootData[,names(ObsData) != Y]
          X1 <- X; X1[,A] <- 1
          X0 <- X; X0[,A] <- 0
          QbarSL<- snowSuperLearner(Y=bootData[,Y], X=X, SL.library=SL.library, family="binomial", id=bootData[,id], cluster = cluster)
          QbarAW <- predict(QbarSL, newdata=bootData)$pred
          Qbar1W<- predict(QbarSL, newdata=X1)$pred
          Qbar0W<- predict(QbarSL, newdata=X0)$pred
          simple_sub <- mean(Qbar1W-Qbar0W)
        # IPTW
          gHatSL<- snowSuperLearner(Y=bootData[,A], X=bootData[,!(names(bootData) %in% c(A,Y,id))], SL.library=SL.library, family="binomial", id=bootData[,id], cluster = cluster)
          gHat1W <- gHatSL$SL.predict
          gHat0W <- 1-gHatSL$SL.predict
          gHatAW <- ifelse(bootData[,A]==1,gHat1W,gHat0W)
          wt <- 1/gHatAW
          H.AW <- as.numeric(bootData[,A]==1)/gHat1W - as.numeric(bootData[,A]==0)/gHat0W
          H.1W <- as.numeric(X1[,A]==1)/gHat1W - as.numeric(X1[,A]==0)/gHat0W
          H.0W <- as.numeric(X0[,A]==1)/gHat1W - as.numeric(X0[,A]==0)/gHat0W
          iptw <- mean(H.AW*bootData[,Y])
          iptw_ht <- mean(wt*as.numeric(bootData[,A]==1)*bootData[,Y])/mean(wt*as.numeric(bootData[,A]==1)) - 
                     mean(wt*as.numeric(bootData[,A]==0)*bootData[,Y])/mean(wt*as.numeric(bootData[,A]==0))
        # TMLE
          logitUpdate <- glm(bootData[,Y] ~ -1 +offset(qlogis(QbarAW)) + H.AW,family='binomial')
          epsilon <- logitUpdate$coef
          QbarAW.star <- plogis(qlogis(QbarAW)+ epsilon*H.AW)
          Qbar1W.star <- plogis( qlogis(Qbar1W)+ epsilon*H.1W)
          Qbar0W.star <- plogis( qlogis(Qbar0W)+ epsilon*H.0W)
          PsiHat.TMLE <- mean(Qbar1W.star- Qbar0W.star)
      # save results
        estimates[i,] <- c(simple_sub,iptw,iptw_ht,PsiHat.TMLE)
    }
    return(estimates)
}  


include_vars_analysis <- c("math_standard_met","pm25_12_plus","air_basin_binned","afr_am_percent","asian_filipino_percent","two_none_aian_pi_percent","hisp_latinx_percent", "english_learner_percent","frpm_percent","district_cost_per_pupil","student_teacher_ratio","cred_percent","school_k_12_enrollment")

SL.library <- c("SL.mean","SL.glm","SL.glm.interaction","SL.bayesglm")

estimates <- nonparam_boot_parallel(ObsData=dataset_complete_cases[,c("cds_code",include_vars_analysis)],
                           A="pm25_12_plus",Y="math_standard_met",id="cds_code",SL.library=SL.library,num_boots=5) 
summary(estimates)  
apply(estimates,2,hist)

create.CI <- function(pt, boot, alpha=0.05){
   Zquant <- qnorm(alpha/2, lower.tail=F)
   CI.normal <- c(pt - Zquant*sd(boot), pt + Zquant*sd(boot) )
   CI.quant  <- quantile(boot, prob=c(0.025,0.975) )
   out<- data.frame(rbind(CI.normal, NA))*100
 colnames(out)<- c('CI.lo', 'CI.hi')
 out
}
apply(estimates,2,function(x) create.CI(pt=mean(x,na.rm=T),boot=x))
apply(estimates,2,function(x) quantile(x,probs=c(.025,.975),na.rm=T))



```































