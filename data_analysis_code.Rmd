---
title: "Causal Inference Final Project Code"
author: "Katherine Rose Wolf"
date: "March 27, 2020"
output: html_document
---

```{r setup load libraries}

library(extrafont)

# # loads fonts 
# font_import(pattern = "Raleway")

library(praise)
library(tidyverse)
library(tableone)
library(kableExtra)
library(furniture)
library(RColorBrewer)
library(psych)
library(SuperLearner)
library(ltmle)
library(arm)
library(rpart)

# # loads fonts 
# font_import()

# stop r from abbreviating doubles in school ids
options("scipen" = 40)

praise()  # self-esteem boost

load(file = "dataset_full.rdata")

load(file = "dataset_complete_cases.rdata")

load(file = "dataset_complete_cases_forget_funding.rdata") # no per cap variable so N goes to ~9000

```

# BELOW LIES KATIE'S PLAYGROUND

```{r KATIE code pre-SuperLearner}

dataset_complete_cases_working <- 
  dataset_complete_cases %>% 
  mutate(math_proportion = math_standard_met/100,
         pm25_12_plus = ifelse(pm25_12_plus == "yes", 1, 0), 
         air_basin_4_bins = factor(air_basin_4_bins))

# histogram of math percentage (weird, lots of zeroes)
hist(dataset_complete_cases_working$math_standard_met, 
     breaks = 20)

# simple mean: areas with pm above 12 lose 2.03 percentage points of passage
mean(dataset_complete_cases_working$math_proportion[
  dataset_complete_cases_working$pm25_12_plus == 1
  ] - 
    dataset_complete_cases_working$math_proportion[
      dataset_complete_cases_working$pm25_12_plus == 0])

# this code just fits the exposure linearly by the outcome
plain_glm_one_variable <- glm(math_standard_met ~ pm25_12_plus, 
                              data = dataset_complete_cases_working)
summary(plain_glm_one_variable)

# full linear glm no interactions
plain_glm <- glm(math_standard_met ~ 
                   pm25_12_plus + 
                   air_basin_4_bins + 
                   asian_filipino_percent + 
                   two_none_aian_pi_af_am_percent + 
                   hisp_latinx_percent + 
                   english_learner_percent + 
                   frpm_percent + 
                   district_cost_per_pupil + 
                   student_teacher_ratio + 
                   school_k_12_enrollment +
                   cred_percent, 
                 data = dataset_complete_cases_working)

summary(plain_glm)

# full linear glm drop weirds
plain_glm_drop_per_pupil <- glm(math_standard_met ~ 
                   pm25_12_plus + 
                   air_basin + 
                   afr_am_percent + 
                   asian_filipino_percent + 
                   two_none_aian_pi_percent + 
                   hisp_latinx_percent + 
                   english_learner_percent + 
                   frpm_percent + 
                   cred_percent +
                   school_k_12_enrollment, 
                 data = dataset_complete_cases_working)

summary(plain_glm_drop_per_pupil)

summary(dataset_complete_cases_working$student_teacher_ratio)

# make working dataset without funding data
dataset_complete_cases_forget_funding_working <- 
  dataset_complete_cases_forget_funding

# simple glm without funding data
glm_forget_funding <- 
  glm(math_standard_met ~ 
                   pm25_12_plus + 
                   air_basin + 
                   afr_am_percent + 
                   asian_filipino_percent + 
                   two_none_aian_pi_percent + 
                   hisp_latinx_percent + 
                   english_learner_percent + 
                   frpm_percent + 
                   student_teacher_ratio + 
                   cred_percent + 
                   school_k_12_enrollment, 
                 data = dataset_complete_cases_forget_funding_working)

summary(glm_forget_funding)


```


```{r plots for slides}

dataset_complete_cases_working <- 
  dataset_complete_cases %>% 
  mutate(math_proportion = math_standard_met/100,
         pm25_12_plus = factor(pm25_12_plus), 
         air_district = factor(air_district), 
         air_basin = factor(air_basin))

simulated_full_only_complete_cases <- 
  dataset_full %>% 
  filter(!is.na(math_standard_met)) %>% 
  filter(!is.na(english_learner_percent)) %>% 
  filter(!is.na(student_teacher_ratio)) %>% 
  filter(!is.na(air_basin)) %>% 
  filter(!is.na(district_cost_per_pupil))

# # count observations by air basin
# dataset_complete_cases_working %>% 
#   group_by(air_basin) %>% 
#   summarize(count = n()) %>% 
#   View()

#------------------------------------------
# Exposure-Outcome
#------------------------------------------

# only if actually plotting
windowsFonts(Times=windowsFont("Raleway"))

# make basic scatterplot with loess line through it
ggplot(data = simulated_full_only_complete_cases,
       aes(x = modeled_air_at_school,
           y = math_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth(method=loess,
              se=TRUE,
              color="darkred") +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("Mean Percentage of Students Who Met or Exceeded \nMath Standard in 2018-19") +
  theme(text=element_text(family="Raleway"))

# make basic scatterplot with plain line through it
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = modeled_air_at_school, 
           y = math_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_abline() +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("Mean Percentage of Students Who Met or Exceeded \nMath Standard in 2018-19") +
  ggtitle("Complete Cases Only")

# make basic scatterplot with loess line through it
ggplot(data = dataset_full,
       aes(x = modeled_air_at_school,
           y = math_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth(method=loess,
              se=TRUE,
              color="darkred") +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("Mean Percentage of Students Who Met or Exceeded \nMath Standard in 2018-19") +
  ggtitle("Full Data")

# make basic scatterplot with plain line through it
ggplot(data = dataset_full, 
       aes(x = modeled_air_at_school, 
           y = math_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_abline() +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("Mean Percentage of Students Who Met or Exceeded \nMath Standard in 2018-19") +
  ggtitle("Full Data")

# box plot PM2.5 versus math score
ggplot(data = dataset_complete_cases_working, 
       aes(x = pm25_12_plus, 
           y = math_standard_met)) +
  geom_boxplot() +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("Mean Percentage of Students Who Met or Exceeded \nMath Standard in 2018-19") +
  ggtitle("Complete Cases Only")

colnames(dataset_complete_cases)

#------------------------------------------
# Enrollment
#------------------------------------------

# scatterplot enrollment and pm2.5
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = modeled_air_at_school, 
           y = school_k_12_enrollment)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth() +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("School Enrollment 2018-19") +
  ggtitle("Complete Cases Only")

# scatterplot enrollment and test scores
ggplot(data = dataset_complete_cases, 
       aes(x = school_k_12_enrollment, 
           y = math_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth() +
  xlab(expression(paste("School Enrollment 2018-19"))) +
  ylab("Mean Percentage of Students Who Met or Exceeded \nMath Standard in 2018-19") +
  ggtitle("Complete Cases Only")

# box plot PM2.5 versus enrollment
ggplot(data = dataset_complete_cases_working, 
       aes(x = pm25_12_plus, 
           y = school_k_12_enrollment)) +
  geom_boxplot() +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("School Enrollment 2018-19") +
  ggtitle("Complete Cases Only")

#------------------------------------------
# FRPM
#------------------------------------------

# scatterplot frpm air pollution
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = frpm_percent, 
           y = modeled_air_at_school)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth() +
  xlab("Students with Free/Reduced-Price Lunches (%)") +
  ylab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ggtitle("Complete Cases Only")

# boxplot frpm pm
ggplot(data = dataset_complete_cases_working, 
       aes(x = pm25_12_plus, 
           y = frpm_percent)) +
  geom_boxplot() +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("Students with Free/Reduced-Price Lunches (%)") +
  ggtitle("Complete Cases Only")

# scatterplot frpm math standard met
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = frpm_percent, 
           y = math_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth() +
  xlab("Students with Free/Reduced-Price Lunches (%)") +
  ylab("Math Standard Met") +
  ggtitle("Complete Cases Only")

#------------------------------------------
# Per-pupil funding
#------------------------------------------

# scatterplot cost per pupil air pollution
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = district_cost_per_pupil, 
           y = modeled_air_at_school)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth() +
  xlab("District Cost Per Pupil ($)") +
  ylab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ggtitle("Complete Cases Only")

# boxplot cost per pupil pm
ggplot(data = dataset_complete_cases_working, 
       aes(x = pm25_12_plus, 
           y = district_cost_per_pupil)) +
  geom_boxplot() +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("District Cost Per Pupil ($)") +
  ggtitle("Complete Cases Only") +
  ylim(0, 150000)

# scatterplot district cost per pupil math standard met
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = district_cost_per_pupil, 
           y = math_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth() +
  xlab("District Cost Per Pupil ($)") +
  ylab("Math Standard Met") +
  ggtitle("Complete Cases Only") +
  xlim(0, 40000) +
  ylim(0, 100)

#------------------------------------------
# Student-teacher ratio
#------------------------------------------

# scatterplot student-teacher ratio air pollution
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = student_teacher_ratio, 
           y = modeled_air_at_school)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth() +
  xlab("Student-teacher ratio") +
  ylab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ggtitle("Complete Cases Only")

# boxplot student-teacher ratio pm
ggplot(data = dataset_complete_cases_working, 
       aes(x = pm25_12_plus, 
           y = student_teacher_ratio)) +
  geom_boxplot() +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("Student-teacher ratio") +
  ggtitle("Complete Cases Only")

# scatterplot district student-teacher ratio math standard met
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = student_teacher_ratio, 
           y = math_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth(method = "loess") +
  xlab("Student-teacher ratio") +
  ylab("Math standard met (%)") +
  ggtitle("Complete Cases Only")

#------------------------------------------
# Fully-credentialed teachers
#------------------------------------------

# scatterplot credentials / air pollution
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = cred_percent, 
           y = modeled_air_at_school)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth() +
  xlab("Fully credentialed teachers (%)") +
  ylab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ggtitle("Complete Cases Only")

# boxplot credentials / pm
ggplot(data = dataset_complete_cases_working, 
       aes(x = pm25_12_plus, 
           y = cred_percent)) +
  geom_boxplot() +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("Fully credentialed teachers (%)") +
  ggtitle("Complete Cases Only")

# scatterplot credentials / math standard met
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = cred_percent, 
           y = math_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth(method = "loess") +
  xlab("Fully credentialed teachers (%)") +
  ylab("Math standard met (%)") +
  ggtitle("Complete Cases Only")

#------------------------------------------
# Air basin
#------------------------------------------

#------------------------------------------
# Percent white
#------------------------------------------

#------------------------------------------
# Percent Hispanic/Latinx
#------------------------------------------

#------------------------------------------
# Percent Asian/Filipino
#------------------------------------------

#------------------------------------------
# Percent African American
#------------------------------------------

#------------------------------------------
# Percent other
#------------------------------------------

# save plot as pdf
ggsave(device = "bmp", 
       filename = file.path("output_for_writeups", 
                            "scatterplot.bmp"), 
       height = 5, 
       width = 5)

```


```{r KATIE tabling the ancestors}

# load data
load(file = file.path("dataset_full.rdata"))

simulated_full_only_complete_cases <- 
  dataset_full %>% 
  filter(!is.na(math_standard_met)) %>% 
  filter(!is.na(english_learner_percent)) %>% 
  filter(!is.na(student_teacher_ratio)) %>% 
  filter(!is.na(air_basin)) %>% 
  filter(!is.na(district_cost_per_pupil))

data_for_table_one <- 
  simulated_full_only_complete_cases

colnames(data_for_table_one)

# relabel variables
data_for_little_table_one <- 
  data_for_table_one %>% 
  mutate("PM25" = factor(pm25_12_plus), 
         "School enrollment (n)" = school_k_12_enrollment, 
         "English learners (%)" = round(english_learner_percent, 1),
         "Free/reduced-price meal eligibility (%)" = frpm_percent, 
         "Student-teacher ratio" = student_teacher_ratio,
         "Fully credentialed teachers" = cred_percent,
         "Cost per pupil" = district_cost_per_pupil,
         "Air basin group" = air_basin_binned, 
         "Air basin" = air_basin,
         "asian_filipino_percent" = round(asian_filipino_percent, 1),
         "Asian (%)" = round(asian_percent, 1),
         "Filipino (%)" = round(filipino_percent, 1),
         "Hispanic/Latino (any race) (%)" = round(hisp_latinx_percent, 1),
         "White (%)" = round(white_percent, 1),
         "two_none_aian_pi_af_am_percent" = round(two_none_aian_pi_af_am_percent, 1),
         "African American (%)" = round(afr_am_percent, 1),
         "American Indian or Alaska Native (%)" = round(aian_percent, 1),
         "Pacific Islander (%)" = round(pacific_islander_percent, 1),
         "Two or more races (not Hispanic/Latino) (%)" = round(two_plus_percent, 1),
         "Race/ethnicity not reported (%)" = round(not_reported_percent, 1),
         "Math standard met or exceeded (%)" = math_standard_met) %>% 
  dplyr::select(`PM25`, 
         `School enrollment (n)`, 
         `English learners (%)`,
         `Free/reduced-price meal eligibility (%)`, 
         `Student-teacher ratio`,
         `Fully credentialed teachers`,
         `Cost per pupil`,
         `Air basin group`, 
         `Air basin`,
         `asian_filipino_percent`,
         `Asian (%)`,
         `Filipino (%)`,
         `Hispanic/Latino (any race) (%)`,
         `White (%)`,
         `two_none_aian_pi_af_am_percent`,
         `African American (%)`,
         `American Indian or Alaska Native (%)`,
         `Pacific Islander (%)`,
         `Two or more races (not Hispanic/Latino) (%)`,
         `Race/ethnicity not reported (%)`,
         `Math standard met or exceeded (%)`) %>% 
  as.data.frame()



# create a list of variables for the table
# (not including the stratification variable)
little_table_one_variables <- 
  c(     "PM25", 
         "School enrollment (n)", 
         "English learners (%)",
         "Free/reduced-price meal eligibility (%)", 
         "Student-teacher ratio",
         "Fully credentialed teachers",
         "Cost per pupil",
         "Air basin group", 
         "Air basin",
         "asian_filipino_percent",
         "Asian (%)",
         "Filipino (%)",
         "Hispanic/Latino (any race) (%)",
         "White (%)",
         "two_none_aian_pi_af_am_percent",
         "African American (%)",
         "American Indian or Alaska Native (%)",
         "Pacific Islander (%)",
         "Two or more races (not Hispanic/Latino) (%)",
         "Race/ethnicity not reported (%)",
         "Math standard met or exceeded (%)")

table_one <- CreateTableOne(vars = little_table_one_variables,
                            # factorVars = factor_variables,
                            strata = "PM25",
                            data = data_for_little_table_one,
                            test = FALSE,
                            includeNA = TRUE)

save(table_one, 
     file = "table_one.rdata")

# Creates a formatted table, using kable from the knitr package
# Would want to clean this up for publication purposes:
hi <- kable(print(table_one,
                  showAllLevels = TRUE,
                  printToggle = FALSE,
                  noSpaces = TRUE,
                  catDigits = 1,
                  contDigits = 3),
            col.names = c("", "Mean PM2.5 <= 12 ug/m3", "Mean PM2.5 > 12 ug/m3"),
            caption=paste(".  Descriptive statistics for California public schools in 2018-19, stratified by 2018 mean PM2.5 concentration.")) %>% 
  kable_styling("striped")

hi

table_total <- CreateTableOne(vars = little_table_one_variables,
                            # factorVars = factor_variables,
                            data = data_for_little_table_one,
                            test = FALSE,
                            includeNA = TRUE)

save(table_total, 
     file = "table_one.rdata")

hi2 <- kable(print(table_total,
                  showAllLevels = TRUE,
                  printToggle = FALSE,
                  noSpaces = TRUE,
                  catDigits = 1,
                  contDigits = 3),
            col.names = c("", "Total"),
            caption=paste(".  Descriptive statistics for California public schools in 2018-19, stratified by 2018 mean PM2.5 concentration.")) %>% 
  kable_styling("striped")

hi2

```


```{r old code not to run}

# # reduce runs of biglasso from 100 to 5 to save time
# new.algorithm.1 = create.Learner("SL.biglasso", 
#                                  params = list(nlambda = 5))
# 
# new.algorithm.2 = create.Learner("SL.randomForest", 
#                                 params = list(ntrees = 10))

```



```{r KATIE attempt at all 4 separately}

# set up parallelization (my computer has 8 cores and this uses 7 of them)
# (cluster = parallel::makeCluster(7))
# parallel::clusterEvalQ(cluster, library(SuperLearner))


load(file = "dataset_complete_cases.rdata")

dataset_complete_cases_working <- 
  dataset_complete_cases %>% 
  mutate(math_proportion = math_standard_met/100,
         pm25_12_plus = ifelse(pm25_12_plus == "yes", 1, 0),
         air_basin_4_bins = factor(air_basin_4_bins))

adjustment_set_variables <- 
  c("air_basin_4_bins", 
    "asian_filipino_percent", 
    "two_none_aian_pi_af_am_percent",
    "hisp_latinx_percent", 
    "english_learner_percent", 
    "frpm_percent",
    "district_cost_per_pupil", 
    "student_teacher_ratio", 
    "cred_percent",
    "school_k_12_enrollment")

ObsData <- 
  dataset_complete_cases_working %>% 
  dplyr::select(Y = math_proportion,
                A = pm25_12_plus, 
                all_of(adjustment_set_variables))

ObsData <- as.data.frame(ObsData)

# subset of rows
little_data <- ObsData[sample(x = 1:nrow(ObsData),
                              size = 1000,
                              replace = F),]
nrow(little_data)
duplicated(little_data$cds_code)
table(little_data$A)

# specify SL library
listWrappers()
katie.SL.library <- c("SL.mean", 
                      "SL.glm", 
                      "SL.glm.interaction", 
                      "SL.bayesglm", 
                      "SL.gam", 
                      "SL.glm.complex")
                      # "SL.rpartPrune")
                      # "SL.biglasso"
                      # "SL.extraTrees"
                      # "SL.glmnet"
                      # new.algorithm.2$names
                      # "SL.polymars"
                      # "SL.ridge"
                      # "SL.caret"
# "SL.ranger"
# "SL.biglasso"
# "SL.loess"

################################
# SIMPLE SUBSTITUTION HARD CODED (sample data)
################################

#------------------------------------------
# Estimate Qbar_0(A,W) with Super Learner
#------------------------------------------
# dataframe X with baseline covariates and exposure
X <- subset(little_data,
            select = c(A,
                       get(adjustment_set_variables)))

# set the exposure=1 in X1 and the exposure=0 in X0
X1 <- X0 <- X
X1$A <- 1 # under exposure
X0$A <- 0 # under control

# call Super Learner
QbarSL <- SuperLearner(Y = little_data$Y,
                           X = X,
                           SL.library = katie.SL.library,
                           # cluster = cluster,
                           family = "quasibinomial")
QbarSL
summary(QbarSL)

# get the expected PM2.5, given the observed exposure and covariates
QbarAW <- predict(QbarSL, newdata = little_data)$pred
# expected injury severity, given A=1 and covariates
Qbar1W <- predict(QbarSL, newdata = X1)$pred
# expected injury severity, given A=0 and covariates
Qbar0W <- predict(QbarSL, newdata = X0)$pred
# the fitted value at the observed exposure should equal the fitted value
# under when A = a
tail(data.frame(A = little_data$A, QbarAW, Qbar1W, Qbar0W))

# note the simple substitution estimator would be
PsiHat.SS <- mean(Qbar1W - Qbar0W)

PsiHat.SS



################################
# SIMPLE SUBSTITUTION HARD CODED (full data)
################################

#------------------------------------------
# Estimate Qbar_0(A,W) with Super Learner
#------------------------------------------
# dataframe X with baseline covariates and exposure
X <- subset(ObsData, 
            select = c(A, 
                       get(adjustment_set_variables)))

# set the exposure=1 in X1 and the exposure=0 in X0
X1 <- X0 <- X
X1$A <- 1 # under exposure
X0$A <- 0 # under control

# call Super Learner
QbarSL <- SuperLearner(Y = ObsData$Y, 
                           X = X, 
                           SL.library = katie.SL.library,
                           # cluster = cluster, 
                           family = "binomial")
QbarSL

summary(QbarSL$SL.predict)
length(QbarSL$SL.predict)

# get the expected PM2.5, given the observed exposure and covariates
QbarAW <- predict(QbarSL, newdata = ObsData)$pred
# expected injury severity, given A=1 and covariates
Qbar1W <- predict(QbarSL, newdata = X1)$pred
# expected injury severity, given A=0 and covariates
Qbar0W <- predict(QbarSL, newdata = X0)$pred
# the fitted value at the observed exposure should equal the fitted value
# under when A = a
tail(data.frame(A = ObsData$A, QbarAW, Qbar1W, Qbar0W))

# note the simple substitution estimator would be
PsiHat.SS <- mean(Qbar1W - Qbar0W)

PsiHat.SS


# ################################
# # RAW IPTW HARD CODED (sample data)
# ################################
# 
# # Estimate the exposure mechanism g(A|W)
# #------------------------------------------
# 
# # call Super Learner for the exposure mechanism
# gHatSL <- SuperLearner(Y = little_data$A, 
#                        X = subset(little_data, 
#                                   select = -c(A, Y, id)),
#                        SL.library = katie.SL.library, 
#                        family = "binomial")
# 
# # generate predicted prob being exposed, given baseline covariates
# gHat1W <- gHatSL$SL.predict
# 
# # predicted prob of not being exposed, given baseline covariates
# gHat0W <- 1- gHat1W
# 
# # predicted prob of observed exposure, given baseline cov
# gHatAW <- rep(NA, nrow(little_data))
# gHatAW[little_data$A==1] <- gHat1W[little_data$A==1]
# gHatAW[little_data$A==0] <- gHat0W[little_data$A==0]
# 
# #-------------------------------------------------
# # Clever covariate H(A,W) for each subject
# #-------------------------------------------------
# H.AW <- as.numeric(little_data$A==1)/gHat1W - as.numeric(little_data$A==0)/gHat0W
# 
# # also want to evaluate the clever covariates at A=1 and A=0 for all subjects
# H.1W <- 1/gHat1W
# H.0W <- -1/gHat0W
# 
# # IPTW estimator of the G-computation formula:
# PsiHat.IPTW <- mean(H.AW*little_data$Y)
# 
# ####
# # HERE LIES THE HW ESTIMATOR
# ####
# 
# # weight for HT estimator
# wt <- 1/gHatAW
# 
# # calculate HT stabilized estimator
# PsiHat.HW.IPTW <- mean(wt*as.numeric(little_data$A==1)*little_data$Y)/
#   mean(wt*as.numeric(little_data$A==1)) -
#   mean(wt*as.numeric(little_data$A==0)*little_data$Y)/
#   mean(wt*as.numeric(litte_data$A==0))
# 
# # report it
# PsiHat.HW.IPTW






#################################
# RAW IPTW HARD CODED (full data)
#################################

# Estimate the exposure mechanism g(A|W)
#------------------------------------------

# call Super Learner for the exposure mechanism
gHatSL <- SuperLearner(Y = ObsData$A, 
                       X = subset(ObsData, 
                                  select = -c(A, Y, id)),
                       SL.library = katie.SL.library, 
                       family = "binomial")

# generate predicted prob being exposed, given baseline covariates
gHat1W <- gHatSL$SL.predict

# predicted prob of not being exposed, given baseline covariates
gHat0W <- 1- gHat1W

# predicted prob of observed exposure, given baseline cov
gHatAW <- rep(NA, nrow(ObsData))
gHatAW[ObsData$A==1] <- gHat1W[ObsData$A==1]
gHatAW[ObsData$A==0] <- gHat0W[ObsData$A==0]

#-------------------------------------------------
# Clever covariate H(A,W) for each subject
#-------------------------------------------------
H.AW <- as.numeric(ObsData$A==1)/gHat1W - as.numeric(ObsData$A==0)/gHat0W

# also want to evaluate the clever covariates at A=1 and A=0 for all subjects
H.1W<- 1/gHat1W
H.0W<- -1/gHat0W

# IPTW estimator of the G-computation formula:
PsiHat.IPTW <- mean(H.AW*ObsData$Y)
PsiHat.IPTW

####
# HERE LIES THE HW ESTIMATOR
####

# weight for HT estimator
wt <- 1/gHatAW

# calculate HT stabilized estimator
PsiHat.HW.IPTW <- mean(wt*as.numeric(ObsData$A==1)*ObsData$Y)/
  mean(wt*as.numeric(ObsData$A==1)) -
  mean(wt*as.numeric(ObsData$A==0)*ObsData$Y)/
  mean(wt*as.numeric(ObsData$A==0))

# report it
PsiHat.HW.IPTW

# ###################################
# # RAW TMLE HARD CODED (sample data)
# ###################################
# # MUST RUN IPTW CODE FIRST
# 
# #------------------------------------------
# # Update the initial estimator of Qbar_0(A,W)
# #------------------------------------------
# logitUpdate <- glm(little_data$Y ~ -1 + offset(qlogis(QbarAW)) + H.AW,
#                    family='binomial')
# epsilon <- logitUpdate$coef
# 
# QbarAW.star <- plogis(qlogis(QbarAW)+ epsilon*H.AW)
# Qbar1W.star <- plogis(qlogis(Qbar1W)+ epsilon*H.1W)
# Qbar0W.star <- plogis(qlogis(Qbar0W)+ epsilon*H.0W)
# 
# #------------------------------------------
# # Estimate Psi(P_0)
# #------------------------------------------
# 
# PsiHat.TMLE <- mean(Qbar1W.star - Qbar0W.star)
# 
# #------------------------------------------
# # Return point estimates, targeted estimates of Qbar_0(A,W),
# # and thevector of clever covariates
# #------------------------------------------
# 
# estimates <- data.frame(cbind(PsiHat.SS=PsiHat.SS, 
#                               PsiHat.IPTW, 
#                               PsiHat.TMLE))
# Qbar.star <- data.frame(cbind(QbarAW.star, 
#                               Qbar1W.star, 
#                               Qbar0W.star))
# names(Qbar.star)<- c('QbarAW.star', 
#                      'Qbar1W.star', 
#                      'Qbar0W.star')
# 
# PsiHat.TMLE <- mean(Qbar1W.star - Qbar0W.star)
# 
# #------------------------------------------
# # Return point estimates, targeted estimates of Qbar_0(A,W),
# # and thevector of clever covariates
# #------------------------------------------
# 
# estimates <- data.frame(cbind(PsiHat.SS,
#                               PsiHat.IPTW, 
#                               PsiHat.TMLE))
# 
# Qbar.star <- data.frame(cbind(QbarAW.star, 
#                               Qbar1W.star, 
#                               Qbar0W.star))
# names(Qbar.star) <- c('QbarAW.star', 
#                       'Qbar1W.star', 
#                       'Qbar0W.star')
# 
# list(estimates = estimates, 
#      Qbar.star = Qbar.star, 
#      H.AW = H.AW)


###################################
# RAW TMLE HARD CODED (full data)
###################################
# MUST RUN IPTW CODE FIRST

#------------------------------------------
# Update the initial estimator of Qbar_0(A,W)
#------------------------------------------
logitUpdate <- glm(ObsData$Y ~ -1 + offset(qlogis(QbarAW)) + H.AW,
                   family='binomial')
epsilon <- logitUpdate$coef

QbarAW.star <- plogis(qlogis(QbarAW)+ epsilon*H.AW)
Qbar1W.star <- plogis(qlogis(Qbar1W)+ epsilon*H.1W)
Qbar0W.star <- plogis(qlogis(Qbar0W)+ epsilon*H.0W)

#------------------------------------------
# Estimate Psi(P_0)
#------------------------------------------

PsiHat.TMLE <- mean(Qbar1W.star - Qbar0W.star)
PsiHat.TMLE

#------------------------------------------
# Return point estimates, targeted estimates of Qbar_0(A,W),
# and thevector of clever covariates
#------------------------------------------

estimates <- data.frame(cbind(PsiHat.SS=PsiHat.SS, 
                              PsiHat.IPTW, 
                              PsiHat.TMLE))
Qbar.star <- data.frame(cbind(QbarAW.star, 
                              Qbar1W.star, 
                              Qbar0W.star))
names(Qbar.star)<- c('QbarAW.star', 
                     'Qbar1W.star', 
                     'Qbar0W.star')

PsiHat.TMLE <- mean(Qbar1W.star - Qbar0W.star)
PsiHat.TMLE

#------------------------------------------
# Return point estimates, targeted estimates of Qbar_0(A,W),
# and thevector of clever covariates
#------------------------------------------

estimates <- data.frame(cbind(PsiHat.SS,
                              PsiHat.IPTW, 
                              PsiHat.HW.IPTW,
                              PsiHat.TMLE))

Qbar.star <- data.frame(cbind(QbarAW.star, 
                              Qbar1W.star, 
                              Qbar0W.star))

names(Qbar.star) <- c('QbarAW.star', 
                      'Qbar1W.star', 
                      'Qbar0W.star')

estimates




``` 


```{r KATIE sad wrapper failures}

###################
# CUSTOM WRAPPERS OF AWESOME
###################

SL.glm.complex <- function(Y, X, newX, family, ...) {
  
  fit.glm <- glm(Y ~ 
                   A + 
                   air_basin_4_bins*frpm_percent +  
                   asian_filipino_percent + 
                   two_none_aian_pi_af_am_percent + 
                   hisp_latinx_percent +
                   english_learner_percent + 
                   district_cost_per_pupil*frpm_percent + 
                   district_cost_per_pupil*cred_percent*student_teacher_ratio + 
                   two_none_aian_pi_af_am_percent*frpm_percent*cred_percent*hisp_latinx_percent, 
                 data = X, 
                 family = family)
  
  pred <- predict.glm(fit.glm, newdata = newX, type = 'response')
    
  fit <- list(object = fit.glm)
  
  out <- list(pred = pred, fit = fit)
  
  class(out$fit) <- c('SL.glm')
  
  return(out)
}

SL.polyspline.adapt <- function (Y, X, newX, family, obsWeights, ...) 
{
        fit.mars <- polspline::polymars(Y, X, weights = obsWeights)
        pred <- predict(fit.mars, x = newX)
        fit <- list(object = fit.mars)

    out <- list(pred = pred, fit = fit)
    class(out$fit) <- c("SL.polymars")
    return(out)
}

SL.mean.2 <- function (Y, X, newX, family, obsWeights, id, ...) 
{
    meanY <- weighted.mean(Y, w = obsWeights)
    pred <- rep.int(meanY, times = nrow(newX))
    fit <- list(object = meanY)
    out <- list(pred = pred, fit = fit)
    class(out$fit) <- c("SL.glm")
    return(out)
}



```



```{r KATIE make function to run them all}

###################
# FUNCTION OF ALL 3
###################

set.seed(1)

load(file = file.path("dataset_complete_cases.rdata"))

dataset_complete_cases_working <- 
  dataset_complete_cases %>% 
  mutate(math_proportion = math_standard_met/100,
         pm25_12_plus = ifelse(pm25_12_plus == "yes", 1, 0),
         air_basin_4_bins = factor(air_basin_4_bins))

adjustment_set_variables <- 
  c("air_basin_4_bins", 
    "asian_filipino_percent", 
    "two_none_aian_pi_af_am_percent",
    "hisp_latinx_percent", 
    "english_learner_percent", 
    "frpm_percent",
    "district_cost_per_pupil", 
    "student_teacher_ratio", 
    "cred_percent",
    "school_k_12_enrollment")

ObsData <- 
  dataset_complete_cases_working %>% 
  dplyr::select(Y = math_proportion,
                A = pm25_12_plus, 
                all_of(adjustment_set_variables))

ObsData <- as.data.frame(ObsData)

###################
# CUSTOM WRAPPERS OF AWESOME
###################

SL.glm.complex <- function(Y, X, newX, family, ...) {
  
  fit.glm <- glm(Y ~ 
                   A + 
                   air_basin_4_bins +  
                   asian_filipino_percent + 
                   two_none_aian_pi_af_am_percent + 
                   hisp_latinx_percent +
                   english_learner_percent*frpm_percent + 
                   district_cost_per_pupil*frpm_percent + 
                   cred_percent*student_teacher_ratio + 
                   two_none_aian_pi_af_am_percent*frpm_percent*cred_percent*hisp_latinx_percent, 
                 data = X, 
                 family = family)
  
  pred <- predict.glm(fit.glm, newdata = newX, type = 'response')
    
  fit <- list(object = fit.glm)
  
  out <- list(pred = pred, fit = fit)
  
  class(out$fit) <- c('SL.glm')
  
  return(out)
}

# specify SL library
# listWrappers()
Q.SL.library <- c("SL.mean", 
                  "SL.glm", 
                  "SL.glm.interaction", 
                  "SL.bayesglm", 
                  "SL.ranger", 
                  "SL.glm.complex")
                      # "SL.rpartPrune") 
                      # "SL.polymars", 
                      # "SL.rpartPrune")
                      # "SL.biglasso"
                      # "SL.extraTrees"
                      # "SL.glmnet"
                      # new.algorithm.2$names
                      # SL.glm.complex

g.SL.library <- c("SL.mean", 
                  "SL.glm", 
                  "SL.glm.interaction", 
                  "SL.bayesglm")

# #####################
# # LITTLE SUPERLEARNER FOR TESTING
# #####################
# 
# # subset of rows
# little_data <- ObsData[sample(x = 1:nrow(ObsData),
#                               size = 500,
#                               replace = F),]
# 
# # number of rows
# n = nrow(little_data)
# 
# # id to null
# id = NULL
# 
# # dataframe X with baseline covariates and exposure but not outcome
# X <- little_data[,names(little_data) %in% c(adjustment_set_variables, "A")]
# 
# # set the exposure=1 in X1 and the exposure=0 in X0
# X1 <- X0 <- X
# X1$A <- 1 # under exposure
# X0$A <- 0 # under control
#   
# # call Super Learner
# QbarSL <- SuperLearner(Y = little_data$Y, 
#                          X = X, 
#                          SL.library = Q.SL.library,
#                          # cluster = cluster, 
#                          family = "quasibinomial", 
#                          id = id)
# 
# head(QbarSL$library.predict)
# 
# # get the expected PM2.5, given the observed exposure and covariates
# QbarAW <- predict(QbarSL, newdata = ObsData)$pred
# # expected injury severity, given A=1 and covariates
# Qbar1W <- predict(QbarSL, newdata = X1)$pred
# # expected injury severity, given A=0 and covariates
# Qbar0W <- predict(QbarSL, newdata = X0)$pred
# # # the fitted value at the observed exposure should equal the fitted value
# # # under when A = a
# # tail(data.frame(A = ObsData$A, QbarAW, Qbar1W, Qbar0W))
#   
# # note the simple substitution estimator would be
# PsiHat.SS <- mean(Qbar1W - Qbar0W)
#   
# PsiHat.SS


################################
# HERE LIES THE FUNCTION
################################

run.tmle.katie <- function(ObsData, 
                           Q.SL.library, 
                           g.SL.library,
                           adjustment_set, 
                           id = NULL){

  ################################
  # SIMPLE SUBSTITUTION HARD CODED (full data)
  ################################
  
  #------------------------------------------
  # Estimate Qbar_0(A,W) with Super Learner
  #------------------------------------------
  # dataframe X with baseline covariates and exposure but not outcome
  X <- ObsData[,names(ObsData) %in% c(adjustment_set_variables, "A")]
  
  # set the exposure=1 in X1 and the exposure=0 in X0
  X1 <- X0 <- X
  X1$A <- 1 # under exposure
  X0$A <- 0 # under control
  
  # call Super Learner
  QbarSL <- SuperLearner(Y = ObsData$Y, 
                         X = X, 
                         SL.library = Q.SL.library,
                         # cluster = cluster, 
                         family = "quasibinomial", 
                         id = id)
  
  # get the expected PM2.5, given the observed exposure and covariates
  QbarAW <- predict(QbarSL, newdata = ObsData)$pred
  # expected injury severity, given A=1 and covariates
  Qbar1W <- predict(QbarSL, newdata = X1)$pred
  # expected injury severity, given A=0 and covariates
  Qbar0W <- predict(QbarSL, newdata = X0)$pred
  # # the fitted value at the observed exposure should equal the fitted value
  # # under when A = a
  # tail(data.frame(A = ObsData$A, QbarAW, Qbar1W, Qbar0W))
  
  # note the simple substitution estimator would be
  PsiHat.SS <- mean(Qbar1W - Qbar0W)
  
  PsiHat.SS
  
  
  #################################
  # RAW IPTW HARD CODED (full data)
  #################################
  
  # Estimate the exposure mechanism g(A|W)
  #------------------------------------------
  
  # call Super Learner for the exposure mechanism
  gHatSL <- SuperLearner(Y = ObsData$A, 
                         X = subset(ObsData, 
                                    select = -c(A, Y, id)),
                         SL.library = g.SL.library, 
                         family = "binomial", 
                         id = id)
  
  # generate predicted prob being exposed, given baseline covariates
  gHat1W <- gHatSL$SL.predict
  
  # predicted prob of not being exposed, given baseline covariates
  gHat0W <- 1- gHat1W
  
  # predicted prob of observed exposure, given baseline cov
  gHatAW <- rep(NA, nrow(ObsData))
  gHatAW[ObsData$A==1] <- gHat1W[ObsData$A==1]
  gHatAW[ObsData$A==0] <- gHat0W[ObsData$A==0]
  
  #-------------------------------------------------
  # Clever covariate H(A,W) for each subject
  #-------------------------------------------------
  H.AW <- as.numeric(ObsData$A==1)/gHat1W - as.numeric(ObsData$A==0)/gHat0W
  
  # also want to evaluate the clever covariates at A=1 and A=0 for all subjects
  H.1W<- 1/gHat1W
  H.0W<- -1/gHat0W
  
  # IPTW estimator of the G-computation formula:
  PsiHat.IPTW <- mean(H.AW*ObsData$Y)
  PsiHat.IPTW
  
  ############################
  # HERE LIES THE HW ESTIMATOR
  ############################
  
  # weight for HT estimator
  wt <- 1/gHatAW
  
  # calculate HT stabilized estimator
  PsiHat.HW.IPTW <- mean(wt*as.numeric(ObsData$A==1)*ObsData$Y)/
    mean(wt*as.numeric(ObsData$A==1)) -
    mean(wt*as.numeric(ObsData$A==0)*ObsData$Y)/
    mean(wt*as.numeric(ObsData$A==0))
  
  # report it
  PsiHat.HW.IPTW
  
  ###################################
  # RAW TMLE HARD CODED (full data)
  ###################################
  # MUST RUN IPTW CODE FIRST
  
  #------------------------------------------
  # Update the initial estimator of Qbar_0(A,W)
  #------------------------------------------
  logitUpdate <- glm(ObsData$Y ~ -1 + offset(qlogis(QbarAW)) + H.AW,
                     family='quasibinomial')
  epsilon <- logitUpdate$coef
  
  QbarAW.star <- plogis(qlogis(QbarAW)+ epsilon*H.AW)
  Qbar1W.star <- plogis(qlogis(Qbar1W)+ epsilon*H.1W)
  Qbar0W.star <- plogis(qlogis(Qbar0W)+ epsilon*H.0W)
  
  #------------------------------------------
  # Estimate Psi(P_0)
  #------------------------------------------
  
  PsiHat.TMLE <- mean(Qbar1W.star - Qbar0W.star)
  PsiHat.TMLE
  
  ############################
  # HERE LIE VARIOUS STUPIDLY TRUNCATED ESTIMATORS
  ############################
  
  ######
  # iptw trun
  ######
  
  # bound gn at 0.025, 0.975
  gHatAW_025_975 <- gHatAW
  gHatAW_025_975[gHatAW_025_975 > 0.975] <- 0.975
  gHatAW_025_975[gHatAW_025_975 < 0.025] <- 0.025
  
  # bound gn at 0.05, 0.95
  gHatAW_05_95 <- gHatAW
  gHatAW_05_95[gHatAW_05_95 > 0.95] <- 0.95
  gHatAW_05_95[gHatAW_05_95 < 0.05] <- 0.05
  
  # bound gn at 0.10, 0.90
  gHatAW_10_90 <- gHatAW
  gHatAW_10_90[gHatAW_10_90 > 0.90] <- 0.90
  gHatAW_10_90[gHatAW_10_90 < 0.10] <- 0.10
  
  # truncate the weights
  wt_trunc_025_975 <- 1/gHatAW_025_975
  wt_trunc_05_95 <- 1/gHatAW_05_95
  wt_trunc_10_90 <- 1/gHatAW_10_90
  
  # calculate IPTW!
  PsiHat.IPTW.025.975 <- 
    mean(wt_trunc_025_975*as.numeric(ObsData$A==1)*ObsData$Y) -
    mean(wt_trunc_025_975*as.numeric(ObsData$A==0)*ObsData$Y)
  
  PsiHat.IPTW.05.95 <- 
    mean(wt_trunc_05_95*as.numeric(ObsData$A==1)*ObsData$Y) -
    mean(wt_trunc_05_95*as.numeric(ObsData$A==0)*ObsData$Y)
  
  PsiHat.IPTW.10.90 <- 
    mean(wt_trunc_10_90*as.numeric(ObsData$A==1)*ObsData$Y) -
    mean(wt_trunc_10_90*as.numeric(ObsData$A==0)*ObsData$Y)
  
  ######
  # tmle trunc
  ######
  
  # bound gn at 0.025, 0.975
  gHat0W_025_975 <- gHat0W
  gHat0W_025_975[gHat0W_025_975 > 0.975] <- 0.975
  gHat0W_025_975[gHat0W_025_975 < 0.025] <- 0.025
  
  gHat1W_025_975 <- gHat1W
  gHat1W_025_975[gHat1W_025_975 > 0.975] <- 0.975
  gHat1W_025_975[gHat1W_025_975 < 0.025] <- 0.025
  
  # bound gn at 0.05, 0.95
  gHat0W_05_95 <- gHat0W
  gHat0W_05_95[gHat0W_05_95 > 0.95] <- 0.95
  gHat0W_05_95[gHat0W_05_95 < 0.05] <- 0.05
  
  gHat1W_05_95 <- gHat1W
  gHat1W_05_95[gHat1W_05_95 > 0.95] <- 0.95
  gHat1W_05_95[gHat1W_05_95 < 0.05] <- 0.05
  
  # bound gn at 0.10, 0.90
  gHat0W_10_90 <- gHat0W
  gHat0W_10_90[gHat0W_10_90 > 0.90] <- 0.90
  gHat0W_10_90[gHat0W_10_90 < 0.10] <- 0.10
  
  gHat1W_10_90 <- gHat1W
  gHat1W_10_90[gHat1W_10_90 > 0.90] <- 0.90
  gHat1W_10_90[gHat1W_10_90 < 0.10] <- 0.10
  
  # weighted clever covariates
  
  # 0.025
  H.AW_025_975 <- as.numeric(ObsData$A==1)/gHat1W_025_975 - 
    as.numeric(ObsData$A==0)/gHat0W_025_975
  
  # also want to evaluate the clever covariates at A=1 and A=0 for all subjects
  H.1W_025_975<- 1/gHat1W_025_975
  H.0W_025_975<- -1/gHat0W_025_975
  
  # 0.05
  H.AW_05_95 <- as.numeric(ObsData$A==1)/gHat1W_05_95 - 
  as.numeric(ObsData$A==0)/gHat0W_05_95
  
  H.1W_05_95 <- 1/gHat1W_05_95
  H.0W_05_95 <- -1/gHat0W_05_95
  
  # 0.10
  H.AW_10_90 <- as.numeric(ObsData$A==1)/gHat1W_10_90 - 
  as.numeric(ObsData$A==0)/gHat0W_10_90
    
  H.1W_10_90 <- 1/gHat1W_10_90
  H.0W_10_90 <- -1/gHat0W_10_90
  
  # Update the initial estimator of Qbar_0(A,W)

  # 0.025!
  logitUpdate_025_975 <- glm(ObsData$Y ~ -1 + offset(qlogis(QbarAW)) + H.AW_025_975,
                     family='quasibinomial')
  epsilon_025_975 <- logitUpdate_025_975$coef
  
  QbarAW_025_975.star <- plogis(qlogis(QbarAW)+ epsilon_025_975*H.AW_025_975)
  Qbar1W_025_975.star <- plogis(qlogis(Qbar1W)+ epsilon_025_975*H.1W_025_975)
  Qbar0W_025_975.star <- plogis(qlogis(Qbar0W)+ epsilon_025_975*H.0W_025_975)
  
  # 0.05!
  logitUpdate_05_95 <- glm(ObsData$Y ~ -1 + offset(qlogis(QbarAW)) + H.AW_05_95,
                     family='quasibinomial')
  epsilon_05_95 <- logitUpdate_05_95$coef
  
  QbarAW_05_95.star <- plogis(qlogis(QbarAW)+ epsilon_05_95*H.AW_05_95)
  Qbar1W_05_95.star <- plogis(qlogis(Qbar1W)+ epsilon_05_95*H.1W_05_95)
  Qbar0W_05_95.star <- plogis(qlogis(Qbar0W)+ epsilon_05_95*H.0W_05_95)
  
  # 0.10!
  logitUpdate_10_90 <- glm(ObsData$Y ~ -1 + offset(qlogis(QbarAW)) + H.AW_10_90,
                     family='quasibinomial')
  epsilon_10_90 <- logitUpdate_10_90$coef
  
  QbarAW_10_90.star <- plogis(qlogis(QbarAW)+ epsilon*H.AW_10_90)
  Qbar1W_10_90.star <- plogis(qlogis(Qbar1W)+ epsilon*H.1W_10_90)
  Qbar0W_10_90.star <- plogis(qlogis(Qbar0W)+ epsilon*H.0W_10_90)
  
  #------------------------------------------
  # Estimate Psi(P_0)
  #------------------------------------------
  
  PsiHat.TMLE_trunc_025_975 <- mean(Qbar1W_025_975.star - Qbar0W_025_975.star)
  PsiHat.TMLE_trunc_05_95 <- mean(Qbar1W_05_95.star - Qbar0W_05_95.star)
  PsiHat.TMLE_trunc_10_90 <- mean(Qbar1W_10_90.star - Qbar0W_10_90.star)
  
  #------------------------------------------
  # Return point estimates, targeted estimates of Qbar_0(A,W),
  # and the vector of clever covariates
  #------------------------------------------
  
  estimates <- data.frame(cbind(PsiHat.SS=PsiHat.SS, 
                                PsiHat.IPTW, 
                                PsiHat.HW.IPTW,
                                PsiHat.TMLE, 
                                PsiHat.IPTW.025.975, 
                                PsiHat.IPTW.05.95, 
                                PsiHat.IPTW.10.90, 
                                PsiHat.TMLE_trunc_025_975, 
                                PsiHat.TMLE_trunc_05_95, 
                                PsiHat.TMLE_trunc_10_90
                                ))
  
  Qbar.star <- data.frame(cbind(QbarAW.star, 
                                Qbar1W.star, 
                                Qbar0W.star, 
                                QbarAW_025_975.star, 
                                Qbar1W_025_975.star, 
                                Qbar0W_025_975.star,
                                QbarAW_05_95.star, 
                                Qbar1W_05_95.star, 
                                Qbar0W_05_95.star, 
                                QbarAW_10_90.star, 
                                Qbar1W_10_90.star, 
                                Qbar0W_10_90.star))
  
  names(Qbar.star) <- c('QbarAW.star', 
                        'Qbar1W.star', 
                        'Qbar0W.star',
                        'QbarAW_025_975.star', 
                        'Qbar1W_025_975.star', 
                        'Qbar0W_025_975.star',
                        'QbarAW_05_95.star', 
                        'Qbar1W_05_95.star', 
                        'Qbar0W_05_95.star', 
                        'QbarAW_10_90.star', 
                        'Qbar1W_10_90.star', 
                        'Qbar0W_10_90.star')
  
  H.AWs <- c(H.AW, 
             H.AW_025_975, 
             H.AW_05_95, 
             H.AW_10_90)
  
  names(H.AWs) <- c("H.AW", 
             "H.AW_025_975", 
             "H.AW_05_95", 
             "H.AW_10_90")
  
  list(estimates = estimates, 
       Qbar.star = Qbar.star, 
       H.AWs = H.AWs, 
       QbarSL = QbarSL, 
       gHatSL = gHatSL)
  
}

save(run.tmle.katie, file = file.path("run.tmle.katie.rdata"))

########
# TEST
########

# subset of rows
little_data <- ObsData[sample(x = 1:nrow(ObsData),
                              size = 500,
                              replace = F),]

# number of rows
n = nrow(little_data)

# function little output
out <- run.tmle.katie(ObsData = little_data,
                      adjustment_set = adjustment_set_variables,
                      Q.SL.library = Q.SL.library, 
                      g.SL.library = g.SL.library)

est <- out$estimates

out$Qbar.star

est*100

Qfit <- out$QbarSL

Gfit <- out$gHatSL


```

```{r KATIE here lies the parallelized function}

  # set up parallelization (my computer has 8 cores and this uses all of them)
(cluster = parallel::makeCluster(8))
parallel::clusterEvalQ(cluster, library(SuperLearner))

################################
# HERE LIES THE FUNCTION
################################

run.tmle.katie.para <- function(ObsData, 
                           Q.SL.library, 
                           g.SL.library,
                           cluster,
                           adjustment_set, 
                           id = NULL){
  
  ################################
  # SIMPLE SUBSTITUTION HARD CODED (full data)
  ################################
  
  #------------------------------------------
  # Estimate Qbar_0(A,W) with Super Learner
  #------------------------------------------
  # dataframe X with baseline covariates and exposure but not outcome
  X <- ObsData[,names(ObsData) %in% c(adjustment_set_variables, "A")]
  
  # set the exposure=1 in X1 and the exposure=0 in X0
  X1 <- X0 <- X
  X1$A <- 1 # under exposure
  X0$A <- 0 # under control
  
  # call Super Learner
  QbarSL <- snowSuperLearner(Y = ObsData$Y, 
                         X = X, 
                         SL.library = Q.SL.library,
                         cluster = cluster, 
                         family = "quasibinomial", 
                         id = id)
  
  # get the expected PM2.5, given the observed exposure and covariates
  QbarAW <- predict(QbarSL, newdata = ObsData)$pred
  # expected injury severity, given A=1 and covariates
  Qbar1W <- predict(QbarSL, newdata = X1)$pred
  # expected injury severity, given A=0 and covariates
  Qbar0W <- predict(QbarSL, newdata = X0)$pred
  # # the fitted value at the observed exposure should equal the fitted value
  # # under when A = a
  # tail(data.frame(A = ObsData$A, QbarAW, Qbar1W, Qbar0W))
  
  # note the simple substitution estimator would be
  PsiHat.SS <- mean(Qbar1W - Qbar0W)
  
  PsiHat.SS
  
  
  #################################
  # RAW IPTW HARD CODED (full data)
  #################################
  
  # Estimate the exposure mechanism g(A|W)
  #------------------------------------------
  
  # call Super Learner for the exposure mechanism
  gHatSL <- snowSuperLearner(Y = ObsData$A, 
                         X = subset(ObsData, 
                                    select = -c(A, Y, id)),
                         SL.library = g.SL.library, 
                         family = "binomial", 
                         cluster = cluster,
                         id = id)
  
  # generate predicted prob being exposed, given baseline covariates
  gHat1W <- gHatSL$SL.predict
  
  # predicted prob of not being exposed, given baseline covariates
  gHat0W <- 1- gHat1W
  
  # predicted prob of observed exposure, given baseline cov
  gHatAW <- rep(NA, nrow(ObsData))
  gHatAW[ObsData$A==1] <- gHat1W[ObsData$A==1]
  gHatAW[ObsData$A==0] <- gHat0W[ObsData$A==0]
  
  #-------------------------------------------------
  # Clever covariate H(A,W) for each subject
  #-------------------------------------------------
  H.AW <- as.numeric(ObsData$A==1)/gHat1W - as.numeric(ObsData$A==0)/gHat0W
  
  # also want to evaluate the clever covariates at A=1 and A=0 for all subjects
  H.1W<- 1/gHat1W
  H.0W<- -1/gHat0W
  
  # IPTW estimator of the G-computation formula:
  PsiHat.IPTW <- mean(H.AW*ObsData$Y)
  PsiHat.IPTW
  
  ############################
  # HERE LIES THE HW ESTIMATOR
  ############################
  
  # weight for HT estimator
  wt <- 1/gHatAW
  
  # calculate HT stabilized estimator
  PsiHat.HW.IPTW <- mean(wt*as.numeric(ObsData$A==1)*ObsData$Y)/
    mean(wt*as.numeric(ObsData$A==1)) -
    mean(wt*as.numeric(ObsData$A==0)*ObsData$Y)/
    mean(wt*as.numeric(ObsData$A==0))
  
  # report it
  PsiHat.HW.IPTW
  
  ###################################
  # RAW TMLE HARD CODED (full data)
  ###################################
  # MUST RUN IPTW CODE FIRST
  
  #------------------------------------------
  # Update the initial estimator of Qbar_0(A,W)
  #------------------------------------------
  logitUpdate <- glm(ObsData$Y ~ -1 + offset(qlogis(QbarAW)) + H.AW,
                     family='quasibinomial')
  epsilon <- logitUpdate$coef
  
  QbarAW.star <- plogis(qlogis(QbarAW)+ epsilon*H.AW)
  Qbar1W.star <- plogis(qlogis(Qbar1W)+ epsilon*H.1W)
  Qbar0W.star <- plogis(qlogis(Qbar0W)+ epsilon*H.0W)
  
  #------------------------------------------
  # Estimate Psi(P_0)
  #------------------------------------------
  
  PsiHat.TMLE <- mean(Qbar1W.star - Qbar0W.star)
  PsiHat.TMLE
  
  ############################
  # HERE LIE VARIOUS STUPIDLY TRUNCATED ESTIMATORS
  ############################
  
  ######
  # iptw trun
  ######
  
  # bound gn at 0.025, 0.975
  gHatAW_025_975 <- gHatAW
  gHatAW_025_975[gHatAW_025_975 > 0.975] <- 0.975
  gHatAW_025_975[gHatAW_025_975 < 0.025] <- 0.025
  
  # bound gn at 0.05, 0.95
  gHatAW_05_95 <- gHatAW
  gHatAW_05_95[gHatAW_05_95 > 0.95] <- 0.95
  gHatAW_05_95[gHatAW_05_95 < 0.05] <- 0.05
  
  # bound gn at 0.10, 0.90
  gHatAW_10_90 <- gHatAW
  gHatAW_10_90[gHatAW_10_90 > 0.90] <- 0.90
  gHatAW_10_90[gHatAW_10_90 < 0.10] <- 0.10
  
  # truncate the weights
  wt_trunc_025_975 <- 1/gHatAW_025_975
  wt_trunc_05_95 <- 1/gHatAW_05_95
  wt_trunc_10_90 <- 1/gHatAW_10_90
  
  # calculate IPTW!
  PsiHat.IPTW.025.975 <- 
    mean(wt_trunc_025_975*as.numeric(ObsData$A==1)*ObsData$Y) -
    mean(wt_trunc_025_975*as.numeric(ObsData$A==0)*ObsData$Y)
  
  PsiHat.IPTW.05.95 <- 
    mean(wt_trunc_05_95*as.numeric(ObsData$A==1)*ObsData$Y) -
    mean(wt_trunc_05_95*as.numeric(ObsData$A==0)*ObsData$Y)
  
  PsiHat.IPTW.10.90 <- 
    mean(wt_trunc_10_90*as.numeric(ObsData$A==1)*ObsData$Y) -
    mean(wt_trunc_10_90*as.numeric(ObsData$A==0)*ObsData$Y)
  
  ######
  # tmle trunc
  ######
  
  # bound gn at 0.025, 0.975
  gHat0W_025_975 <- gHat0W
  gHat0W_025_975[gHat0W_025_975 > 0.975] <- 0.975
  gHat0W_025_975[gHat0W_025_975 < 0.025] <- 0.025
  
  gHat1W_025_975 <- gHat1W
  gHat1W_025_975[gHat1W_025_975 > 0.975] <- 0.975
  gHat1W_025_975[gHat1W_025_975 < 0.025] <- 0.025
  
  # bound gn at 0.05, 0.95
  gHat0W_05_95 <- gHat0W
  gHat0W_05_95[gHat0W_05_95 > 0.95] <- 0.95
  gHat0W_05_95[gHat0W_05_95 < 0.05] <- 0.05
  
  gHat1W_05_95 <- gHat1W
  gHat1W_05_95[gHat1W_05_95 > 0.95] <- 0.95
  gHat1W_05_95[gHat1W_05_95 < 0.05] <- 0.05
  
  # bound gn at 0.10, 0.90
  gHat0W_10_90 <- gHat0W
  gHat0W_10_90[gHat0W_10_90 > 0.90] <- 0.90
  gHat0W_10_90[gHat0W_10_90 < 0.10] <- 0.10
  
  gHat1W_10_90 <- gHat1W
  gHat1W_10_90[gHat1W_10_90 > 0.90] <- 0.90
  gHat1W_10_90[gHat1W_10_90 < 0.10] <- 0.10
  
  # weighted clever covariates
  
  # 0.025
  H.AW_025_975 <- as.numeric(ObsData$A==1)/gHat1W_025_975 - 
    as.numeric(ObsData$A==0)/gHat0W_025_975
  
  # also want to evaluate the clever covariates at A=1 and A=0 for all subjects
  H.1W_025_975<- 1/gHat1W_025_975
  H.0W_025_975<- -1/gHat0W_025_975
  
  # 0.05
  H.AW_05_95 <- as.numeric(ObsData$A==1)/gHat1W_05_95 - 
  as.numeric(ObsData$A==0)/gHat0W_05_95
  
  H.1W_05_95 <- 1/gHat1W_05_95
  H.0W_05_95 <- -1/gHat0W_05_95
  
  # 0.10
  H.AW_10_90 <- as.numeric(ObsData$A==1)/gHat1W_10_90 - 
  as.numeric(ObsData$A==0)/gHat0W_10_90
    
  H.1W_10_90 <- 1/gHat1W_10_90
  H.0W_10_90 <- -1/gHat0W_10_90
  
  # Update the initial estimator of Qbar_0(A,W)

  # 0.025!
  logitUpdate_025_975 <- glm(ObsData$Y ~ -1 + offset(qlogis(QbarAW)) + H.AW_025_975,
                     family='quasibinomial')
  epsilon_025_975 <- logitUpdate_025_975$coef
  
  QbarAW_025_975.star <- plogis(qlogis(QbarAW)+ epsilon_025_975*H.AW_025_975)
  Qbar1W_025_975.star <- plogis(qlogis(Qbar1W)+ epsilon_025_975*H.1W_025_975)
  Qbar0W_025_975.star <- plogis(qlogis(Qbar0W)+ epsilon_025_975*H.0W_025_975)
  
  # 0.05!
  logitUpdate_05_95 <- glm(ObsData$Y ~ -1 + offset(qlogis(QbarAW)) + H.AW_05_95,
                     family='quasibinomial')
  epsilon_05_95 <- logitUpdate_05_95$coef
  
  QbarAW_05_95.star <- plogis(qlogis(QbarAW)+ epsilon_05_95*H.AW_05_95)
  Qbar1W_05_95.star <- plogis(qlogis(Qbar1W)+ epsilon_05_95*H.1W_05_95)
  Qbar0W_05_95.star <- plogis(qlogis(Qbar0W)+ epsilon_05_95*H.0W_05_95)
  
  # 0.10!
  logitUpdate_10_90 <- glm(ObsData$Y ~ -1 + offset(qlogis(QbarAW)) + H.AW_10_90,
                     family='quasibinomial')
  epsilon_10_90 <- logitUpdate_10_90$coef
  
  QbarAW_10_90.star <- plogis(qlogis(QbarAW)+ epsilon*H.AW_10_90)
  Qbar1W_10_90.star <- plogis(qlogis(Qbar1W)+ epsilon*H.1W_10_90)
  Qbar0W_10_90.star <- plogis(qlogis(Qbar0W)+ epsilon*H.0W_10_90)
  
  #------------------------------------------
  # Estimate Psi(P_0)
  #------------------------------------------
  
  PsiHat.TMLE_trunc_025_975 <- mean(Qbar1W_025_975.star - Qbar0W_025_975.star)
  PsiHat.TMLE_trunc_05_95 <- mean(Qbar1W_05_95.star - Qbar0W_05_95.star)
  PsiHat.TMLE_trunc_10_90 <- mean(Qbar1W_10_90.star - Qbar0W_10_90.star)
  
  #------------------------------------------
  # Return point estimates, targeted estimates of Qbar_0(A,W),
  # and the vector of clever covariates
  #------------------------------------------
  
  estimates <- data.frame(cbind(PsiHat.SS=PsiHat.SS, 
                                PsiHat.IPTW, 
                                PsiHat.HW.IPTW,
                                PsiHat.TMLE, 
                                PsiHat.IPTW.025.975, 
                                PsiHat.IPTW.05.95, 
                                PsiHat.IPTW.10.90, 
                                PsiHat.TMLE_trunc_025_975, 
                                PsiHat.TMLE_trunc_05_95, 
                                PsiHat.TMLE_trunc_10_90
                                ))
  
  Qbar.star <- data.frame(cbind(QbarAW.star, 
                                Qbar1W.star, 
                                Qbar0W.star, 
                                QbarAW_025_975.star, 
                                Qbar1W_025_975.star, 
                                Qbar0W_025_975.star,
                                QbarAW_05_95.star, 
                                Qbar1W_05_95.star, 
                                Qbar0W_05_95.star, 
                                QbarAW_10_90.star, 
                                Qbar1W_10_90.star, 
                                Qbar0W_10_90.star))
  
  names(Qbar.star) <- c('QbarAW.star', 
                        'Qbar1W.star', 
                        'Qbar0W.star',
                        'QbarAW_025_975.star', 
                        'Qbar1W_025_975.star', 
                        'Qbar0W_025_975.star',
                        'QbarAW_05_95.star', 
                        'Qbar1W_05_95.star', 
                        'Qbar0W_05_95.star', 
                        'QbarAW_10_90.star', 
                        'Qbar1W_10_90.star', 
                        'Qbar0W_10_90.star')
  
  H.AWs <- c(H.AW, 
             H.AW_025_975, 
             H.AW_05_95, 
             H.AW_10_90)
  
  names(H.AWs) <- c("H.AW", 
             "H.AW_025_975", 
             "H.AW_05_95", 
             "H.AW_10_90")
  
  list(estimates = estimates, 
       Qbar.star = Qbar.star, 
       H.AWs = H.AWs, 
       QbarSL = QbarSL, 
       gHatSL = gHatSL)
  
}

save(run.tmle.katie.para, 
     file = file.path("run.tmle.katie.para.rdata"))



```



```{r KATIE bootstrap test}

########################################
# NP-Boot for B=500 bootstrapped samples
########################################

set.seed(1)

load(file = file.path("dataset_complete_cases.rdata"))
load(file = file.path("run.tmle.katie.rdata"))

dataset_complete_cases_working <- 
  dataset_complete_cases %>% 
  mutate(math_proportion = math_standard_met/100,
         pm25_12_plus = ifelse(pm25_12_plus == "yes", 1, 0),
         air_basin_4_bins = factor(air_basin_4_bins))

adjustment_set_variables <- 
  c("air_basin_4_bins", 
    "asian_filipino_percent", 
    "two_none_aian_pi_af_am_percent",
    "hisp_latinx_percent", 
    "english_learner_percent", 
    "frpm_percent",
    "district_cost_per_pupil", 
    "student_teacher_ratio", 
    "cred_percent",
    "school_k_12_enrollment")

ObsData <- 
  dataset_complete_cases_working %>% 
  dplyr::select(Y = math_proportion,
                A = pm25_12_plus, 
                all_of(adjustment_set_variables))

ObsData <- as.data.frame(ObsData)

###################
# CUSTOM WRAPPERS OF AWESOME
###################

SL.glm.complex <- function(Y, X, newX, family, ...) {
  
  fit.glm <- glm(Y ~ 
                   A + 
                   air_basin_4_bins +  
                   asian_filipino_percent + 
                   two_none_aian_pi_af_am_percent + 
                   hisp_latinx_percent +
                   english_learner_percent*frpm_percent + 
                   district_cost_per_pupil*frpm_percent + 
                   cred_percent*student_teacher_ratio + 
                   two_none_aian_pi_af_am_percent*frpm_percent*cred_percent*hisp_latinx_percent, 
                 data = X, 
                 family = family)
  
  pred <- predict.glm(fit.glm, newdata = newX, type = 'response')
    
  fit <- list(object = fit.glm)
  
  out <- list(pred = pred, fit = fit)
  
  class(out$fit) <- c('SL.glm')
  
  return(out)
}

# specify SL libraries
# listWrappers()
Q.SL.library <- c("SL.mean", 
                  "SL.glm", 
                  "SL.glm.interaction", 
                  "SL.bayesglm", 
                  "SL.ranger", 
                  "SL.glm.complex")
                      # "SL.rpartPrune") 
                      # "SL.polymars", 
                      # "SL.rpartPrune")
                      # "SL.biglasso"
                      # "SL.extraTrees"
                      # "SL.glmnet"
                      # new.algorithm.2$names
                      # SL.glm.complex

g.SL.library <- c("SL.mean", 
                  "SL.glm", 
                  "SL.glm.interaction", 
                  "SL.bayesglm")

ObsData <- 
  dataset_complete_cases_working %>% 
  dplyr::select(Y = math_proportion,
                A = pm25_12_plus, 
                all_of(adjustment_set_variables))

ObsData <- as.data.frame(ObsData)

########
# TEST IT
########

# subset of rows
little_data <- ObsData[sample(x = 1:nrow(ObsData),
                              size = 1000,
                              replace = F),]

# number of rows
n = nrow(little_data)

# number of bootstrap samples
B = 2

# data frame for estimates based on the boot strap sample
estimates <- data.frame(matrix(NA, nrow = B, ncol = 10))

# make an id variable
little_data$id <- 1:n

# for loop from b=1 to total number of bootstrap samples
for(b in 1:B){

  # sample the indices 1 to n with replacement
  bootIndices<- sample(1:n, replace = T)
  bootData<- little_data[bootIndices,]
  
  # calling the above function
  estimates[b,]<- run.tmle.katie(ObsData = bootData,
                                 g.SL.library = g.SL.library,
                                 Q.SL.library = Q.SL.library,
                                 id = bootData$id)$estimates

  # keep track of the iterations completed
  print(b)
}

colnames(estimates)<-c("SimpSubs", 
                       "IPTW", 
                       "IPTW.HT", 
                       "TMLE", 
                       "PsiHat.IPTW.025.975", 
                       "PsiHat.IPTW.05.95", 
                       "PsiHat.IPTW.10.90", 
                       "PsiHat.TMLE_trunc_025_975", 
                       "PsiHat.TMLE_trunc_05_95", 
                       "PsiHat.TMLE_trunc_10_90")

save(estimates, file='little_run.Rdata')

summary(estimates)

# #saving the histograms as a pdf
# pdf(file="RLab6_hist_boot.pdf")
# par(mfrow=c(4,1))
# hist(estimates[,1], 
#      main = "Histogram of point estimates from the Simple Substitution estimator over B bootstrapped samples", 
#      xlab = "Point Estimates")
# hist(estimates[,2], 
#      main = "Histogram of point estimates from IPTW HT estimator over B bootstrapped samples", xlab="Point Estimates")
# hist(estimates[,3], main="Histogram of point estimates from IPTW over B bootstrapped samples", xlab="Point Estimates")
# hist(estimates[,4], main="Histogram of point estimates from TMLE over B bootstrapped samples", xlab="Point Estimates")
# dev.off()

```



```{r KATIE bootstrap REAL}

########################################
# NP-Boot for B=500 bootstrapped samples
########################################

set.seed(1)

load(file = file.path("dataset_complete_cases.rdata"))
load(file = file.path("run.tmle.katie.para.rdata"))

dataset_complete_cases_working <- 
  dataset_complete_cases %>% 
  mutate(math_proportion = math_standard_met/100,
         pm25_12_plus = ifelse(pm25_12_plus == "yes", 1, 0),
         air_basin_4_bins = factor(air_basin_4_bins))

adjustment_set_variables <- 
  c("air_basin_4_bins", 
    "asian_filipino_percent", 
    "two_none_aian_pi_af_am_percent",
    "hisp_latinx_percent", 
    "english_learner_percent", 
    "frpm_percent",
    "district_cost_per_pupil", 
    "student_teacher_ratio", 
    "cred_percent",
    "school_k_12_enrollment")

ObsData <- 
  dataset_complete_cases_working %>% 
  dplyr::select(Y = math_proportion,
                A = pm25_12_plus, 
                all_of(adjustment_set_variables))

ObsData <- as.data.frame(ObsData)

###################
# CUSTOM WRAPPERS OF AWESOME
###################

SL.glm.complex <- function(Y, X, newX, family, ...) {
  fit.glm <- glm(Y ~ 
                   A + 
                   air_basin_4_bins +  
                   asian_filipino_percent + 
                   two_none_aian_pi_af_am_percent + 
                   hisp_latinx_percent +
                   english_learner_percent*frpm_percent + 
                   district_cost_per_pupil*frpm_percent + 
                   cred_percent*student_teacher_ratio + 
                   two_none_aian_pi_af_am_percent*frpm_percent*cred_percent*hisp_latinx_percent, 
                 data = X, 
                 family = family)
  pred <- predict.glm(fit.glm, newdata = newX, type = 'response')
  fit <- list(object = fit.glm)
  out <- list(pred = pred, fit = fit)
  class(out$fit) <- c('SL.glm')
  return(out)
}

# set up parallelization (my computer has 8 cores and this uses all of them)
(cluster = parallel::makeCluster(8))
parallel::clusterEvalQ(cluster, 
                       library(SuperLearner))
parallel::clusterExport(cluster, 
                        "SL.glm.complex")

parallel::clusterSetRNGStream(cluster, 1)

# specify SL libraries
# listWrappers()
Q.SL.library <- c("SL.mean", 
                  "SL.glm", 
                  "SL.glm.interaction", 
                  "SL.bayesglm", 
                  "SL.ranger", 
                  "SL.glm.complex")
                      # "SL.rpartPrune") 
                      # "SL.polymars", 
                      # "SL.rpartPrune")
                      # "SL.biglasso"
                      # "SL.extraTrees"
                      # "SL.glmnet"
                      # new.algorithm.2$names
                      # SL.glm.complex

g.SL.library <- c("SL.mean", 
                  "SL.glm", 
                  "SL.glm.interaction", 
                  "SL.bayesglm")

ObsData <- 
  dataset_complete_cases_working %>% 
  dplyr::select(Y = math_proportion,
                A = pm25_12_plus, 
                all_of(adjustment_set_variables))

ObsData <- as.data.frame(ObsData)

# number of rows
n = nrow(ObsData)

# number of bootstrap samples
B = 2

# data frame for estimates based on the boot strap sample
estimates <- data.frame(matrix(NA, nrow = B, ncol = 10))

# make an id variable
ObsData$id <- 1:n

# for loop from b=1 to total number of bootstrap samples
for(b in 1:B){

  # sample the indices 1 to n with replacement
  bootIndices<- sample(1:n, replace = T)
  bootData<- ObsData[bootIndices,]
  
  # calling the above function
  estimates[b,]<- run.tmle.katie.para(ObsData = bootData,
                                 g.SL.library = g.SL.library,
                                 Q.SL.library = Q.SL.library,
                                 cluster = cluster,
                                 id = bootData$id)$estimates

  # keep track of the iterations completed
  print(b)
}

colnames(estimates)<-c("SimpSubs", 
                       "IPTW", 
                       "IPTW.HT", 
                       "TMLE", 
                       "PsiHat.IPTW.025.975", 
                       "PsiHat.IPTW.05.95", 
                       "PsiHat.IPTW.10.90", 
                       "PsiHat.TMLE_trunc_025_975", 
                       "PsiHat.TMLE_trunc_05_95", 
                       "PsiHat.TMLE_trunc_10_90")

# save here CHANGE THIS NAME
save(estimates, file='big_run_one.Rdata')

summary(estimates)

# #saving the histograms as a pdf
# pdf(file="RLab6_hist_boot.pdf")
# par(mfrow=c(4,1))
# hist(estimates[,1], 
#      main = "Histogram of point estimates from the Simple Substitution estimator over B bootstrapped samples", 
#      xlab = "Point Estimates")
# hist(estimates[,2], 
#      main = "Histogram of point estimates from IPTW HT estimator over B bootstrapped samples", xlab="Point Estimates")
# hist(estimates[,3], main="Histogram of point estimates from IPTW over B bootstrapped samples", xlab="Point Estimates")
# hist(estimates[,4], main="Histogram of point estimates from TMLE over B bootstrapped samples", xlab="Point Estimates")
# dev.off()

```



```{r playing with wrappers}

arm::bayesglm(Y ~ ., data = ObsData, family = "binomial")

```



```{r KATIE positivity}

#------------------------------------------
# Playing with LTMLE
#------------------------------------------

# run with ltmle for comparison
ltmle_data <- 
  dataset_complete_cases_working %>% 
  mutate(id_cds = as.numeric(as.factor(cds_code))) %>% 
  dplyr::select(all_of(adjustment_set_variables), 
                Y = math_proportion,
                A = pm25_12_plus)

ltmle_data <- as.data.frame(ltmle_data)

ltmle_tmle <- ltmle(data = ltmle_data, 
                Anodes = 'A', 
                Ynodes = 'Y', 
                abar = list(1, 0),
                SL.library = list(Q = katie.SL.library, g = katie.SL.library), 
                gform = "A ~ air_basin_4_bins + asian_filipino_percent + two_none_aian_pi_af_am_percent + hisp_latinx_percent + english_learner_percent + frpm_percent + district_cost_per_pupil + student_teacher_ratio + cred_percent + school_k_12_enrollment")

summary(ltmle_tmle) # Stephen: OMG!!!!! good to see TMLE does the weird "basically equal means but stat sig diff" thing for both of us 

ltmle_tmle$beta.iptw

# run with gcomp
ltmle_data <- 
  dataset_complete_cases_working %>% 
  mutate(id_cds = as.numeric(as.factor(cds_code))) %>% 
  dplyr::select(all_of(adjustment_set_variables), 
                Y = math_proportion,
                A = pm25_12_plus)

ltmle_data <- as.data.frame(ltmle_data)

ltmle_gcomp <- ltmle(data = ltmle_data, 
                Anodes = 'A', 
                Ynodes = 'Y', 
                abar = list(1, 0),
                SL.library = katie.SL.library, 
                gcomp = TRUE)

summary(ltmle_gcomp)

summary(ltmle_tmle) 

tmle_data <- 
  dataset_complete_cases_working %>% 
  mutate(id_cds = as.numeric(as.factor(cds_code))) %>% 
  dplyr::select(all_of(adjustment_set_variables), 
                Y = math_proportion,
                A = pm25_12_plus)

tmle_data <- as.data.frame(tmle_data)

baseline_covariates <- subset(tmle_data, select=-c(Y,A))

tmle::tmle(Y = tmle_data$Y, 
     A = tmle_data$A, 
     W = baseline_covariates,
     # data = ltmle_data,
     g.SL.library = katie.SL.library, 
     Q.SL.library = katie.SL.library)



adjustment_set_variables <- 
  c("air_basin_4_bins", 
    "asian_filipino_percent", 
    "two_none_aian_pi_af_am_percent",
    "hisp_latinx_percent", 
    "english_learner_percent", 
    "frpm_percent",
    "district_cost_per_pupil", 
    "student_teacher_ratio", 
    "cred_percent",
    "school_k_12_enrollment")




#######################
# EVALUATING POSITIVITY
#######################

# call Super Learner for the exposure mechanism
gHatSL <- SuperLearner(Y=ObsData$A, 
                       X=subset(ObsData, 
                                select= -c(Y, A)),
                       SL.library = katie.SL.library, 
                       family="binomial")

# generate predicted prob being exposed, given baseline covariates
gHat1W <- gHatSL$SL.predict

# predicted prob of not being exposed, given baseline covariates
gHat0W <- 1- gHat1W

# # predicted prob of observed exposure, given baseline cov
gHatAW <- rep(NA, n = nrow(ObsData))
gHatAW[ObsData$A==1] <- gHat1W[ObsData$A==1]
gHatAW[ObsData$A==0] <- gHat0W[ObsData$A==0]

# this gives a little summary of the distribution
summary(gHatAW)

# calculate weights
wt <- 1/gHatAW

summary(wt)

# # variables in adjustment set
#                 air_basin_4_bins, 
#                 asian_filipino_percent, 
#                 two_none_aian_pi_af_am_percent, 
#                 hisp_latinx_percent, 
#                 english_learner_percent, 
#                 frpm_percent, 
#                 district_cost_per_pupil, 
#                 student_teacher_ratio, 
#                 cred_percent,
#                 school_k_12_enrollment

# combine gHat1W with original data
data_with_probability_of_exposure <- 
  dataset_complete_cases_working %>% 
  bind_cols(tibble(gHat1W))

# probability of exposure by African American
data_with_probability_of_exposure %>% 
  ggplot(aes(x = afr_am_percent, y = gHat1W)) +
  geom_point()

# probability of exposure by African American
data_with_probability_of_exposure %>% 
  ggplot(aes(x = afr_am_percent, y = gHat1W)) +
  geom_point()

# probability of exposure by African American
data_with_probability_of_exposure %>% 
  ggplot(aes(x = afr_am_percent, y = gHat1W)) +
  geom_point()

# probability of exposure by African American
data_with_probability_of_exposure %>% 
  ggplot(aes(x = afr_am_percent, y = gHat1W)) +
  geom_point()

# probabilty of exposure by region
data_with_probability_of_exposure %>% 
  ggplot(aes(x = air_basin_4_bins, y = gHat1W)) +
  geom_boxplot()

# african american proportion by bin


#######################################
## plotting g in propensity score chart
#######################################


## distribution of propensity scores
p1 <- data.frame(gHat1W[ObsData$A==1])
colnames(p1) <- "ps1"
p1$type <- 1
p0 <- data.frame(gHat1W[ObsData$A==0]) 
colnames(p0) <- "ps1"
p0$type <- 0 

plot_df <- data.frame(rbind(p1,p0))
ggplot(data=plot_df) + geom_histogram(aes(ps1), 
                                      bins=30, 
                                      stat = "bin") + 
  facet_wrap(~type) + 
  coord_flip() + 
  theme_bw() +
  xlab = "Probability of exposure (g(A|W))" +
  ylab = "Count"




```



```{r KATIE attempt at full code set from lab 6}

# run.tmle <- function(ObsData, SL.library, id = NULL){
# 
# #------------------------------------------
# # Estimate the conditional mean outcome Qbar(A,W)
# #------------------------------------------
# 
# # dataframe X with baseline covariates and exposure
# X <- subset(ObsData, 
#             select = c(pm25_12_plus, 
#                        air_basin, 
#                        afr_am_percent, 
#                        asian_filipino_percent, 
#                        two_none_aian_pi_percent, 
#                        hisp_latinx_percent, 
#                        english_learner_percent, 
#                        frpm_percent, 
#                        district_cost_per_pupil, 
#                        student_teacher_ratio))
# 
# # set the A=1 in X1 and the A=0 in X0
# X1 <- X0 <- X
# X1$pm25_12_plus <- 1 # under exposure
# X0$pm25_12_plus <- 0 # under control
# 
# # call Super Learner for estimation of QbarAW
# QbarSL<- SuperLearner(Y = ObsData$math_standard_met, 
#                       X = X, 
#                       SL.library = SL.library, 
#                       family = "gaussian", 
#                       id = id)
# # QbarSL
# 
# # initial estimates of the outcome, given the observed exposure & covariates
# QbarAW <- predict(QbarSL, newdata=ObsData)$pred
# # estimates of the outcome, given A=1 and covariates
# Qbar1W<- predict(QbarSL, newdata=X1)$pred
# # estimates of the outcome, given A=0 and covariates
# Qbar0W<- predict(QbarSL, newdata=X0)$pred
# 
# # simple substitution estimator:
# PsiHat.SS<-mean(Qbar1W - Qbar0W)
# 
# #------------------------------------------
# # Estimate the exposure mechanism g(pm25_12_plus|W)
# #------------------------------------------
# 
# # call Super Learner for the exposure mechanism
# gHatSL<- SuperLearner(Y = ObsData$pm25_12_plus, 
#                       X = subset(ObsData, select= -c(pm25_12_plus,
#                                                      math_standard_met,
#                                                      id)),
# SL.library = SL.library, family="binomial", id=id)
# # generate predicted prob being exposed, given baseline covariates
# gHat1W <- gHatSL$SL.predict
# # predicted prob of not being exposed, given baseline covariates
# gHat0W <- 1- gHat1W
# 
# # # predicted prob of observed exposure, given baseline cov
# # gHatAW<- rep(NA, n)
# # gHatAW[ObsData$A==1]<- gHat1W[ObsData$A==1]
# # gHatAW[ObsData$A==0]<- gHat0W[ObsData$A==0]
# 
# #-------------------------------------------------
# # Clever covariate H(A,W) for each subject
# #-------------------------------------------------
# H.AW<- as.numeric(ObsData$pm25_12_plus==1)/gHat1W - as.numeric(ObsData$pm25_12_plus==0)/gHat0W
# 
# # also want to evaluate the clever covariates at A=1 and A=0 for all subjects
# H.1W<- 1/gHat1W
# H.0W<- -1/gHat0W
# 
# 
# #IPTW estimator of the G-computation formula:
# PsiHat.IPTW <-mean( H.AW*ObsData$Y)
# 
# #------------------------------------------
# # Update the initial estimator of Qbar_0(A,W)
# #------------------------------------------
# logitUpdate<- glm(ObsData$Y ~ -1 +offset(qlogis(QbarAW)) + H.AW, family='binomial')
# epsilon <- logitUpdate$coef
# 
# QbarAW.star<- plogis(qlogis(QbarAW)+ epsilon*H.AW)
# Qbar1W.star<- plogis(qlogis(Qbar1W)+ epsilon*H.1W)
# Qbar0W.star<- plogis(qlogis(Qbar0W)+ epsilon*H.0W)
# 
# #------------------------------------------
# # Estimate Psi(P_0)
# #------------------------------------------
# 
# PsiHat.TMLE <- mean(Qbar1W.star - Qbar0W.star)
# 
# #------------------------------------------
# # Return point estimates, targeted estimates of Qbar_0(A,W),
# # and thevector of clever covariates
# #------------------------------------------
# 
# estimates <- data.frame(cbind(PsiHat.SS=PsiHat.SS, PsiHat.IPTW, PsiHat.TMLE))
# Qbar.star <- data.frame(cbind(QbarAW.star, Qbar1W.star, Qbar0W.star))
# names(Qbar.star)<- c('QbarAW.star', 'Qbar1W.star', 'Qbar0W.star')
# 
# list(estimates=estimates, Qbar.star=Qbar.star, H.AW=H.AW)
# }
# 
# out <- run.tmle(ObsData = ObsData, 
#                 SL.library = katie.SL.library)
# 
# est <- out$estimates
# est*100



```



```{r g-computation by KATIE, eval=FALSE, include=FALSE}

set.seed(252)
# specify SL library
  listWrappers()
  SL.library <- c("SL.mean","SL.glm")
  other_options <- c("SL.glm.interaction","SL.randomForest","SL.polymars","SL.rpartPrune","SL.biglasso")

```


# BELOW LIES STEPHEN'S CODE


```{r preliminary data analysis BY STEPHEN, eval=FALSE, include=FALSE}
# make into a dataframe
  dataset_complete_cases <- as.data.frame(dataset_complete_cases)
# basic bits about data
  names(dataset_complete_cases)
  (n <- nrow(dataset_complete_cases)) # checks out with Katie's description -> yay, using correct dataset
  describe(dataset_complete_cases)
    unique(dataset_complete_cases$air_district)
    unique(dataset_complete_cases$pm25_12_plus)
    unique(dataset_complete_cases$cds_code)
# descriptives for numeric columns    
  num_cols <- c("math_standard_met","modeled_air_at_school","afr_am_percent","asian_filipino_percent","two_none_aian_pi_percent","hisp_latinx_percent","white_percent","english_learner_percent","frpm_percent","district_cost_per_pupil","student_teacher_ratio","cred_percent","school_k_12_enrollment")
  str(dataset_complete_cases[,num_cols])
  describe(dataset_complete_cases[,num_cols])
  by(dataset_complete_cases[,num_cols],list(dataset_complete_cases$pm25_12_plus),describe)
  aggregate(dataset_complete_cases[,num_cols],list(dataset_complete_cases$pm25_12_plus),
            function(x) {round(c(length(x),mean(x,na.rm=T),sd(x,na.rm=T)),2)})
    
  for (i in 1:length(num_cols)) {hist(dataset_complete_cases[,num_cols[i]],main=num_cols[i],breaks=20)}  
    # looks like there are some outlier for student-teacher ratio and cost per pupil (one school spending $346k per student?)
    dataset_complete_cases[dataset_complete_cases$district_cost_per_pupil>50000,] # ok so the Bay Area has $$$$
    unique(dataset_complete_cases[dataset_complete_cases$district_cost_per_pupil>50000,c("air_district","district_cost_per_pupil")])
    cost_hist <- hist(dataset_complete_cases$district_cost_per_pupil,breaks=20,plot=FALSE)
    plot(cost_hist$counts,log="x",type="h",lwd=10,lend=2)
    
    dataset_complete_cases[dataset_complete_cases$student_teacher_ratio>50,]
    table(dataset_complete_cases$air_district) # 70 schools in Feather River. why STR so high??
    
  round(prop.table(table(dataset_complete_cases$air_district)),3)
  round(prop.table(table(dataset_complete_cases$air_basin)),3)
  round(prop.table(table(dataset_complete_cases$air_basin_binned)),3) # much better
  round(prop.table(table(dataset_complete_cases$air_basin_4_bins)),3)
# make A variable into numeric & Y variable proportion
  dataset_complete_cases$pm25_12_plus <- ifelse(dataset_complete_cases$pm25_12_plus=="yes",1,0)
  dataset_complete_cases$math_standard_met <- dataset_complete_cases$math_standard_met/100
# correlations between predictors
  predictor_corrs <- Hmisc::rcorr(as.matrix(dataset_complete_cases[,c("pm25_12_plus",num_cols)]))$r
  #openxlsx::write.xlsx(list(predictor_corrs),"~/Desktop/PH252D/cifp/output_for_writeups/predictor_correlations.xlsx",colNames=T,rowNames=T)
  
```

```{r iptw & tmle BY STEPHEN, eval=FALSE, include=FALSE}
set.seed(252)
# specify SL library
  listWrappers()
  
  SL.interaction.LunchRace <- function(Y, X, newX, family, obsWeights, model = TRUE, ...) {
    if (is.matrix(X)) {
        X = as.data.frame(X)
    }
    fit.glm <- glm(Y ~ . #+ frpm_percent:two_none_aian_pi_af_am_percent 
                         #+ frpm_percent:asian_filipino_percent 
                         + frpm_percent:hisp_latinx_percent,
                   , data = X, family = family, weights = obsWeights, model = model)
    if (is.matrix(newX)) {
        newX = as.data.frame(newX)
    }
    pred <- predict(fit.glm, newdata = newX, type = "response")
    fit <- list(object = fit.glm)
    class(fit) <- "SL.interaction.LunchRace"
    out <- list(pred = pred, fit = fit)
    return(out)
  }
  
  SL.interaction.CostRace <- function(Y, X, newX, family, obsWeights, model = TRUE, ...) {
    if (is.matrix(X)) {
        X = as.data.frame(X)
    }
    fit.glm <- glm(Y ~ . #+ district_cost_per_pupil:two_none_aian_pi_af_am_percent 
                         #+ district_cost_per_pupil:asian_filipino_percent 
                         + district_cost_per_pupil:hisp_latinx_percent,
                   , data = X, family = family, weights = obsWeights, model = model)
    if (is.matrix(newX)) {
        newX = as.data.frame(newX)
    }
    pred <- predict(fit.glm, newdata = newX, type = "response")
    fit <- list(object = fit.glm)
    class(fit) <- "SL.interaction.CostRace"
    out <- list(pred = pred, fit = fit)
    return(out)
  }
  
  SL.interaction.CredRace <- function(Y, X, newX, family, obsWeights, model = TRUE, ...) {
    if (is.matrix(X)) {
        X = as.data.frame(X)
    }
    fit.glm <- glm(Y ~ . #+ cred_percent:two_none_aian_pi_af_am_percent 
                         #+ cred_percent:asian_filipino_percent 
                         + cred_percent:hisp_latinx_percent,
                   , data = X, family = family, weights = obsWeights, model = model)
    if (is.matrix(newX)) {
        newX = as.data.frame(newX)
    }
    pred <- predict(fit.glm, newdata = newX, type = "response")
    fit <- list(object = fit.glm)
    class(fit) <- "SL.interaction.CredRace"
    out <- list(pred = pred, fit = fit)
    return(out)
  }
  
  SL.library <- c("SL.mean","SL.glm","SL.glm.interaction","SL.bayesglm"
                  #,"SL.interaction.LunchRace","SL.interaction.CostRace","SL.interaction.CredRace"
                  )
  #other_options <- c("SL.glm.interaction","SL.xgboost","SL.randomForest","SL.polymars","SL.rpartPrune")
# SS, IPTW, & TMLE function
  run.tmle <- function(Y,A,ObsData, SL.library, id=NULL) {
   
    #------------------------------------------
    # Estimate the conditional mean outcome Qbar(A,W)
    #------------------------------------------
   
      # dataframe X with baseline covariates and exposure
        X <- ObsData[,names(ObsData) != Y]
      # settheA=1inX1andtheA=0inX0
        X1 <- X0<-X
        X1[,A] <- 1 # under exposure
        X0[,A] <- 0 # under control
    
       # call Super Learner for estimation of QbarAW
        QbarSL<- SuperLearner(Y=ObsData[,Y], X=X, SL.library=SL.library, family="quasibinomial", id=id)
        # QbarSL
        
       # initial estimates of the outcome, given the observed exposure & covariates
        QbarAW <- predict(QbarSL, newdata=ObsData)$pred
       # estimates of the outcome, given A=1 and covariates
        Qbar1W<- predict(QbarSL, newdata=X1)$pred
       # estimates of the outcome, given A=0 and covariates
        Qbar0W<- predict(QbarSL, newdata=X0)$pred
    
       # simple substitution estimator:
        PsiHat.SS<-mean(Qbar1W - Qbar0W) 
    
    #------------------------------------------
    # Estimate the exposure mechanism g(A|W)
    #------------------------------------------
  
     # call Super Learner for the exposure mechanism
      gHatSL<- SuperLearner(Y=ObsData[,A], X=ObsData[,!(names(ObsData) %in% c(A,Y,id))],SL.library=SL.library, family="binomial", id=id)
     # generate predicted prob being exposed, given baseline covariates
      gHat1W<- gHatSL$SL.predict
     # predicted prob of not being exposed, given baseline covariates
      gHat0W<- 1- gHat1W
  
     # predicted prob of observed exposure, given baseline cov
      gHatAW<- rep(NA, n)
      gHatAW[ObsData[,A]==1]<- gHat1W[ObsData[,A]==1]
      gHatAW[ObsData[,A]==0]<- gHat0W[ObsData[,A]==0]
      wt <- 1/gHatAW
  
    #-------------------------------------------------
    # Clever covariate H(A,W) for each subject
    #-------------------------------------------------
      H.AW<- as.numeric(ObsData[,A]==1)/gHat1W - as.numeric(ObsData[,A]==0)/gHat0W
  
     # also want to evaluate the clever covariates at A=1 and A=0 for all subjects
       H.1W<- 1/gHat1W
       H.0W<- -1/gHat0W
       
     # IPTW estimator of the G-computation formula: -> INCLUDE HORVITZ-THOMPSON 
      PsiHat.IPTW <- mean(H.AW*ObsData[,Y])
      PsiHat.IPTW <- mean(wt*as.numeric(ObsData[,A]==1)*ObsData[,Y]) -
                     mean(wt*as.numeric(ObsData[,A]==0)*ObsData[,Y])
      
     # IPTW estimator of G-comp w Horvitz-Thompson adjusmtent
      PsiHat.IPTW_HT <- mean(wt*as.numeric(ObsData[,A]==1)*ObsData[,Y])/mean(wt*as.numeric(ObsData[,A]==1)) - 
                        mean(wt*as.numeric(ObsData[,A]==0)*ObsData[,Y])/mean(wt*as.numeric(ObsData[,A]==0))
  
    #------------------------------------------
    # Update the initial estimator of Qbar_0(A,W)
    #------------------------------------------
      logitUpdate<- glm(ObsData[,Y] ~ -1 +offset(qlogis(QbarAW)) + H.AW, family='quasibinomial')
      epsilon <- logitUpdate$coef
  
      QbarAW.star<- plogis(qlogis(QbarAW)+ epsilon*H.AW)
      Qbar1W.star<- plogis(qlogis(Qbar1W)+ epsilon*H.1W)
      Qbar0W.star<- plogis(qlogis(Qbar0W)+ epsilon*H.0W)
  
    #------------------------------------------
    # Estimate Psi(P_0)
    #------------------------------------------
  
      PsiHat.TMLE <- mean(Qbar1W.star - Qbar0W.star)
  
    #------------------------------------------
    # Return point estimates, targeted estimates of Qbar_0(A,W),
    # and thevector of clever covariates
    #------------------------------------------
  
      estimates <- data.frame(cbind(PsiHat.SS=PsiHat.SS, PsiHat.IPTW, PsiHat.IPTW_HT, PsiHat.TMLE))
      Qbar.star <- data.frame(cbind(QbarAW.star, Qbar1W.star, Qbar0W.star))
      names(Qbar.star)<- c('QbarAW.star', 'Qbar1W.star', 'Qbar0W.star')
      list(estimates=estimates, Qbar.star=Qbar.star, H.AW=H.AW)
  }
# practice run of function
  # subset of rows
    sub_data <- dataset_complete_cases[sample(1:n,1000,replace=F),]
    nrow(sub_data)
    duplicated(sub_data$cds_code)
    table(sub_data$pm25_12_plus)
  # run function
    names(sub_data)
    include_vars_analysis <- c("math_standard_met","pm25_12_plus","air_basin_4_bins","asian_filipino_percent","two_none_aian_pi_af_am_percent","hisp_latinx_percent",
                               "english_learner_percent","frpm_percent","district_cost_per_pupil","student_teacher_ratio","cred_percent","school_k_12_enrollment")
    out <- run.tmle(Y="math_standard_met", A="pm25_12_plus", ObsData=sub_data[,include_vars_analysis], SL.library=SL.library) 
      # generating a ton of errors. removing algorithms from SuperLearner
      # think interaction algorithms will be super problematic bc number of covariates is really high from all the districts
      # FIGURED OUT ISSUE: When we resample, not all districts are in each sample and predictions are generated for non-existent districts and not generated for extisting districts
    est <- out$estimates
    est*100
  # run with ltmle for comparison
    sub_data2 <- sub_data[,include_vars_analysis]
    names(sub_data2)
    sub_data2 <- sub_data2[,c(names(sub_data2[3:12]),"math_standard_met","pm25_12_plus")]
    ltmle.SL<- ltmle(data=sub_data2, Anodes='pm25_12_plus', Ynodes='math_standard_met', abar=list(1,0),SL.library=SL.library)
    summary(ltmle.SL)    
    
# the real thing
  set.seed(252)
  out <- run.tmle(Y="math_standard_met", A="pm25_12_plus", ObsData=dataset_complete_cases[,include_vars_analysis], SL.library=SL.library) 
  est <- out$estimates
  est*100
# real thing with ltmle for comparison  
  dataset_complete_cases_ltmle <- dataset_complete_cases[,include_vars_analysis]
  names(dataset_complete_cases_ltmle)
  dataset_complete_cases_ltmle <- dataset_complete_cases_ltmle[,c(3:12,1:2)]
  ltmle.SL<- ltmle(data=dataset_complete_cases_ltmle, Anodes='pm25_12_plus', Ynodes='math_standard_met', abar=list(1,0),SL.library=SL.library)  
  summary(ltmle.SL)  
  ltmle.SL$beta.iptw
# get gcomp with ltmle  
   ltmle.SL.gcomp <- ltmle(data=dataset_complete_cases_ltmle, Anodes='pm25_12_plus', Ynodes='math_standard_met', abar=list(1,0),SL.library=SL.library, gcomp=TRUE)  
  summary(ltmle.SL.gcomp)  
# get iptw with ltmle
  ltmle.SL.iptw <- ltmle(data=dataset_complete_cases_ltmle, Anodes='pm25_12_plus', Ynodes='math_standard_met', abar=list(1,0),SL.library=SL.library, gcomp=FALSE, iptw.only=TRUE, variance.method='iptw')  
  summary(ltmle.SL.iptw)  
```



```{r, Non-Parametric Bootstrap!!!! Runs just really slowly, eval=FALSE, include=FALSE}
# how many cores
  parallel::detectCores(all.tests = FALSE, logical = TRUE)
# non-parametric bootstrap
nonparam_boot <- function(ObsData,A,Y,id,SL.library,num_boots) {
  # make output matrix
    estimates <- matrix(NA,nrow=num_boots,ncol=4)
  # bootstrap data
    for (i in 1:num_boots) {
      # make data
        n <- nrow(ObsData)
        ObsData[,id] <- 1:n
        bootIndices<- sample(1:n, replace=T)
        bootData<- ObsData[bootIndices,]
      # estimate target parameters
        # simple sub
          X <- bootData[,names(ObsData) != Y]
          X1 <- X; X1[,A] <- 1
          X0 <- X; X0[,A] <- 0
          QbarSL<- SuperLearner(Y=bootData[,Y], X=X, SL.library=SL.library, family="binomial", id=bootData[,id])
          QbarAW <- predict(QbarSL, newdata=bootData)$pred
          Qbar1W<- predict(QbarSL, newdata=X1)$pred
          Qbar0W<- predict(QbarSL, newdata=X0)$pred
          simple_sub <- mean(Qbar1W-Qbar0W)
        # IPTW
          gHatSL<- SuperLearner(Y=bootData[,A], X=bootData[,!(names(bootData) %in% c(A,Y,id))], SL.library=SL.library, family="binomial", id=bootData[,id])
          gHat1W <- gHatSL$SL.predict
          gHat0W <- 1-gHatSL$SL.predict
          gHatAW <- ifelse(bootData[,A]==1,gHat1W,gHat0W)
          wt <- 1/gHatAW
          H.AW <- as.numeric(bootData[,A]==1)/gHat1W - as.numeric(bootData[,A]==0)/gHat0W
          H.1W <- as.numeric(X1[,A]==1)/gHat1W - as.numeric(X1[,A]==0)/gHat0W
          H.0W <- as.numeric(X0[,A]==1)/gHat1W - as.numeric(X0[,A]==0)/gHat0W
          iptw <- mean(H.AW*bootData[,Y])
          iptw_ht <- mean(wt*as.numeric(bootData[,A]==1)*bootData[,Y])/mean(wt*as.numeric(bootData[,A]==1)) - 
                     mean(wt*as.numeric(bootData[,A]==0)*bootData[,Y])/mean(wt*as.numeric(bootData[,A]==0))
        # TMLE
          logitUpdate <- glm(bootData[,Y] ~ -1 +offset(qlogis(QbarAW)) + H.AW,family='binomial')
          epsilon <- logitUpdate$coef
          QbarAW.star <- plogis(qlogis(QbarAW)+ epsilon*H.AW)
          Qbar1W.star <- plogis( qlogis(Qbar1W)+ epsilon*H.1W)
          Qbar0W.star <- plogis( qlogis(Qbar0W)+ epsilon*H.0W)
          PsiHat.TMLE <- mean(Qbar1W.star- Qbar0W.star)
      # save results
        estimates[i,] <- c(simple_sub,iptw,iptw_ht,PsiHat.TMLE)
      # periodically print i
        if (i%%5==0) (print(i))
    }
    return(estimates)
}  
npb_estimates <- nonparam_boot(ObsData=dataset_complete_cases[,c("cds_code",include_vars_analysis)],
                           A="pm25_12_plus",Y="math_standard_met",id="cds_code",SL.library=SL.library,num_boots=1) 
summary(npb_estimates)  
apply(npb_estimates,2,hist)

create.CI <- function(pt, boot, alpha=0.05){
   Zquant <- qnorm(alpha/2, lower.tail=F)
   CI.normal <- c(pt - Zquant*sd(boot), pt + Zquant*sd(boot) )
   CI.quant  <- quantile(boot, prob=c(0.025,0.975) )
   out<- data.frame(rbind(CI.normal, NA))*100
 colnames(out)<- c('CI.lo', 'CI.hi')
 out
}
apply(npb_estimates,2,function(x) create.CI(pt=mean(x,na.rm=T),boot=x))
apply(npb_estimates,2,function(x) quantile(x,probs=c(.025,.975),na.rm=T))



```

```{r, Influence Curves!!!, eval=FALSE, include=FALSE}
# TMLE inference -> p-value + CI assuming normal distribution
  IC <- out$H.AW*(dataset_complete_cases$math_standard_met - out$Qbar.star$QbarAW.star) + out$Qbar.star$Qbar1W.star - out$Qbar.star$Qbar0W.star - out$estimates$PsiHat.TMLE
  var(IC)
  (IC_se <- sqrt(var(IC)/n))
  (Psy.LL <- out$estimates$PsiHat.TMLE-1.96*IC_se)*100
  (Psy.UL <- out$estimates$PsiHat.TMLE+1.96*IC_se)*100
  2*pnorm(abs(out$estimates$PsiHat.TMLE/IC_se),lower.tail=F)
# IPTW inference -> p-value + CI assuming normal distribution
  IC <- out$H.AW*(dataset_complete_cases$math_standard_met - out$Qbar.star$QbarAW.star) + out$Qbar.star$Qbar1W.star - out$Qbar.star$Qbar0W.star - out$estimates$PsiHat.IPTW
  var(IC)
  (IC_se <- sqrt(var(IC)/n))
  (Psy.LL <- out$estimates$PsiHat.IPTW-1.96*IC_se)*100
  (Psy.UL <- out$estimates$PsiHat.IPTW+1.96*IC_se)*100
  2*pnorm(abs(out$estimates$PsiHat.IPTW/IC_se),lower.tail=F)  
# IPTW_HT inference -> p-value + CI assuming normal distribution
  IC <- out$H.AW*(dataset_complete_cases$math_standard_met - out$Qbar.star$QbarAW.star) + out$Qbar.star$Qbar1W.star - out$Qbar.star$Qbar0W.star - out$estimates$PsiHat.IPTW_HT
  var(IC)
  (IC_se <- sqrt(var(IC)/n))
  (Psy.LL <- out$estimates$PsiHat.IPTW_HT-1.96*IC_se)*100
  (Psy.UL <- out$estimates$PsiHat.IPTW_HT+1.96*IC_se)*100
  2*pnorm(abs(out$estimates$PsiHat.IPTW_HT/IC_se),lower.tail=F)  
  
  
  
  
# CI coverage and Type 1 error rates -> NOTE TO KATIE: I DIDN'T EDIT THIS PART BECAUSE I DON'T THINK WE HAVE TO INCLUDE IT. IT'S NOT MENTIONED IN THE GUIDELINES +, MORE IMPORTANTLY, DOING IT DEPENDS ON KNOWING THE TRUE VALUE OF THE TARGET CAUSAL PARAMETER, WHICH WE DON'T KNOW
  # extra functions
  generateY<- function(W1, W2, W3, W4, A, U.Y){
    prob <- plogis(-1.5+A-2*W3+0.5*W4+5*W1*W2*W4)
    as.numeric(U.Y < prob)
  }
  generateData<- function(n, effect=T, get.psi.F=F){
    W1 <- rbinom(n, size=1, prob=0.5)
    W2 <- rbinom(n, size=1, prob=0.5)
    W3 <- runif(n, min=0, max=1)
    W4 <- runif(n, min=0, max=5)
    pscore <- plogis(1+2*W1*W2-W4)
    A<- rbinom(n, size=1, prob=pscore)
    U.Y<- runif(n,0,1)
    # generate the counterfactual outcome with A=0
    Y.0<- generateY(W1=W1, W2=W2, W3=W3, W4=W4, A=0, U.Y=U.Y)
    if(!effect){ # if there is no effect, the counterfactual under txt =
      # the counterfactual under the control
      Y.1<- Y.0 } else { 
        # otherwise, generated the counterfactual outcome with A=1
        Y.1<- generateY(W1=W1, W2=W2, W3=W3, W4=W4, A=1, U.Y=U.Y)
      }
    # assign the observed outcome based on the observed exposure
    Y<- rep(NA, n)
    Y[A==1]<- Y.1[A==1]
    Y[A==0]<- Y.0[A==0]
    data<- data.frame(W1, W2, W3, W4, A, Y, Y.1, Y.0)
    if(!get.psi.F){
      data <- subset(data,select=c(W1,W2,W3,W4,A,Y))
    }
    data
  }
  
  Psi.0 <- 0
  n <- nrow(dataset_complete_cases)
  R <- 500

  pt.est <- rep(NA,R)
  ci.cov <- rep(NA,R)
  reject <- rep(NA,R)

  for (i in 1:R) {
    # make data
      NewData<- generateData(n, effect=F, get.psi.F=F)
    # TMLE
      X <- NewData[,1:5]
      X1 <- X; X1$A <- 1
      X0 <- X; X0$A <- 0
      QbarSL <- SuperLearner(Y=NewData$Y, X=X, SL.library=SL.library, family="binomial")
      QbarAW <- predict(QbarSL, newdata=NewData)$pred
      Qbar1W<- predict(QbarSL, newdata=X1)$pred
      Qbar0W<- predict(QbarSL, newdata=X0)$pred
      
      gHatSL<- SuperLearner(Y=NewData$A, X=subset(NewData, select= -c(A,Y)),SL.library=SL.library, family="binomial")
      gHat1W <- gHatSL$SL.predict
      gHat0W <- 1-gHatSL$SL.predict
      gHatAW <- ifelse(NewData$A==1,gHat1W,gHat0W)
      H.AW <- as.numeric(NewData$A==1)/gHat1W - as.numeric(NewData$A==0)/gHat0W
      H.1W <- as.numeric(X1$A==1)/gHat1W - as.numeric(X1$A==0)/gHat0W
      H.0W <- as.numeric(X0$A==1)/gHat1W - as.numeric(X0$A==0)/gHat0W
      
      logitUpdate <- glm(NewData$Y ~ -1 +offset(qlogis(QbarAW)) + H.AW,family='binomial')
      epsilon <- logitUpdate$coef
      QbarAW.star <- plogis(qlogis(QbarAW)+ epsilon*H.AW)
      Qbar1W.star <- plogis( qlogis(Qbar1W)+ epsilon*H.1W)
      Qbar0W.star <- plogis( qlogis(Qbar0W)+ epsilon*H.0W)
      PsiHat.TMLE <- mean(Qbar1W.star- Qbar0W.star)
      
      IC <- H.AW*(NewData$Y - QbarAW.star) + Qbar1W.star - Qbar0W.star - PsiHat.TMLE
      IC_se <- sqrt(var(IC)/n)
      Psi.LL <- PsiHat.TMLE-1.96*IC_se
      Psi.UL <- PsiHat.TMLE+1.96*IC_se
      p.val <- 2*pnorm(abs(PsiHat.TMLE/IC_se),lower.tail=F)
    # save output
      pt.est[i] <- PsiHat.TMLE
      ci.cov[i] <- Psi.0 >= Psi.LL & Psi.0 <= Psi.UL
      reject[i] <- p.val < .05
  }

  hist(pt.est) # looks distributed around zero
  mean(ci.cov) # 93% contain the true value (pretty close to what it should be)
  mean(reject) # 0% were falsely rejected


```



```{r, KATIE PARALLEL Inference Curve!!!! DOES NOT RUN YET, eval=FALSE, include=FALSE}

# set up parallelization (my computer has 8 cores and this uses 7 of them)
(cluster = parallel::makeCluster(3))
parallel::clusterEvalQ(cluster, library(SuperLearner))

# load dataset
load(file = file.path("dataset_complete_cases.rdata"))

# non-parametric bootstrap
nonparam_boot_parallel <- function(ObsData,A,Y,id,SL.library,num_boots) {
  # make output matrix
    estimates <- matrix(NA,nrow=num_boots,ncol=4)
  # bootstrap data
    for (i in 1:num_boots) {
      # make data
        n <- nrow(ObsData)
        ObsData[,id] <- 1:n
        bootIndices<- sample(1:n, replace=T)
        bootData<- ObsData[bootIndices,]
      # estimate target parameters
        # simple sub
          X <- bootData[,names(ObsData) != Y]
          X1 <- X; X1[,A] <- 1
          X0 <- X; X0[,A] <- 0
          QbarSL<- snowSuperLearner(Y=bootData[,Y], X=X, SL.library=SL.library, family="binomial", id=bootData[,id], cluster = cluster)
          QbarAW <- predict(QbarSL, newdata=bootData)$pred
          Qbar1W<- predict(QbarSL, newdata=X1)$pred
          Qbar0W<- predict(QbarSL, newdata=X0)$pred
          simple_sub <- mean(Qbar1W-Qbar0W)
        # IPTW
          gHatSL<- snowSuperLearner(Y=bootData[,A], X=bootData[,!(names(bootData) %in% c(A,Y,id))], SL.library=SL.library, family="binomial", id=bootData[,id], cluster = cluster)
          gHat1W <- gHatSL$SL.predict
          gHat0W <- 1-gHatSL$SL.predict
          gHatAW <- ifelse(bootData[,A]==1,gHat1W,gHat0W)
          wt <- 1/gHatAW
          H.AW <- as.numeric(bootData[,A]==1)/gHat1W - as.numeric(bootData[,A]==0)/gHat0W
          H.1W <- as.numeric(X1[,A]==1)/gHat1W - as.numeric(X1[,A]==0)/gHat0W
          H.0W <- as.numeric(X0[,A]==1)/gHat1W - as.numeric(X0[,A]==0)/gHat0W
          iptw <- mean(H.AW*bootData[,Y])
          iptw_ht <- mean(wt*as.numeric(bootData[,A]==1)*bootData[,Y])/mean(wt*as.numeric(bootData[,A]==1)) - 
                     mean(wt*as.numeric(bootData[,A]==0)*bootData[,Y])/mean(wt*as.numeric(bootData[,A]==0))
        # TMLE
          logitUpdate <- glm(bootData[,Y] ~ -1 +offset(qlogis(QbarAW)) + H.AW,family='binomial')
          epsilon <- logitUpdate$coef
          QbarAW.star <- plogis(qlogis(QbarAW)+ epsilon*H.AW)
          Qbar1W.star <- plogis( qlogis(Qbar1W)+ epsilon*H.1W)
          Qbar0W.star <- plogis( qlogis(Qbar0W)+ epsilon*H.0W)
          PsiHat.TMLE <- mean(Qbar1W.star- Qbar0W.star)
      # save results
        estimates[i,] <- c(simple_sub,iptw,iptw_ht,PsiHat.TMLE)
    }
    return(estimates)
}  


include_vars_analysis <- c("math_standard_met","pm25_12_plus","air_basin_binned","afr_am_percent","asian_filipino_percent","two_none_aian_pi_percent","hisp_latinx_percent", "english_learner_percent","frpm_percent","district_cost_per_pupil","student_teacher_ratio","cred_percent","school_k_12_enrollment")

SL.library <- c("SL.mean","SL.glm","SL.glm.interaction","SL.bayesglm")

estimates <- nonparam_boot_parallel(ObsData=dataset_complete_cases[,c("cds_code",include_vars_analysis)],
                           A="pm25_12_plus",Y="math_standard_met",id="cds_code",SL.library=SL.library,num_boots=5) 
summary(estimates)  
apply(estimates,2,hist)

create.CI <- function(pt, boot, alpha=0.05){
   Zquant <- qnorm(alpha/2, lower.tail=F)
   CI.normal <- c(pt - Zquant*sd(boot), pt + Zquant*sd(boot) )
   CI.quant  <- quantile(boot, prob=c(0.025,0.975) )
   out<- data.frame(rbind(CI.normal, NA))*100
 colnames(out)<- c('CI.lo', 'CI.hi')
 out
}
apply(estimates,2,function(x) create.CI(pt=mean(x,na.rm=T),boot=x))
apply(estimates,2,function(x) quantile(x,probs=c(.025,.975),na.rm=T))



```































