---
title: "Causal Inference Final Project Code"
author: "Katherine Rose Wolf"
date: "March 27, 2020"
output: html_document
---

```{r setup load libraries}

library(praise)
library(raster)
library(tigris)
library(sf)
library(httr)
library(tidyverse)
library(ggmap)
library(nngeo)
library(tableone)
library(kableExtra)
library(furniture)
library(RColorBrewer)
library(psych)
library(SuperLearner)
library(ltmle)

# stop r from abbreviating doubles in school ids
options("scipen" = 40)

praise()  # self-esteem boost

```


```{r load datasets}

load(file = file.path("dataset_full.rdata"))

load(file = file.path("dataset_complete_cases.rdata"))

#<<<<<<< HEAD
```



```{r preliminary data analysis BY KATIE, eval=FALSE, include=FALSE}

# load data
load(file = file.path("causal_inference_preliminary_dataset.rdata"))

# relabel variables
data_for_table_one <- 
  causal_inference_preliminary_dataset %>% 
  mutate("PM25" = factor(pm25_12_plus),
         "School enrollment (n)" = school_k_12_enrollment,
         "Female (%)" = round(percent_female*100, 2),
         "African American (%)" = round(percent_afr_am*100, 2),
         "American Indian or Alaska Native (%)" = round(percent_aian*100, 2),
         "Asian (%)" = round(percent_asian*100, 2),
         "Filipino (%)" = round(percent_filipino*100, 2),
         "Hispanic/Latino (any race) (%)" = round(percent_hispanic_latino*100, 2),
         "Pacific Islander (%)" = round(percent_pacific_islander*100, 2),
         "White (%)" = round(percent_white*100, 2),
         "Two or more races (not Hispanic/Latino) (%)" = round(percent_two_plus*100, 2),
         "Race/ethnicity not reported (%)" = round(percent_not_reported*100, 2),
         "Chronic absenteeism (%)" = as.numeric(chronic_absenteeism_rate),
         "Suspension rate (%)" = as.numeric(suspension_rate), 
         "Free meal eligibility (%)" = free_lunch_percent, 
         "Free/reduced-price meal eligibility (%)" = frpm_percent, 
         "English learners (%)" = round(english_learner_percent*100, 2),
         "English standard met or exceeded (%)" = as.numeric(english_standard_met), 
         "Math standard met or exceeded (%)" = as.numeric(math_standard_met), 
         "Science standard met or exceeded (%)" = as.numeric(science_standard_met), 
         "Overall mean of standards met or exceeded (%)" = round(as.numeric(overall_standard_met), 2)) %>% 
  select("PM25",
         "School enrollment (n)",
         "Female (%)",
         "African American (%)",
         "American Indian or Alaska Native (%)",
         "Asian (%)",
         "Filipino (%)",
         "Hispanic/Latino (any race) (%)",
         "Pacific Islander (%)",
         "White (%)",
         "Two or more races (not Hispanic/Latino) (%)",
         "Race/ethnicity not reported (%)",
         "Chronic absenteeism (%)",
         "Suspension rate (%)", 
         "Free meal eligibility (%)", 
         "Free/reduced-price meal eligibility (%)", 
         "English learners (%)",
         "English standard met or exceeded (%)", 
         "Math standard met or exceeded (%)", 
         "Science standard met or exceeded (%)", 
         "Overall mean of standards met or exceeded (%)") %>% 
  st_drop_geometry() %>% 
  as.data.frame()



# create a list of variables for the table
# (not including the stratification variable)
table_one_variables <- 
  c(     "School enrollment (n)",
         "Female (%)",
         "African American (%)",
         "American Indian or Alaska Native (%)",
         "Asian (%)",
         "Filipino (%)",
         "Hispanic/Latino (any race) (%)",
         "Pacific Islander (%)",
         "White (%)",
         "Two or more races (not Hispanic/Latino) (%)",
         "Race/ethnicity not reported (%)",
         "Chronic absenteeism (%)",
         "Suspension rate (%)", 
         "Free meal eligibility (%)", 
         "Free/reduced-price meal eligibility (%)", 
         "English learners (%)",
         "English standard met or exceeded (%)", 
         "Math standard met or exceeded (%)", 
         "Science standard met or exceeded (%)", 
         "Overall mean of standards met or exceeded (%)")

table_one <- CreateTableOne(vars = table_one_variables,
                            # factorVars = factor_variables,
                            strata = "PM25",
                            data = data_for_table_one,
                            test = FALSE,
                            includeNA = TRUE)

save(table_one, 
     file = "table_one.rdata")

# Creates a formatted table, using kable from the knitr package
# Would want to clean this up for publication purposes:
hi <- kable(print(table_one,
                  showAllLevels = TRUE,
                  printToggle = FALSE,
                  noSpaces = TRUE,
                  catDigits = 1,
                  contDigits = 1),
            col.names = c("", "Mean PM2.5 <= 12 ug/m3", "Mean PM2.5 > 12 ug/m3"),
            caption=paste(".  Descriptive statistics for California public schools in 2018-19, stratified by 2018 mean PM2.5 concentration.")) %>% 
  kable_styling("striped")

hi


expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))

# make plots

# histogram for test scores
ggplot(data = data_for_table_one, 
       aes(x = `Overall mean of standards met or exceeded (%)`)) +
  geom_histogram(binwidth = 1) +
  xlab("Mean percentage of students who met or exceeded standard across \nEnglish, science, and math in 2018-19") +
  ylab("Count")

# histogram for air pollution
ggplot(data = causal_inference_preliminary_dataset, 
       aes(x = modeled_air_at_school)) +
  geom_histogram(binwidth = 0.1) +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("Count")

# make basic scatterplot with line through it
ggplot(data = causal_inference_preliminary_dataset, 
       aes(x = modeled_air_at_school, 
           y = overall_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth(method=loess, 
              se=TRUE, 
              color="darkred") +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("Mean Percentage of Students Who Met or Exceeded Standard \n(English, Science, and Math) in 2018-19")

# get basic summary statistics
mean(causal_inference_preliminary_dataset$overall_standard_met)
summary(causal_inference_preliminary_dataset$overall_standard_met)
sd(causal_inference_preliminary_dataset$overall_standard_met)



load(file = file.path( "causal_inference_preliminary_dataset.rdata"))

load(file = file.path("intermediate_data", "air_cropped.rdata"))

# plot air data
plot(air_cropped)

dev.off()

load(file = file.path("intermediate_data", "california_shapefile.rdata"))

# make plot of modeled air at each school
plot.new()

plot(california_shapefile[1], main=NULL, color=rgb(1, 1, 1, alpha = 1), border = "black", add=TRUE)
plot(causal_inference_preliminary_dataset["modeled_air_at_school"], pch=16, cex=0.4, 
     xlab=NULL, ylab=NULL, main=NULL)
title(main = expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018")))

# plot test scores
plot(causal_inference_preliminary_dataset["overall_standard_met"], pch=16, cex=0.4, 
     xlab=NULL, ylab=NULL, main=NULL, pal=brewer.pal(10, "RdYlGn"))

mean(causal_inference_preliminary_dataset$modeled_air_at_school)
sd(causal_inference_preliminary_dataset$modeled_air_at_school)
=======
View(dataset_complete_cases)
>>>>>>> fb6e4f0c35f405fb73a4de63a6cad2f95d54113f

```

```{r preliminary data analysis BY STEPHEN, eval=FALSE, include=FALSE}
# basic bits about data
  names(dataset_complete_cases)
  (n <- nrow(dataset_complete_cases)) # checks out with Katie's description -> yay, using correct dataset
  describe(dataset_complete_cases)
    unique(dataset_complete_cases$air_district)
    unique(dataset_complete_cases$pm25_12_plus)
    unique(dataset_complete_cases$cds_code)
# descriptives for numeric columns    
  num_cols <- c(2,5:13)
  describe(dataset_complete_cases[,num_cols])
  by(dataset_complete_cases[,num_cols],list(dataset_complete_cases$pm25_12_plus),describe)
  aggregate(dataset_complete_cases[,num_cols],list(dataset_complete_cases$pm25_12_plus),
            function(x) {round(c(length(x),mean(x,na.rm=T),sd(x,na.rm=T)),2)})
    
  for (i in 1:length(num_cols)) {hist(dataset_complete_cases[,num_cols[i]],
                                      main=colnames(dataset_complete_cases[num_cols[i]]),breaks=20)}  
    # looks like there are some outlier for student-teacher ratio and cost per pupil (one school spending $346k per student?)
    dataset_complete_cases[dataset_complete_cases$district_cost_per_pupil>50000,] # ok so the Bay Area has $$$$
    unique(dataset_complete_cases[dataset_complete_cases$district_cost_per_pupil>50000,c("air_district","district_cost_per_pupil")])
    cost_hist <- hist(dataset_complete_cases$district_cost_per_pupil,breaks=20,plot=FALSE)
    plot(cost_hist$counts,log="x",type="h",lwd=10,lend=2)
    
    dataset_complete_cases[dataset_complete_cases$student_teacher_ratio>50,]
    table(dataset_complete_cases$air_district) # 70 schools in Feather River. why STR so high??
    
  round(prop.table(table(dataset_complete_cases$air_district)),3)
```

```{r iptw & tmle BY STEPHEN, eval=FALSE, include=FALSE}
set.seed(252)
# specify SL library
  listWrappers()
  SL.library <- c("SL.mean","SL.glm")
  other_options <- c("SL.glm.interaction","SL.randomForest","SL.polymars","SL.rpartPrune","SL.biglasso")
# SS, IPTW, & TMLE function
  run.tmle <- function(Y,ObsData, SL.library, id=NULL) {
   
    #------------------------------------------
    # Estimate the conditional mean outcome Qbar(A,W)
    #------------------------------------------
   
      # dataframe X with baseline covariates and exposure
        X <- ObsData[,names(ObsData) != Y]
      # settheA=1inX1andtheA=0inX0
        X1 <- X0<-X
        X1$A <- 1 # under exposure
        X0$A <- 0 # under control
    
       # call Super Learner for estimation of QbarAW
        QbarSL<- SuperLearner(Y=ObsData[,Y], X=X, SL.library=SL.library, family="gaussian", id=id)
        # QbarSL
        
       # initial estimates of the outcome, given the observed exposure & covariates
        QbarAW <- predict(QbarSL, newdata=ObsData)$pred
       # estimates of the outcome, given A=1 and covariates
        Qbar1W<- predict(QbarSL, newdata=X1)$pred
       # estimates of the outcome, given A=0 and covariates
        Qbar0W<- predict(QbarSL, newdata=X0)$pred
    
       # simple substitution estimator:
        PsiHat.SS<-mean(Qbar1W - Qbar0W)
    
    #------------------------------------------
    # Estimate the exposure mechanism g(A|W)
    #------------------------------------------
  
     # call Super Learner for the exposure mechanism
      gHatSL<- SuperLearner(Y=ObsData$A, X=subset(ObsData, select= -c(A,Y,id)),SL.library=SL.library, family="binomial", id=id)
     # generate predicted prob being exposed, given baseline covariates
      gHat1W<- gHatSL$SL.predict
     # predicted prob of not being exposed, given baseline covariates
      gHat0W<- 1- gHat1W
  
     # # predicted prob of observed exposure, given baseline cov
     # gHatAW<- rep(NA, n)
     # gHatAW[ObsData$A==1]<- gHat1W[ObsData$A==1]
     # gHatAW[ObsData$A==0]<- gHat0W[ObsData$A==0]
  
    #-------------------------------------------------
    # Clever covariate H(A,W) for each subject
    #-------------------------------------------------
      H.AW<- as.numeric(ObsData$A==1)/gHat1W - as.numeric(ObsData$A==0)/gHat0W
  
     # also want to evaluate the clever covariates at A=1 and A=0 for all subjects
       H.1W<- 1/gHat1W
       H.0W<- -1/gHat0W
       
     # IPTW estimator of the G-computation formula:
      PsiHat.IPTW <-mean( H.AW*ObsData$Y)
  
    #------------------------------------------
    # Update the initial estimator of Qbar_0(A,W)
    #------------------------------------------
      logitUpdate<- glm(ObsData$Y ~ -1 +offset(qlogis(QbarAW)) + H.AW, family='binomial')
      epsilon <- logitUpdate$coef
  
      QbarAW.star<- plogis(qlogis(QbarAW)+ epsilon*H.AW)
      Qbar1W.star<- plogis(qlogis(Qbar1W)+ epsilon*H.1W)
      Qbar0W.star<- plogis(qlogis(Qbar0W)+ epsilon*H.0W)
  
    #------------------------------------------
    # Estimate Psi(P_0)
    #------------------------------------------
  
      PsiHat.TMLE <- mean(Qbar1W.star - Qbar0W.star)
  
    #------------------------------------------
    # Return point estimates, targeted estimates of Qbar_0(A,W),
    # and thevector of clever covariates
    #------------------------------------------
  
      estimates <- data.frame(cbind(PsiHat.SS=PsiHat.SS, PsiHat.IPTW, PsiHat.TMLE))
      Qbar.star <- data.frame(cbind(QbarAW.star, Qbar1W.star, Qbar0W.star))
      names(Qbar.star)<- c('QbarAW.star', 'Qbar1W.star', 'Qbar0W.star')
      list(estimates=estimates, Qbar.star=Qbar.star, H.AW=H.AW)
  }
# practice run of function
  # subset of rows
    sub_data <- dataset_complete_cases[sample(1:n,500,replace=F),]
    nrow(sub_data)
    duplicated(sub_data$cds_code)
  # run function
    out <- run.tmle(Y="math_standard_met", ObsData=sub_data[,c(2,3,12,13)], SL.library=SL.library) 
      # generating a ton of errors. removing algorithms from SuperLearner
      # think interaction algorithms will be super problematic bc number of covariates is really high from all the districts
      # FIGURED OUT ISSUE: When we resample, not all districts are in each sample and predictions are generated for non-existent districts and not generated for extisting districts
    est <- out$estimates
    est*100
 



 






```


































