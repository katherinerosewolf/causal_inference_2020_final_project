---
title: "Causal Inference Final Project Code"
author: "Katherine Rose Wolf"
date: "March 27, 2020"
output: html_document
---

```{r setup load libraries}

library(praise)
library(tidyverse)
library(tableone)
library(kableExtra)
library(furniture)
library(RColorBrewer)
library(psych)
library(SuperLearner)
library(ltmle)
library(arm)

# stop r from abbreviating doubles in school ids
options("scipen" = 40)

praise()  # self-esteem boost

```


```{r load datasets}

load(file = "dataset_full.rdata")

load(file = "dataset_complete_cases.rdata")

load(file = "dataset_complete_cases_forget_funding.rdata") # no per cap variable so N goes to ~9000

```


# BELOW LIES KATIE'S PLAYGROUND

```{r KATIE code pre-SuperLearner}

dataset_complete_cases_working <- 
  dataset_complete_cases %>% 
  mutate(math_proportion = math_standard_met/100,
         pm25_12_plus = ifelse(pm25_12_plus == "yes", 1, 0), 
         air_district = factor(air_district), 
         air_basin = factor(air_basin))

# histogram of math percentage (weird, lots of zeroes)
hist(dataset_complete_cases_working$math_standard_met, 
     breaks = 20)

# simple mean: areas with pm above 12 lose 2.03 percentage points of passage
mean(dataset_complete_cases_working$math_proportion[
  dataset_complete_cases_working$pm25_12_plus == 1
  ] - 
    dataset_complete_cases_working$math_proportion[
      dataset_complete_cases_working$pm25_12_plus == 0])

# this code just fits the exposure linearly by the outcome
plain_glm_one_variable <- glm(math_standard_met ~ pm25_12_plus, 
                              data = dataset_complete_cases_working)
summary(plain_glm_one_variable)

# full linear glm no interactions
plain_glm <- glm(math_standard_met ~ 
                   pm25_12_plus + 
                   air_basin + 
                   afr_am_percent + 
                   asian_filipino_percent + 
                   two_none_aian_pi_percent + 
                   hisp_latinx_percent + 
                   english_learner_percent + 
                   frpm_percent + 
                   district_cost_per_pupil + 
                   student_teacher_ratio + 
                   school_k_12_enrollment +
                   cred_percent, 
                 data = dataset_complete_cases_working)

summary(plain_glm)

# full linear glm drop weirds
plain_glm_drop_per_pupil <- glm(math_standard_met ~ 
                   pm25_12_plus + 
                   air_basin + 
                   afr_am_percent + 
                   asian_filipino_percent + 
                   two_none_aian_pi_percent + 
                   hisp_latinx_percent + 
                   english_learner_percent + 
                   frpm_percent + 
                   cred_percent +
                   school_k_12_enrollment, 
                 data = dataset_complete_cases_working)

summary(plain_glm_drop_per_pupil)

summary(dataset_complete_cases_working$student_teacher_ratio)

# make working dataset without funding data
dataset_complete_cases_forget_funding_working <- 
  dataset_complete_cases_forget_funding

# simple glm without funding data
glm_forget_funding <- 
  glm(math_standard_met ~ 
                   pm25_12_plus + 
                   air_basin + 
                   afr_am_percent + 
                   asian_filipino_percent + 
                   two_none_aian_pi_percent + 
                   hisp_latinx_percent + 
                   english_learner_percent + 
                   frpm_percent + 
                   student_teacher_ratio + 
                   cred_percent + 
                   school_k_12_enrollment, 
                 data = dataset_complete_cases_forget_funding_working)

summary(glm_forget_funding)


```


```{r plots for slides}

dataset_complete_cases_working <- 
  dataset_complete_cases %>% 
  mutate(math_proportion = math_standard_met/100,
         pm25_12_plus = factor(pm25_12_plus), 
         air_district = factor(air_district), 
         air_basin = factor(air_basin))

simulated_full_only_complete_cases <- 
  dataset_full %>% 
  filter(!is.na(math_standard_met)) %>% 
  filter(!is.na(english_learner_percent)) %>% 
  filter(!is.na(student_teacher_ratio)) %>% 
  filter(!is.na(air_district))

# count observations by air basin
dataset_complete_cases_working %>% 
  group_by(air_basin) %>% 
  summarize(count = n()) %>% 
  View()

#------------------------------------------
# Exposure-Outcome
#------------------------------------------

# make basic scatterplot with loess line through it
ggplot(data = simulated_full_only_complete_cases,
       aes(x = modeled_air_at_school,
           y = math_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth(method=loess,
              se=TRUE,
              color="darkred") +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("Mean Percentage of Students Who Met or Exceeded \nMath Standard in 2018-19") +
  ggtitle("Complete Cases Only")

# make basic scatterplot with plain line through it
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = modeled_air_at_school, 
           y = math_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_abline() +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("Mean Percentage of Students Who Met or Exceeded \nMath Standard in 2018-19") +
  ggtitle("Complete Cases Only")

# make basic scatterplot with loess line through it
ggplot(data = dataset_full,
       aes(x = modeled_air_at_school,
           y = math_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth(method=loess,
              se=TRUE,
              color="darkred") +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("Mean Percentage of Students Who Met or Exceeded \nMath Standard in 2018-19") +
  ggtitle("Full Data")

# make basic scatterplot with plain line through it
ggplot(data = dataset_full, 
       aes(x = modeled_air_at_school, 
           y = math_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_abline() +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("Mean Percentage of Students Who Met or Exceeded \nMath Standard in 2018-19") +
  ggtitle("Full Data")

# box plot PM2.5 versus math score
ggplot(data = dataset_complete_cases_working, 
       aes(x = pm25_12_plus, 
           y = math_standard_met)) +
  geom_boxplot() +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("Mean Percentage of Students Who Met or Exceeded \nMath Standard in 2018-19") +
  ggtitle("Complete Cases Only")

colnames(dataset_complete_cases)

#------------------------------------------
# Enrollment
#------------------------------------------

# scatterplot enrollment and pm2.5
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = modeled_air_at_school, 
           y = school_k_12_enrollment)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth() +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("School Enrollment 2018-19") +
  ggtitle("Complete Cases Only")

# scatterplot enrollment and test scores
ggplot(data = dataset_complete_cases, 
       aes(x = school_k_12_enrollment, 
           y = math_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth() +
  xlab(expression(paste("School Enrollment 2018-19"))) +
  ylab("Mean Percentage of Students Who Met or Exceeded \nMath Standard in 2018-19") +
  ggtitle("Complete Cases Only")

# box plot PM2.5 versus enrollment
ggplot(data = dataset_complete_cases_working, 
       aes(x = pm25_12_plus, 
           y = school_k_12_enrollment)) +
  geom_boxplot() +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("School Enrollment 2018-19") +
  ggtitle("Complete Cases Only")

#------------------------------------------
# FRPM
#------------------------------------------

# scatterplot frpm air pollution
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = frpm_percent, 
           y = modeled_air_at_school)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth() +
  xlab("Students with Free/Reduced-Price Lunches (%)") +
  ylab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ggtitle("Complete Cases Only")

# boxplot frpm pm
ggplot(data = dataset_complete_cases_working, 
       aes(x = pm25_12_plus, 
           y = frpm_percent)) +
  geom_boxplot() +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("Students with Free/Reduced-Price Lunches (%)") +
  ggtitle("Complete Cases Only")

# scatterplot frpm math standard met
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = frpm_percent, 
           y = math_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth() +
  xlab("Students with Free/Reduced-Price Lunches (%)") +
  ylab("Math Standard Met") +
  ggtitle("Complete Cases Only")

#------------------------------------------
# Per-pupil funding
#------------------------------------------

# scatterplot cost per pupil air pollution
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = district_cost_per_pupil, 
           y = modeled_air_at_school)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth() +
  xlab("District Cost Per Pupil ($)") +
  ylab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ggtitle("Complete Cases Only")

# boxplot cost per pupil pm
ggplot(data = dataset_complete_cases_working, 
       aes(x = pm25_12_plus, 
           y = district_cost_per_pupil)) +
  geom_boxplot() +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("District Cost Per Pupil ($)") +
  ggtitle("Complete Cases Only") +
  ylim(0, 150000)

# scatterplot district cost per pupil math standard met
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = district_cost_per_pupil, 
           y = math_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth() +
  xlab("District Cost Per Pupil ($)") +
  ylab("Math Standard Met") +
  ggtitle("Complete Cases Only") +
  xlim(0, 40000) +
  ylim(0, 100)

#------------------------------------------
# Student-teacher ratio
#------------------------------------------

# scatterplot student-teacher ratio air pollution
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = student_teacher_ratio, 
           y = modeled_air_at_school)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth() +
  xlab("Student-teacher ratio") +
  ylab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ggtitle("Complete Cases Only")

# boxplot student-teacher ratio pm
ggplot(data = dataset_complete_cases_working, 
       aes(x = pm25_12_plus, 
           y = student_teacher_ratio)) +
  geom_boxplot() +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("Student-teacher ratio") +
  ggtitle("Complete Cases Only")

# scatterplot district student-teacher ratio math standard met
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = student_teacher_ratio, 
           y = math_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth(method = "loess") +
  xlab("Student-teacher ratio") +
  ylab("Math standard met (%)") +
  ggtitle("Complete Cases Only")

#------------------------------------------
# Fully-credentialed teachers
#------------------------------------------

# scatterplot credentials / air pollution
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = cred_percent, 
           y = modeled_air_at_school)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth() +
  xlab("Fully credentialed teachers (%)") +
  ylab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ggtitle("Complete Cases Only")

# boxplot credentials / pm
ggplot(data = dataset_complete_cases_working, 
       aes(x = pm25_12_plus, 
           y = cred_percent)) +
  geom_boxplot() +
  xlab(expression(paste("Mean PM"["2.5"]," concentration (",mu,"g/m"^"3",") in 2018"))) +
  ylab("Fully credentialed teachers (%)") +
  ggtitle("Complete Cases Only")

# scatterplot credentials / math standard met
ggplot(data = simulated_full_only_complete_cases, 
       aes(x = cred_percent, 
           y = math_standard_met)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth(method = "loess") +
  xlab("Fully credentialed teachers (%)") +
  ylab("Math standard met (%)") +
  ggtitle("Complete Cases Only")

#------------------------------------------
# Air basin
#------------------------------------------

#------------------------------------------
# Percent white
#------------------------------------------

#------------------------------------------
# Percent Hispanic/Latinx
#------------------------------------------

#------------------------------------------
# Percent Asian/Filipino
#------------------------------------------

#------------------------------------------
# Percent African American
#------------------------------------------

#------------------------------------------
# Percent other
#------------------------------------------



```



```{r KATIE attempt at simple substitution}

# set up parallelization (my computer has 8 cores and this uses 7 of them)
(cluster = parallel::makeCluster(7))
parallel::clusterEvalQ(cluster, library(SuperLearner))

dataset_complete_cases_working <- 
  dataset_complete_cases %>% 
  mutate(math_proportion = math_standard_met/100,
         pm25_12_plus = ifelse(pm25_12_plus == "yes", 1, 0), 
         air_district = factor(air_district), 
         air_basin = factor(air_basin))

ObsData <- 
  dataset_complete_cases_working %>% 
  mutate(id_cds = as.numeric(as.factor(cds_code))) %>% 
  dplyr::select(Y = math_proportion,
                A = pm25_12_plus, 
                air_basin, 
                afr_am_percent, 
                asian_filipino_percent, 
                two_none_aian_pi_percent, 
                hisp_latinx_percent, 
                english_learner_percent, 
                frpm_percent, 
                district_cost_per_pupil, 
                student_teacher_ratio, 
                cred_percent,
                school_k_12_enrollment)

ObsData <- as.data.frame(ObsData)

set.seed(252)

# reduce runs of biglasso from 100 to 5 to save time
new.algorithm.1 = create.Learner("SL.biglasso", 
                                params = list(nlambda = 5))

new.algorithm.2 = create.Learner("SL.randomForest", 
                                params = list(ntrees = 10))

# specify SL library
listWrappers()
katie.SL.library <- c("SL.mean", 
                      "SL.glm", 
                      "SL.glm.interaction", 
                      "SL.bayesglm"
                      # "SL.extraTrees"
                      # "SL.glmnet"
                      # new.algorithm.2$names
                      # "SL.rpartPrune" # this one worked
                      )

#------------------------------------------
# 2. Estimate Qbar_0(A,W) with Super Learner
#------------------------------------------
# dataframe X with baseline covariates and exposure
X<-subset(ObsData, 
          select=c(A, 
                   # air_basin, 
                   afr_am_percent, 
                   asian_filipino_percent, 
                   two_none_aian_pi_percent, 
                   hisp_latinx_percent, 
                   english_learner_percent, 
                   frpm_percent, 
                   district_cost_per_pupil, 
                   student_teacher_ratio, 
                   cred_percent,
                   school_k_12_enrollment))

# set the exposure=1 in X1 and the exposure=0 in X0
X1 <- X0 <- X
X1$A <- 1 # under exposure
X0$A <- 0 # under control

# call Super Learner
QbarSL <- snowSuperLearner(Y = ObsData$Y, 
                           X = X, 
                           SL.library = katie.SL.library,
                           cluster = cluster, 
                           family = "binomial")
QbarSL

summary(QbarSL)


# get the expected PM2.5 , given the observed exposure and covariates
QbarAW <- predict(QbarSL, newdata=ObsData)$pred
# expected injury severity, given A=1 and covariates
Qbar1W <- predict(QbarSL, newdata=X1)$pred
# expected injury severity, given A=0 and covariates
Qbar0W <- predict(QbarSL, newdata=X0)$pred
# the fitted value at the observed exposure should equal the fitted value
# under when A=a
tail(data.frame(A=ObsData$A, QbarAW, Qbar1W, Qbar0W))

# note the simple substitution estimator would be
PsiHat.SS <- mean(Qbar1W - Qbar0W)
PsiHat.SS

#------------------------------------------
# Playing with LTMLE
#------------------------------------------

# run with ltmle for comparison
ltmle_data <- 
  dataset_complete_cases_working %>% 
  mutate(id_cds = as.numeric(as.factor(cds_code))) %>% 
  dplyr::select(air_basin, 
                afr_am_percent, 
                asian_filipino_percent, 
                two_none_aian_pi_percent, 
                hisp_latinx_percent, 
                english_learner_percent, 
                frpm_percent, 
                district_cost_per_pupil, 
                student_teacher_ratio, 
                cred_percent,
                school_k_12_enrollment, 
                Y = math_proportion,
                A = pm25_12_plus)

ltmle_data <- as.data.frame(ltmle_data)

ltmle_result <- ltmle(data = ltmle_data, 
                Anodes = 'A', 
                Ynodes = 'Y', 
                abar = list(1, 0),
                SL.library = katie.SL.library)

summary(ltmle_result) # Stephen: OMG!!!!! good to see TMLE does the weird "basically equal means but stat sig diff" thing for both of us 

ltmle_result$fit

```


```{r KATIE evaluating positivity}

# call Super Learner for the exposure mechanism
gHatSL <- SuperLearner(Y=ObsData$A, 
                       X=subset(ObsData, 
                                select= -c(Y, A)),
                       SL.library = katie.SL.library, 
                       family="binomial")

# generate predicted prob being exposed, given baseline covariates
gHat1W <- gHatSL$SL.predict

# predicted prob of not being exposed, given baseline covariates
gHat0W <- 1- gHat1W

# # predicted prob of observed exposure, given baseline cov
gHatAW <- rep(NA, n = nrow(ObsData))
gHatAW[ObsData$A==1] <- gHat1W[ObsData$A==1]
gHatAW[ObsData$A==0] <- gHat0W[ObsData$A==0]

# this gives a little summary of the distribution
summary(gHatAW)

# calculate weights
wt <- 1/gHatAW

summary(wt)

```



```{r KATIE attempt at full code set from lab 6}

# run.tmle <- function(ObsData, SL.library, id = NULL){
# 
# #------------------------------------------
# # Estimate the conditional mean outcome Qbar(A,W)
# #------------------------------------------
# 
# # dataframe X with baseline covariates and exposure
# X <- subset(ObsData, 
#             select = c(pm25_12_plus, 
#                        air_basin, 
#                        afr_am_percent, 
#                        asian_filipino_percent, 
#                        two_none_aian_pi_percent, 
#                        hisp_latinx_percent, 
#                        english_learner_percent, 
#                        frpm_percent, 
#                        district_cost_per_pupil, 
#                        student_teacher_ratio))
# 
# # set the A=1 in X1 and the A=0 in X0
# X1 <- X0 <- X
# X1$pm25_12_plus <- 1 # under exposure
# X0$pm25_12_plus <- 0 # under control
# 
# # call Super Learner for estimation of QbarAW
# QbarSL<- SuperLearner(Y = ObsData$math_standard_met, 
#                       X = X, 
#                       SL.library = SL.library, 
#                       family = "gaussian", 
#                       id = id)
# # QbarSL
# 
# # initial estimates of the outcome, given the observed exposure & covariates
# QbarAW <- predict(QbarSL, newdata=ObsData)$pred
# # estimates of the outcome, given A=1 and covariates
# Qbar1W<- predict(QbarSL, newdata=X1)$pred
# # estimates of the outcome, given A=0 and covariates
# Qbar0W<- predict(QbarSL, newdata=X0)$pred
# 
# # simple substitution estimator:
# PsiHat.SS<-mean(Qbar1W - Qbar0W)
# 
# #------------------------------------------
# # Estimate the exposure mechanism g(pm25_12_plus|W)
# #------------------------------------------
# 
# # call Super Learner for the exposure mechanism
# gHatSL<- SuperLearner(Y = ObsData$pm25_12_plus, 
#                       X = subset(ObsData, select= -c(pm25_12_plus,
#                                                      math_standard_met,
#                                                      id)),
# SL.library = SL.library, family="binomial", id=id)
# # generate predicted prob being exposed, given baseline covariates
# gHat1W <- gHatSL$SL.predict
# # predicted prob of not being exposed, given baseline covariates
# gHat0W <- 1- gHat1W
# 
# # # predicted prob of observed exposure, given baseline cov
# # gHatAW<- rep(NA, n)
# # gHatAW[ObsData$A==1]<- gHat1W[ObsData$A==1]
# # gHatAW[ObsData$A==0]<- gHat0W[ObsData$A==0]
# 
# #-------------------------------------------------
# # Clever covariate H(A,W) for each subject
# #-------------------------------------------------
# H.AW<- as.numeric(ObsData$pm25_12_plus==1)/gHat1W - as.numeric(ObsData$pm25_12_plus==0)/gHat0W
# 
# # also want to evaluate the clever covariates at A=1 and A=0 for all subjects
# H.1W<- 1/gHat1W
# H.0W<- -1/gHat0W
# 
# 
# #IPTW estimator of the G-computation formula:
# PsiHat.IPTW <-mean( H.AW*ObsData$Y)
# 
# #------------------------------------------
# # Update the initial estimator of Qbar_0(A,W)
# #------------------------------------------
# logitUpdate<- glm(ObsData$Y ~ -1 +offset(qlogis(QbarAW)) + H.AW, family='binomial')
# epsilon <- logitUpdate$coef
# 
# QbarAW.star<- plogis(qlogis(QbarAW)+ epsilon*H.AW)
# Qbar1W.star<- plogis(qlogis(Qbar1W)+ epsilon*H.1W)
# Qbar0W.star<- plogis(qlogis(Qbar0W)+ epsilon*H.0W)
# 
# #------------------------------------------
# # Estimate Psi(P_0)
# #------------------------------------------
# 
# PsiHat.TMLE <- mean(Qbar1W.star - Qbar0W.star)
# 
# #------------------------------------------
# # Return point estimates, targeted estimates of Qbar_0(A,W),
# # and thevector of clever covariates
# #------------------------------------------
# 
# estimates <- data.frame(cbind(PsiHat.SS=PsiHat.SS, PsiHat.IPTW, PsiHat.TMLE))
# Qbar.star <- data.frame(cbind(QbarAW.star, Qbar1W.star, Qbar0W.star))
# names(Qbar.star)<- c('QbarAW.star', 'Qbar1W.star', 'Qbar0W.star')
# 
# list(estimates=estimates, Qbar.star=Qbar.star, H.AW=H.AW)
# }
# 
# out <- run.tmle(ObsData = ObsData, 
#                 SL.library = katie.SL.library)
# 
# est <- out$estimates
# est*100



```



```{r g-computation by KATIE, eval=FALSE, include=FALSE}

set.seed(252)
# specify SL library
  listWrappers()
  SL.library <- c("SL.mean","SL.glm")
  other_options <- c("SL.glm.interaction","SL.randomForest","SL.polymars","SL.rpartPrune","SL.biglasso")

```


```{r checking positivity assumption by KATIE, eval=FALSE, include=FALSE}



```


# BELOW LIES STEPHEN'S CODE


```{r preliminary data analysis BY STEPHEN, eval=FALSE, include=FALSE}
# make into a dataframe
  dataset_complete_cases <- as.data.frame(dataset_complete_cases)
# basic bits about data
  names(dataset_complete_cases)
  (n <- nrow(dataset_complete_cases)) # checks out with Katie's description -> yay, using correct dataset
  describe(dataset_complete_cases)
    unique(dataset_complete_cases$air_district)
    unique(dataset_complete_cases$pm25_12_plus)
    unique(dataset_complete_cases$cds_code)
# descriptives for numeric columns    
  num_cols <- c("math_standard_met","modeled_air_at_school","afr_am_percent","asian_filipino_percent","two_none_aian_pi_percent","hisp_latinx_percent","white_percent","english_learner_percent","frpm_percent","district_cost_per_pupil","student_teacher_ratio","cred_percent","school_k_12_enrollment")
  str(dataset_complete_cases[,num_cols])
  describe(dataset_complete_cases[,num_cols])
  by(dataset_complete_cases[,num_cols],list(dataset_complete_cases$pm25_12_plus),describe)
  aggregate(dataset_complete_cases[,num_cols],list(dataset_complete_cases$pm25_12_plus),
            function(x) {round(c(length(x),mean(x,na.rm=T),sd(x,na.rm=T)),2)})
    
  for (i in 1:length(num_cols)) {hist(dataset_complete_cases[,num_cols[i]],main=num_cols[i],breaks=20)}  
    # looks like there are some outlier for student-teacher ratio and cost per pupil (one school spending $346k per student?)
    dataset_complete_cases[dataset_complete_cases$district_cost_per_pupil>50000,] # ok so the Bay Area has $$$$
    unique(dataset_complete_cases[dataset_complete_cases$district_cost_per_pupil>50000,c("air_district","district_cost_per_pupil")])
    cost_hist <- hist(dataset_complete_cases$district_cost_per_pupil,breaks=20,plot=FALSE)
    plot(cost_hist$counts,log="x",type="h",lwd=10,lend=2)
    
    dataset_complete_cases[dataset_complete_cases$student_teacher_ratio>50,]
    table(dataset_complete_cases$air_district) # 70 schools in Feather River. why STR so high??
    
  round(prop.table(table(dataset_complete_cases$air_district)),3)
  round(prop.table(table(dataset_complete_cases$air_basin_binned)),3) # much better
# make A variable into numeric & Y variable proportion
  dataset_complete_cases$pm25_12_plus <- ifelse(dataset_complete_cases$pm25_12_plus=="yes",1,0)
  dataset_complete_cases$math_standard_met <- dataset_complete_cases$math_standard_met/100
# correlations between predictors
  predictor_corrs <- Hmisc::rcorr(as.matrix(dataset_complete_cases[,c("pm25_12_plus",num_cols)]))$r
  #openxlsx::write.xlsx(list(predictor_corrs),"~/Desktop/PH252D/cifp/output_for_writeups/predictor_correlations.xlsx",colNames=T,rowNames=T)
  
```

```{r iptw & tmle BY STEPHEN, eval=FALSE, include=FALSE}
set.seed(252)
# specify SL library
  listWrappers()
  SL.library <- c("SL.mean","SL.glm","SL.glm.interaction","SL.bayesglm")
  other_options <- c("SL.glm.interaction","SL.randomForest","SL.polymars","SL.rpartPrune")
# SS, IPTW, & TMLE function
  run.tmle <- function(Y,A,ObsData, SL.library, id=NULL) {
   
    #------------------------------------------
    # Estimate the conditional mean outcome Qbar(A,W)
    #------------------------------------------
   
      # dataframe X with baseline covariates and exposure
        X <- ObsData[,names(ObsData) != Y]
      # settheA=1inX1andtheA=0inX0
        X1 <- X0<-X
        X1[,A] <- 1 # under exposure
        X0[,A] <- 0 # under control
    
       # call Super Learner for estimation of QbarAW
        QbarSL<- SuperLearner(Y=ObsData[,Y], X=X, SL.library=SL.library, family="binomial", id=id)
        # QbarSL
        
       # initial estimates of the outcome, given the observed exposure & covariates
        QbarAW <- predict(QbarSL, newdata=ObsData)$pred
       # estimates of the outcome, given A=1 and covariates
        Qbar1W<- predict(QbarSL, newdata=X1)$pred
       # estimates of the outcome, given A=0 and covariates
        Qbar0W<- predict(QbarSL, newdata=X0)$pred
    
       # simple substitution estimator:
        PsiHat.SS<-mean(Qbar1W - Qbar0W)
    
    #------------------------------------------
    # Estimate the exposure mechanism g(A|W)
    #------------------------------------------
  
     # call Super Learner for the exposure mechanism
      gHatSL<- SuperLearner(Y=ObsData[,A], X=ObsData[,!(names(ObsData) %in% c(A,Y,id))],SL.library=SL.library, family="binomial", id=id)
     # generate predicted prob being exposed, given baseline covariates
      gHat1W<- gHatSL$SL.predict
     # predicted prob of not being exposed, given baseline covariates
      gHat0W<- 1- gHat1W
  
     # predicted prob of observed exposure, given baseline cov
      gHatAW<- rep(NA, n)
      gHatAW[ObsData[,A]==1]<- gHat1W[ObsData[,A]==1]
      gHatAW[ObsData[,A]==0]<- gHat0W[ObsData[,A]==0]
      wt <- 1/gHatAW
  
    #-------------------------------------------------
    # Clever covariate H(A,W) for each subject
    #-------------------------------------------------
      H.AW<- as.numeric(ObsData[,A]==1)/gHat1W - as.numeric(ObsData[,A]==0)/gHat0W
  
     # also want to evaluate the clever covariates at A=1 and A=0 for all subjects
       H.1W<- 1/gHat1W
       H.0W<- -1/gHat0W
       
     # IPTW estimator of the G-computation formula: -> INCLUDE HORVITZ-THOMPSON 
      PsiHat.IPTW <-mean( H.AW*ObsData[,Y])
      
     # IPTW estimator of G-comp w Horvitz-Thompson adjusmtent
      PsiHat.IPTW_HT <- mean(wt*as.numeric(ObsData[,A]==1)*ObsData[,Y])/mean(wt*as.numeric(ObsData[,A]==1)) - 
                        mean(wt*as.numeric(ObsData[,A]==0)*ObsData[,Y])/mean(wt*as.numeric(ObsData[,A]==0))
  
    #------------------------------------------
    # Update the initial estimator of Qbar_0(A,W)
    #------------------------------------------
      logitUpdate<- glm(ObsData[,Y] ~ -1 +offset(qlogis(QbarAW)) + H.AW, family='binomial')
      epsilon <- logitUpdate$coef
  
      QbarAW.star<- plogis(qlogis(QbarAW)+ epsilon*H.AW)
      Qbar1W.star<- plogis(qlogis(Qbar1W)+ epsilon*H.1W)
      Qbar0W.star<- plogis(qlogis(Qbar0W)+ epsilon*H.0W)
  
    #------------------------------------------
    # Estimate Psi(P_0)
    #------------------------------------------
  
      PsiHat.TMLE <- mean(Qbar1W.star - Qbar0W.star)
  
    #------------------------------------------
    # Return point estimates, targeted estimates of Qbar_0(A,W),
    # and thevector of clever covariates
    #------------------------------------------
  
      estimates <- data.frame(cbind(PsiHat.SS=PsiHat.SS, PsiHat.IPTW, PsiHat.IPTW_HT, PsiHat.TMLE))
      Qbar.star <- data.frame(cbind(QbarAW.star, Qbar1W.star, Qbar0W.star))
      names(Qbar.star)<- c('QbarAW.star', 'Qbar1W.star', 'Qbar0W.star')
      list(estimates=estimates, Qbar.star=Qbar.star, H.AW=H.AW)
  }
# practice run of function
  # subset of rows
    sub_data <- dataset_complete_cases[sample(1:n,1000,replace=F),]
    nrow(sub_data)
    duplicated(sub_data$cds_code)
    table(sub_data$pm25_12_plus)
  # run function
    names(sub_data)
    include_vars_analysis <- c("math_standard_met","pm25_12_plus","air_basin_binned","afr_am_percent","asian_filipino_percent","two_none_aian_pi_percent","hisp_latinx_percent",
                               "english_learner_percent","frpm_percent","district_cost_per_pupil","student_teacher_ratio","cred_percent","school_k_12_enrollment")
    out <- run.tmle(Y="math_standard_met", A="pm25_12_plus", ObsData=sub_data[,include_vars_analysis], SL.library=SL.library) 
      # generating a ton of errors. removing algorithms from SuperLearner
      # think interaction algorithms will be super problematic bc number of covariates is really high from all the districts
      # FIGURED OUT ISSUE: When we resample, not all districts are in each sample and predictions are generated for non-existent districts and not generated for extisting districts
    est <- out$estimates
    est*100
# the real thing
  out <- run.tmle(Y="math_standard_met", A="pm25_12_plus", ObsData=dataset_complete_cases[,include_vars_analysis], SL.library=SL.library) 
  est <- out$estimates
  est*100
# real thing with ltmle for comparison  
  dataset_complete_cases_ltmle <- dataset_complete_cases[,include_vars_analysis]
  names(dataset_complete_cases_ltmle)
  dataset_complete_cases_ltmle <- dataset_complete_cases_ltmle[,c(3:13,1:2)]
  ltmle.SL<- ltmle(data=dataset_complete_cases_ltmle, Anodes='pm25_12_plus', Ynodes='math_standard_met', abar=list(1,0),SL.library=SL.library)  
  summary(ltmle.SL)  
   
# run with ltmle for comparison
  sub_data2 <- sub_data[,include_vars_analysis]
  names(sub_data2)
  sub_data2 <- sub_data2[,c(names(sub_data2[3:13]),"math_standard_met","pm25_12_plus")]
  ltmle.SL<- ltmle(data=sub_data2, Anodes='pm25_12_plus', Ynodes='math_standard_met', abar=list(1,0),SL.library=SL.library)  
  summary(ltmle.SL)  

```

```{r, Inference Curve!!!!, eval=FALSE, include=FALSE}
# how many cores
  parallel::detectCores(all.tests = FALSE, logical = TRUE)
# non-parametric bootstrap
nonparam_boot <- function(ObsData,A,Y,id,SL.library,num_boots) {
  # make output matrix
    estimates <- matrix(NA,nrow=num_boots,ncol=4)
  # bootstrap data
    for (i in 1:num_boots) {
      # make data
        n <- nrow(ObsData)
        ObsData[,id] <- 1:n
        bootIndices<- sample(1:n, replace=T)
        bootData<- ObsData[bootIndices,]
      # estimate target parameters
        # simple sub
          X <- bootData[,names(ObsData) != Y]
          X1 <- X; X1[,A] <- 1
          X0 <- X; X0[,A] <- 0
          QbarSL<- SuperLearner(Y=bootData[,Y], X=X, SL.library=SL.library, family="binomial", id=bootData[,id])
          QbarAW <- predict(QbarSL, newdata=bootData)$pred
          Qbar1W<- predict(QbarSL, newdata=X1)$pred
          Qbar0W<- predict(QbarSL, newdata=X0)$pred
          simple_sub <- mean(Qbar1W-Qbar0W)
        # IPTW
          gHatSL<- SuperLearner(Y=bootData[,A], X=bootData[,!(names(bootData) %in% c(A,Y,id))], SL.library=SL.library, family="binomial", id=bootData[,id])
          gHat1W <- gHatSL$SL.predict
          gHat0W <- 1-gHatSL$SL.predict
          gHatAW <- ifelse(bootData[,A]==1,gHat1W,gHat0W)
          wt <- 1/gHatAW
          H.AW <- as.numeric(bootData[,A]==1)/gHat1W - as.numeric(bootData[,A]==0)/gHat0W
          H.1W <- as.numeric(X1[,A]==1)/gHat1W - as.numeric(X1[,A]==0)/gHat0W
          H.0W <- as.numeric(X0[,A]==1)/gHat1W - as.numeric(X0[,A]==0)/gHat0W
          iptw <- mean(H.AW*bootData[,Y])
          iptw_ht <- mean(wt*as.numeric(bootData[,A]==1)*bootData[,Y])/mean(wt*as.numeric(bootData[,A]==1)) - 
                     mean(wt*as.numeric(bootData[,A]==0)*bootData[,Y])/mean(wt*as.numeric(bootData[,A]==0))
        # TMLE
          logitUpdate <- glm(bootData[,Y] ~ -1 +offset(qlogis(QbarAW)) + H.AW,family='binomial')
          epsilon <- logitUpdate$coef
          QbarAW.star <- plogis(qlogis(QbarAW)+ epsilon*H.AW)
          Qbar1W.star <- plogis( qlogis(Qbar1W)+ epsilon*H.1W)
          Qbar0W.star <- plogis( qlogis(Qbar0W)+ epsilon*H.0W)
          PsiHat.TMLE <- mean(Qbar1W.star- Qbar0W.star)
      # save results
        estimates[i,] <- c(simple_sub,iptw,iptw_ht,PsiHat.TMLE)
    }
    return(estimates)
}  
estimates <- nonparam_boot(ObsData=dataset_complete_cases[,c("cds_code",include_vars_analysis)],
                           A="pm25_12_plus",Y="math_standard_met",id="cds_code",SL.library=SL.library,num_boots=500) 
summary(estimates)  
apply(estimates,2,hist)

create.CI <- function(pt, boot, alpha=0.05){
   Zquant <- qnorm(alpha/2, lower.tail=F)
   CI.normal <- c(pt - Zquant*sd(boot), pt + Zquant*sd(boot) )
   CI.quant  <- quantile(boot, prob=c(0.025,0.975) )
   out<- data.frame(rbind(CI.normal, NA))*100
 colnames(out)<- c('CI.lo', 'CI.hi')
 out
}
apply(estimates,2,function(x) create.CI(pt=mean(x,na.rm=T),boot=x))
apply(estimates,2,function(x) quantile(x,probs=c(.025,.975),na.rm=T))



```




```{r, KATIE PARALLEL Inference Curve!!!! DOES NOT RUN YET, eval=FALSE, include=FALSE}

# set up parallelization (my computer has 8 cores and this uses 7 of them)
(cluster = parallel::makeCluster(7))
parallel::clusterEvalQ(cluster, library(SuperLearner))

# load dataset
load(file = file.path("dataset_complete_cases.rdata"))

# non-parametric bootstrap
nonparam_boot_parallel <- function(ObsData,A,Y,id,SL.library,num_boots) {
  # make output matrix
    estimates <- matrix(NA,nrow=num_boots,ncol=4)
  # bootstrap data
    for (i in 1:num_boots) {
      # make data
        n <- nrow(ObsData)
        ObsData[,id] <- 1:n
        bootIndices<- sample(1:n, replace=T)
        bootData<- ObsData[bootIndices,]
      # estimate target parameters
        # simple sub
          X <- bootData[,names(ObsData) != Y]
          X1 <- X; X1[,A] <- 1
          X0 <- X; X0[,A] <- 0
          QbarSL<- snowSuperLearner(Y=bootData[,Y], X=X, SL.library=SL.library, family="binomial", id=bootData[,id], cluster = cluster)
          QbarAW <- predict(QbarSL, newdata=bootData)$pred
          Qbar1W<- predict(QbarSL, newdata=X1)$pred
          Qbar0W<- predict(QbarSL, newdata=X0)$pred
          simple_sub <- mean(Qbar1W-Qbar0W)
        # IPTW
          gHatSL<- snowSuperLearner(Y=bootData[,A], X=bootData[,!(names(bootData) %in% c(A,Y,id))], SL.library=SL.library, family="binomial", id=bootData[,id], cluster = cluster)
          gHat1W <- gHatSL$SL.predict
          gHat0W <- 1-gHatSL$SL.predict
          gHatAW <- ifelse(bootData[,A]==1,gHat1W,gHat0W)
          wt <- 1/gHatAW
          H.AW <- as.numeric(bootData[,A]==1)/gHat1W - as.numeric(bootData[,A]==0)/gHat0W
          H.1W <- as.numeric(X1[,A]==1)/gHat1W - as.numeric(X1[,A]==0)/gHat0W
          H.0W <- as.numeric(X0[,A]==1)/gHat1W - as.numeric(X0[,A]==0)/gHat0W
          iptw <- mean(H.AW*bootData[,Y])
          iptw_ht <- mean(wt*as.numeric(bootData[,A]==1)*bootData[,Y])/mean(wt*as.numeric(bootData[,A]==1)) - 
                     mean(wt*as.numeric(bootData[,A]==0)*bootData[,Y])/mean(wt*as.numeric(bootData[,A]==0))
        # TMLE
          logitUpdate <- glm(bootData[,Y] ~ -1 +offset(qlogis(QbarAW)) + H.AW,family='binomial')
          epsilon <- logitUpdate$coef
          QbarAW.star <- plogis(qlogis(QbarAW)+ epsilon*H.AW)
          Qbar1W.star <- plogis( qlogis(Qbar1W)+ epsilon*H.1W)
          Qbar0W.star <- plogis( qlogis(Qbar0W)+ epsilon*H.0W)
          PsiHat.TMLE <- mean(Qbar1W.star- Qbar0W.star)
      # save results
        estimates[i,] <- c(simple_sub,iptw,iptw_ht,PsiHat.TMLE)
    }
    return(estimates)
}  


include_vars_analysis <- c("math_standard_met","pm25_12_plus","air_basin_binned","afr_am_percent","asian_filipino_percent","two_none_aian_pi_percent","hisp_latinx_percent", "english_learner_percent","frpm_percent","district_cost_per_pupil","student_teacher_ratio","cred_percent","school_k_12_enrollment")

SL.library <- c("SL.mean","SL.glm","SL.glm.interaction","SL.bayesglm")

estimates <- nonparam_boot_parallel(ObsData=dataset_complete_cases[,c("cds_code",include_vars_analysis)],
                           A="pm25_12_plus",Y="math_standard_met",id="cds_code",SL.library=SL.library,num_boots=5) 
summary(estimates)  
apply(estimates,2,hist)

create.CI <- function(pt, boot, alpha=0.05){
   Zquant <- qnorm(alpha/2, lower.tail=F)
   CI.normal <- c(pt - Zquant*sd(boot), pt + Zquant*sd(boot) )
   CI.quant  <- quantile(boot, prob=c(0.025,0.975) )
   out<- data.frame(rbind(CI.normal, NA))*100
 colnames(out)<- c('CI.lo', 'CI.hi')
 out
}
apply(estimates,2,function(x) create.CI(pt=mean(x,na.rm=T),boot=x))
apply(estimates,2,function(x) quantile(x,probs=c(.025,.975),na.rm=T))



```































