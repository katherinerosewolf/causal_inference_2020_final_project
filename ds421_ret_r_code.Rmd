---
title: "DS421 Summer 2019 R Markdown Document"
author: "Katherine Rose Wolf"
date: "August 4, 2019"
output: html_document
---

```{r setup load libraries}

library(tidyverse)
library(praise)
library(raster)
library(tigris)
library(sf)

options("scipen" = 40)

praise()  # self-esteem boost

```


```{r create file structure}

# create raw data file
if(!dir.exists("raw_data")) {
  dir.create("raw_data")
}

if(!dir.exists("intermediate_data")) {
  dir.create("intermediate_data")
}

if(!dir.exists("functions")) {
  dir.create("functions")
}

```


```{r functions for general later use}

# function to convert column names to snake case
column_name_fixer <-
  function(tibble) {
  new_names <-
    tibble %>%
    colnames() %>%  # gets the column names from the tibble
    tolower() %>%  # converts the column names to lowercase
    {gsub(" ", "_", .)}  # replaces spaces with "_"
  colnames(tibble) <-
    new_names  # rename the columns in the original tibble
  return(tibble)
  }

# create function that downloads files
file_downloader <-
  function(desired_filename_as_string, 
           web_address_as_string, 
           folder_as_string) {
    
    path <-  # specify the file path
      file.path(
        folder_as_string,
        desired_filename_as_string) 
    
    if(!file.exists(path))  # checks if file exists already
      download.file(url = web_address_as_string,  # if not, downloads/saves file
                    destfile = path, 
                    mode = "wb")
  }

# save functions for later use
save(
  column_name_fixer, 
     file = 
       file.path(
         "functions", 
         "column_name_fixer.rdata"
       )
)

save(
  file_downloader, 
     file = 
       file.path(
         "functions", 
         "file_downloader.rdata"
       )
)

# remove the functions
rm(
  column_name_fixer, 
  file_downloader, 
  download_generated_data_auto
)
invisible(gc())

```




```{r create file structure}

# create raw data file
if(!dir.exists("raw_data")) {
  dir.create("raw_data")
}

if(!dir.exists("intermediate_data")) {
  dir.create("intermediate_data")
}

```

# raw data downloads

## modeled pm2.5

```{r download modeled air pollution data}

# download file from 

# download air pollution file .nc edition
download.file(
  url = "http://fizz.phys.dal.ca/~atmos/datasets/EST2019/GWRwSPEC_PM25_NA_201601_201612-RH35.nc", 
  destfile = 
    file.path(
      "raw_data", 
      "GWRwSPEC_PM25_NA_201601_201612-RH35.nc"
    )
  )

# download air pollution file .asc edition
download.file(
  url = "http://fizz.phys.dal.ca/~atmos/datasets/EST2019/GWRwSPEC_PM25_NA_201601_201612-RH35-NoNegs.asc.zip", 
  destfile = 
    file.path(
      "raw_data", 
      "GWRwSPEC_PM25_NA_201601_201612-RH35-NoNegs.asc.zip"
    )
  )

```

## geographic shapefile

```{r load united states state shapefiles}

# download the raw california shapefile
state_shapefiles <- 
  states(
          cb = FALSE, 
          year = 2016, 
          class = 'sf'
        )

# save raw file to disk
save(
  state_shapefiles, 
  file = 
    file.path(
      "raw_data", 
      "state_shapefiles.rdata"
    )
)

```


## school enrollment

```{r download school enrollment data}

# school enrollment data by racial/ethnic designation, gender, and grade

# make school enrollment raw downloader function
school_enrollment_raw_downloader <-
  function(start_year_numeric){
    end_year_chr <- 
      as.character(start_year_numeric + 1) %>%  # add 1 to the start year
      substr(3, 4)  # get only the last two digits
    
    start_year_chr <- 
      as.character(start_year_numeric)  # convert start year to numeric
    
    url_file <- 
      str_c("http://dq.cde.ca.gov/dataquest/dlfile/dlfile.aspx?cLevel=School&cYear=", 
            start_year_chr, 
            "-", 
            end_year_chr, 
            "&cCat=Enrollment&cPage=filesenr.asp")
    
    download.file(
      url = url_file, 
      destfile = 
        file.path(
          "raw_data", 
          str_c("school_enrollment_", 
                start_year_chr,
                "_", 
                end_year_chr, 
                "_raw.txt")
        )
    )
  }

# make vector of starting years 2009-2018
start_years <- 
  seq(
    from = 2009,
    to = 2018, 
    by = 1
  )

# download all the school files at once
map(
  .f = school_enrollment_raw_downloader, 
  .x = start_years
)

```

## school addresses

```{r download school addresses}

# download school addresses
download.file(
  url = "https://www.cde.ca.gov/schooldirectory/report?rid=dl1&tp=txt", 
  destfile = 
    file.path(
      "raw_data", 
      "public_school_directory.txt"
    )
  )
```

## teaching credentials

```{r download teaching credential data}

# download teaching credential data 2015-16
download.file(
  url = "ftp://ftp.cde.ca.gov/demo/staffclass/StaffCred15.txt", 
  destfile = 
    file.path(
      "raw_data", 
      "StaffCred15.txt"
    ), 
  cacheOK = F, 
  method = 'curl', 
  extra = '-L',
  mode = 'wb'
  )

# download teaching credential data 2016-17
download.file(
  url = "ftp://ftp.cde.ca.gov/demo/staffclass/StaffCred16.txt", 
  destfile = 
    file.path(
      "raw_data", 
      "StaffCred16.txt"
    ), 
  cacheOK = F, 
  method = 'curl', 
  extra = '-L',
  mode = 'wb'
  )

```

## monitors

```{r download carb monitor data for 2015 to 2018}

monitor_urls <- 
  list()

monitor_urls$daily_pm25_monitors_2018 <- 
  "https://www.arb.ca.gov/aqmis2/display.php?download=y&param=PM25&units=001&year=2018&county_name=--COUNTY--&basin=--AIR+BASIN--&latitude=--PART+OF+STATE--&report=SITELIST&order=basin%2Ccounty_name%2Cs.name&submit=Retrieve+Data&ptype=aqd&std15="
  
monitor_urls$hourly_pm25_monitors_2018 <- 
  "https://www.arb.ca.gov/aqmis2/display.php?download=y&param=PM25HR&units=001&year=2018&county_name=--COUNTY--&basin=--AIR+BASIN--&latitude=--PART+OF+STATE--&report=SITELIST&order=basin%2Ccounty_name%2Cs.name&submit=Retrieve+Data&ptype=aqd&std15="

monitor_urls$daily_pm25_monitors_2017 <- 
  "https://www.arb.ca.gov/aqmis2/display.php?download=y&param=PM25&units=001&year=2017&county_name=--COUNTY--&basin=--AIR+BASIN--&latitude=--PART+OF+STATE--&report=SITELIST&order=basin%2Ccounty_name%2Cs.name&submit=Retrieve+Data&ptype=aqd&std15="
  
monitor_urls$hourly_pm25_monitors_2017 <- 
  "https://www.arb.ca.gov/aqmis2/display.php?download=y&param=PM25HR&units=001&year=2017&county_name=--COUNTY--&basin=--AIR+BASIN--&latitude=--PART+OF+STATE--&report=SITELIST&order=basin%2Ccounty_name%2Cs.name&submit=Retrieve+Data&ptype=aqd&std15="

monitor_urls$daily_pm25_monitors_2016 <- 
  "https://www.arb.ca.gov/aqmis2/display.php?download=y&param=PM25&units=001&year=2016&county_name=--COUNTY--&basin=--AIR+BASIN--&latitude=--PART+OF+STATE--&report=SITELIST&order=basin%2Ccounty_name%2Cs.name&submit=Retrieve+Data&ptype=aqd&std15="
  
monitor_urls$hourly_pm25_monitors_2016 <- 
  "https://www.arb.ca.gov/aqmis2/display.php?download=y&param=PM25HR&units=001&year=2016&county_name=--COUNTY--&basin=--AIR+BASIN--&latitude=--PART+OF+STATE--&report=SITELIST&order=basin%2Ccounty_name%2Cs.name&submit=Retrieve+Data&ptype=aqd&std15="
  
monitor_urls$daily_pm25_monitors_2015 <- 
    "https://www.arb.ca.gov/aqmis2/display.php?download=y&param=PM25&units=001&year=2015&county_name=--COUNTY--&basin=--AIR+BASIN--&latitude=--PART+OF+STATE--&report=SITELIST&order=basin%2Ccounty_name%2Cs.name&submit=Retrieve+Data&ptype=aqd&std15="
  
monitor_urls$hourly_pm25_monitors_2015 <- 
  "https://www.arb.ca.gov/aqmis2/display.php?download=y&param=PM25HR&units=001&year=2015&county_name=--COUNTY--&basin=--AIR+BASIN--&latitude=--PART+OF+STATE--&report=SITELIST&order=basin%2Ccounty_name%2Cs.name&submit=Retrieve+Data&ptype=aqd&std15="

# make function to download files and save them in raw data
carb_pm25_monitor_raw_downloader <-
  function(monitor_url, 
           object_name){
    download.file(
      url = monitor_url, 
      destfile = 
        file.path(
          "raw_data", 
          str_c(object_name,
                "_raw.txt")
        ), 
      cacheOK = F, 
      method = 'curl', 
      extra = '-L',
      mode = 'wb'
    )
  }

carb_pm25_monitor_raw_downloader(
  "https://www.arb.ca.gov/aqmis2/display.php?download=y&param=PM25&units=001&year=2016&county_name=--COUNTY--&basin=--AIR+BASIN--&latitude=--PART+OF+STATE--&report=SITELIST&order=basin%2Ccounty_name%2Cs.name&submit=Retrieve+Data&ptype=aqd&std15=", 
  "thingie"
)

# download files from the internet
map2(
  .x = monitor_urls, 
  .y = names(monitor_urls),
  .f = carb_pm25_monitor_raw_downloader
)

```

```{r download epa monitor data for 2016}

# download federal reference method (parameter code 88101) data
download.file(
  url = 
    "https://aqs.epa.gov/aqsweb/airdata/daily_88101_2016.zip", 
  destfile = 
    file.path("raw_data", 
              "daily_88101_2016.zip"), 
  cacheOK = F, 
  method = 'curl', 
  extra= '-L',
  mode = 'wb'
  )

# download non-federal-reference-method (parameter code 88502) data
download.file(
  url = 
    "https://aqs.epa.gov/aqsweb/airdata/daily_88502_2016.zip", 
  destfile = 
    file.path("raw_data", 
              "daily_88502_2016.zip"), 
  cacheOK = F, 
  method = 'curl', 
  extra= '-L',
  mode = 'wb'
  )

# download speciation data
download.file(
  url = 
    "https://aqs.epa.gov/aqsweb/airdata/daily_SPEC_2016.zip", 
  destfile = 
    file.path("raw_data", 
              "daily_SPEC_2016.zip"), 
  cacheOK = F, 
  method = 'curl', 
  extra= '-L',
  mode = 'wb'
)

```


# data preparation

## pm2.5 data

```{r read modeled air pollution data into r}

# unzip the air pollution data file
unzip(
  zipfile = 
    file.path(
      "raw_data", 
      "GWRwSPEC_PM25_NA_201601_201612-RH35-NoNegs.asc.zip"
    ), 
  exdir = 
    file.path(
      "intermediate_data"
    )
  )

# create the working raster
asc_air_data <- 
  raster(
    file.path(
      "intermediate_data", 
      "GWRwSPEC_PM25_NA_201601_201612-RH35-NoNegs.asc"
    )
  )

# save air data raster
save(
  asc_air_data, 
  file = 
    file.path(
      "intermediate_data", 
      "asc_air_data.rdata"
    )
)

```


## california shapefile

```{r pull just california shapefile}

# load us shapefiles
load(
  file = 
    file.path(
      "raw_data", 
      "state_shapefiles.rdata"
    )
)

# load asc air data
load(
  file = 
    file.path(
      "intermediate_data", 
      "asc_air_data.rdata"
    )
)

# isolate california shapefile
california_shapefile <- 
  state_shapefiles %>% 
  filter(STUSPS == "CA") %>%  # get just california
  st_transform(crs = proj4string(asc_air_data))  # convert to pm2.5 coordinates

save(
  california_shapefile, 
  file = 
    file.path(
      "intermediate_data", 
      "california_shapefile.rdata"
    )
)

```


## california shapefile and pm2.5

```{r clip air pollution data to shapefile}

# load california shapefile
load(
  file = 
    file.path(
      "intermediate_data", 
      "california_shapefile.rdata"
    )
)

# convert ca shapefile to raster-package-understandable
# SpatialPolygonsDataFrame
california_sp <- 
  as(california_shapefile, 
     'Spatial')

# load asc air data
load(
  file = 
    file.path(
      "intermediate_data", 
      "asc_air_data.rdata"
    )
)

# crop air data
air_cropped <- 
  raster::crop(
    asc_air_data, 
    california_sp) %>%  # crop air data to the bounding box
  raster::mask(
    california_sp
  )

plot(air_cropped)

```



### school data

```{r read school enrollment data and save as .rdata files}

# make function to read them all into r from tab-delimited files
school_enrollment_tab_reader <- 
  function(start_year_numeric){
    
    end_year_chr <- 
      as.character(start_year_numeric + 1) %>% 
      substr(3, 4)  # get only the last two digits
    
    start_year_chr <- 
      as.character(start_year_numeric)
    
    file_name <- 
      str_c("school_enrollment_", 
            start_year_chr, 
            "_", 
            end_year_chr, 
            "_raw.txt")
      
    variable_name <- 
      str_c("school_enrollment_", 
            start_year_chr, 
            "_", 
            end_year_chr, 
            "_working_from_raw")
    
    read_tibble <- 
      read_delim(
        file = 
          file.path(
            "raw_data", 
            file_name),
        delim = "\t", 
        col_types = cols(.default = "?", CDS_CODE = col_double())
        )
    
    assign(variable_name, 
           read_tibble)
    
    save(
      list = variable_name, 
      file = 
        file.path(
          "intermediate_data", 
          str_c(
            variable_name, 
            ".rdata"
          )
        )
    )
    
    # return(read_tibble)
  }

# make vector of starting years 2009-2018
start_years <- 
  seq(
    from = 2009,
    to = 2018, 
    by = 1
  )

# use function on all files
map(
  .f = school_enrollment_tab_reader, 
  .x = start_years
)
  
```


```{r read and save school addresses}

# read school addresses into tibble
public_school_directory_working_from_raw <-
  read_delim(file =
               file.path(
                 "raw_data",
                 "public_school_directory.txt"
                 ),
             delim = "\t", 
             col_types = cols(.default = "?", CDSCode = col_double())
             )

# save school addresses
save(
  public_school_directory_working_from_raw, 
  file = 
    file.path(
      "intermediate_data", 
      "public_school_directory_working_from_raw.rdata"
    )
)

```


```{r make tibbles for 2015-2016 and 2016-2017}

# load relevant files
## 2015-16 enrollment
load(
  file = 
    file.path(
      "intermediate_data", 
      "school_enrollment_2015_16_working_from_raw.rdata"
    )
)

# load relevant files
## 2016-17 enrollment
load(
  file = 
    file.path(
      "intermediate_data", 
      "school_enrollment_2016_17_working_from_raw.rdata"
    )
)

## school addresses
load(
  file = 
    file.path(
      "intermediate_data", 
      "public_school_directory_working_from_raw.rdata"
    )
)

# load function to make variable names snake case
load(
  file = 
    file.path(
      "functions", 
      "column_name_fixer.rdata"
    )
)

# fix variable names
school_enrollment_2015_16_working <- 
  column_name_fixer(school_enrollment_2015_16_working_from_raw)

school_enrollment_2016_17_working <- 
  column_name_fixer(school_enrollment_2016_17_working_from_raw)

public_school_directory_working <- 
  column_name_fixer(public_school_directory_working_from_raw)

school_lat_long <- 
  public_school_directory_working %>% 
  dplyr::select(cds_code = cdscode, latitude, longitude)
  
# get cds codes for schools with nonzero enrollment in 2015-16
school_cds_codes_2015_16 <- 
  school_enrollment_2015_16_working %>% 
  filter(enr_total > 0) %>% 
  dplyr::select(cds_code, school) %>% 
  unique()

# get cds codes for schools with nonzero enrollment in 2016-17
school_cds_codes_2016_17 <- 
  school_enrollment_2015_16_working %>% 
  filter(enr_total > 0) %>% 
  dplyr::select(cds_code, school) %>% 
  unique()

# join school and latitude/longitude
school_lat_long_2015_16 <- 
  school_cds_codes_2015_16 %>% 
  left_join(school_lat_long, by = "cds_code")
  
school_lat_long_2016_17 <- 
  school_cds_codes_2016_17 %>% 
  left_join(school_lat_long, by = "cds_code")

# two without lat/long
## Gateway to College, San Francisco Unified, 50 Phelan Avenue Science Hall, Room 127, San Francisco, CA 94112-1821
### manually found lat/long via Google Maps: 37.724958, -122.452911
## Academy of Arts and Sciences: Los Angeles (9-12) 17500 Burbank Blvd. Encino, CA 91361-1718 
### manually found lat/long via Google Maps: 34.172299, -118.514622

# fix outliers
## 2015-16
### Gateway to College, CDS code 38684780128876
school_lat_long_2015_16$latitude[school_lat_long_2015_16$cds_code == 38684780128876] <- 37.724958
school_lat_long_2015_16$longitude[school_lat_long_2015_16$cds_code == 38684780128876] <- -122.452911
  
school_lat_long_2016_17$latitude[school_lat_long_2015_16$cds_code == 38684780128876] <- 37.724958
school_lat_long_2016_17$longitude[school_lat_long_2015_16$cds_code == 38684780128876] <- -122.452911

## 2016-17
### Academy of Arts and Sciences: Los Angeles (9-12)
school_lat_long_2015_16$latitude[school_lat_long_2015_16$cds_code == 19753090130781] <- 34.172299
school_lat_long_2015_16$longitude[school_lat_long_2015_16$cds_code == 19753090130781] <- -118.514622

school_lat_long_2016_17$latitude[school_lat_long_2015_16$cds_code == 19753090130781] <- 34.172299
school_lat_long_2016_17$longitude[school_lat_long_2015_16$cds_code == 19753090130781] <- -118.514622

save(
  school_lat_long_2015_16, 
  file = 
    file.path(
      "intermediate_data", 
      "school_lat_long_2015_16.rdata"
    )
)

save(
  school_lat_long_2016_17, 
  file = 
    file.path(
      "intermediate_data", 
      "school_lat_long_2016_17.rdata"
    )
)

```



## monitor data

```{r read monitor data from california air resources board}

# make function to read monitor files into r from csv
monitor_csv_to_tibble_reader <- 
  function(object_name){
    
    monitor_tibble <- 
      read_csv(
        file = 
          file.path(
            "raw_data", 
            str_c(
              object_name,
              "_raw.txt")
            )
      )
    
    assign(object_name, 
           monitor_tibble)
    
    save(
      list = object_name, 
      file = 
        file.path(
          "intermediate_data", 
          str_c(
            object_name, 
            ".rdata"
          )
        )
    )
    
    return(monitor_tibble)
  }

monitor_tibbles <- 
  map(
    .f = monitor_csv_to_tibble_reader, 
    .x = names(monitor_urls)
  )

# name the resulting tibbles
names(monitor_tibbles) <- 
  names(monitor_urls)

# focus on 2016
daily_pm25_monitors_2016 <- 
  monitor_tibbles$daily_pm25_monitors_2016
hourly_pm25_monitors_2016 <- 
  monitor_tibbles$hourly_pm25_monitors_2016

# get number of rows with text signaling the end of the data
data_end_daily_2016 <- 
  which(
    grepl(
      "Quality Flag Definition", 
      daily_pm25_monitors_2016$basin
    )
  ) - 1

data_end_hourly_2016 <- 
  which(
    grepl(
      "Quality Flag Definition", 
      hourly_pm25_monitors_2016$basin
    )
  ) - 1

# remove those rows
daily_pm25_monitors_2016_working <- 
  daily_pm25_monitors_2016 %>% 
  slice(1:data_end_daily_2016)

hourly_pm25_monitors_2016_working <- 
  hourly_pm25_monitors_2016 %>% 
  slice(1:data_end_hourly_2016)

# find monitors in common
intersect(daily_pm25_monitors_2016_working$aqs_id, 
          hourly_pm25_monitors_2016_working$aqs_id)

# fix the monitors for putting together hourlies and dailies
daily_pm25_monitors_2016_working <-
  daily_pm25_monitors_2016_working %>% 
  mutate(observations = pm25daily2016obs, 
         frequency = "daily") %>% 
  dplyr::select(-pm25daily2016obs)

hourly_pm25_monitors_2016_working <- 
  hourly_pm25_monitors_2016_working %>% 
  mutate(observations = pm25hourly2016obs, 
         frequency = "hourly") %>% 
  dplyr::select(-pm25hourly2016obs)

colnames(daily_pm25_monitors_2016_working) ==
  colnames(hourly_pm25_monitors_2016_working)

pm_25_monitors_overall <- 
  daily_pm25_monitors_2016_working %>% 
  bind_rows(hourly_pm25_monitors_2016_working)

```

```{r monitor data from epa}

# unzip epa air data files
unzip(
  zipfile = 
    file.path("raw_data", 
              "daily_88101_2016.zip"), 
  exdir = 
    file.path(
      "intermediate_data"
    )
  )

unzip(
  zipfile = 
    file.path("raw_data", 
              "daily_88502_2016.zip"), 
  exdir = 
    file.path(
      "intermediate_data"
    )
  )

unzip(
  zipfile = 
    file.path("raw_data", 
              "daily_SPEC_2016.zip"), 
  exdir = 
    file.path(
      "intermediate_data"
    )
  )

# load files
load(
  file = 
    file.path(
      "intermediate_data", 
      "daily_88101_2016.csv"
    )
)

load(
  file = 
    file.path(
      "intermediate_data", 
      "daily_88502_2016.csv"
    )
)

load(
  file = 
    file.path(
      "intermediate_data", 
      "daily_SPEC_2016.csv"
    )
)

```


```{r latitude and longitude}



```

