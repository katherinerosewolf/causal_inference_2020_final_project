---
title: "DS421 Summer 2019 R Markdown Document"
author: "Katherine Rose Wolf"
date: "August 4, 2019"
output: html_document
---

```{r setup load libraries}

library(praise)
library(raster)
library(tigris)
library(sf)
library(httr)
library(tidyverse)
library(ggmap)

# stop r from abbreviating doubles in school ids
options("scipen" = 40)

praise()  # self-esteem boost

# set encrypted google api key
register_google(key = Sys.getenv("hidden_google_api_key"))

```


```{r create file structure}

# create raw data file
if(!dir.exists("raw_data")) {
  dir.create("raw_data")
}

if(!dir.exists("intermediate_data")) {
  dir.create("intermediate_data")
}

if(!dir.exists("functions")) {
  dir.create("functions")
}

```


```{r functions for general later use}

# function to convert column names to snake case
column_name_fixer <-
  function(tibble) {
  new_names <-
    tibble %>%
    colnames() %>%  # gets the column names from the tibble
    tolower() %>%  # converts the column names to lowercase
    {gsub(" ", "_", .)}  # replaces spaces with "_"
  colnames(tibble) <-
    new_names  # rename the columns in the original tibble
  return(tibble)
  }

# create function that downloads files
file_downloader <-
  function(desired_filename_as_string, 
           web_address_as_string, 
           folder_as_string) {
    
    path <-  # specify the file path
      file.path(
        folder_as_string,
        desired_filename_as_string) 
    
    if(!file.exists(path))  # checks if file exists already
      download.file(url = web_address_as_string,  # if not, downloads/saves file
                    destfile = path, 
                    mode = "wb")
  }

# save functions for later use
save(
  column_name_fixer, 
     file = 
       file.path(
         "functions", 
         "column_name_fixer.rdata"
       )
)

save(
  file_downloader, 
     file = 
       file.path(
         "functions", 
         "file_downloader.rdata"
       )
)

# remove all objects from the environment
rm(list = ls(all.names = TRUE))

```


# raw data downloads

## modeled pm2.5

```{r download modeled air pollution data}

# download 2017 pm2.5 air pollution file .nc edition
if(!file.exists(file.path("raw_data", 
                          "GWRwSPEC.HEI_PM25_NA_201701_201712-RH35.nc"))) {

  download.file(
    url = "http://fizz.phys.dal.ca/~atmos/datasets/EST2019/GWRwSPEC.HEI_PM25_NA_201701_201712-RH35.nc", 
    destfile = 
      file.path(
        "raw_data", 
        "GWRwSPEC.HEI_PM25_NA_201701_201712-RH35.nc"
      ), 
    method = 'libcurl'
    )
  
  # remove all objects from the environment
  rm(list = ls(all.names = TRUE))

}

# download 2017 pm2.5 air pollution file .asc edition
if(!file.exists(file.path("raw_data", 
                          "GWRwSPEC.HEI_PM25_NA_201701_201712-RH35-NoNegs.asc.zip"))) {
  
  download.file(
    url = "http://fizz.phys.dal.ca/~atmos/datasets/EST2019/GWRwSPEC.HEI_PM25_NA_201701_201712-RH35-NoNegs.asc.zip", 
    destfile = 
      file.path(
        "raw_data", 
        "GWRwSPEC.HEI_PM25_NA_201701_201712-RH35-NoNegs.asc.zip"
      ), 
    method = 'libcurl'
    )
  
  # remove all objects from the environment
  rm(list = ls(all.names = TRUE))
  
}

# download nox air pollution file .nc edition


# download nox air pollution file .asc edition



```

## geographic shapefile

```{r load united states state shapefiles}

# download the united states border shapefiles
if(!file.exists(file.path("raw_data", 
                          "state_shapefiles.rdata"))) {
  
  # states() is a tigris function
  state_shapefiles <- 
    states(
            cb = FALSE,  # downloads detailed outlines
            year = 2017, 
            class = 'sf'  # requests sf format to play nicely with sf package
          )
  
  # save raw file to disk
  save(
    state_shapefiles, 
    file = 
      file.path(
        "raw_data", 
        "state_shapefiles.rdata"
      )
  )
  
  # remove all objects from the environment
  rm(list = ls(all.names = TRUE))
}

```

## school enrollment

```{r download school enrollment data}

# school enrollment data by racial/ethnic designation, gender, and grade

if(!file.exists(file.path("raw_data", 
                          "school_enrollment_2016_17_raw.txt"))
   ||
   !file.exists(file.path("raw_data", 
                          "school_enrollment_2017_18_raw.txt"))) {

  # make school enrollment raw downloader function
  school_enrollment_raw_downloader <-
    function(start_year_numeric){
      end_year_chr <- 
        as.character(start_year_numeric + 1) %>%  # add 1 to the start year
        substr(3, 4)  # get only the last two digits
      
      start_year_chr <- 
        as.character(start_year_numeric)  # convert start year to numeric
      
      url_file <- 
        str_c("http://dq.cde.ca.gov/dataquest/dlfile/dlfile.aspx?cLevel=School&cYear=", 
              start_year_chr, 
              "-", 
              end_year_chr, 
              "&cCat=Enrollment&cPage=filesenr.asp")
      
      # GET(
      #   url = url_file, 
      #   write_disk = 
      #     file.path(
      #       "raw_data", 
      #       str_c("school_enrollment_", 
      #             start_year_chr,
      #             "_", 
      #             end_year_chr, 
      #             "_raw.txt")
      #     ), 
      #   overwrite = TRUE
      # )
      
      download.file(url = url_file, 
                    destfile = file.path(
                      "raw_data", 
                      str_c("school_enrollment_", 
                            start_year_chr,
                            "_", 
                            end_year_chr, 
                            "_raw.txt")
                      ), 
                    method = 'libcurl'
                    )
      
      Sys.sleep(time = 5)  # pause to try to avoid anti-scraping blocker
    }
  
  # make vector of starting years 2015-2018
  start_years <- 
    seq(
      from = 2016,
      to = 2017, 
      by = 1
    )
  
  # download all the school files at once
  map(
    .f = school_enrollment_raw_downloader, 
    .x = start_years
  )
  
  # remove all objects from the environment
  rm(list = ls(all.names = TRUE))

}

```

## school addresses

```{r download school addresses}

# download school addresses

if(!file.exists(file.path("raw_data", 
                          "public_school_directory.txt"))) {
  GET(
    url = "https://www.cde.ca.gov/schooldirectory/report?rid=dl1&tp=txt", 
    write_disk(
      file.path(
        "raw_data",
        "public_school_directory.txt"
      ),
      overwrite = TRUE
      )
    )
  
  # remove all objects from the environment
  rm(list = ls(all.names = TRUE))
}

```

## teaching credentials

```{r download teaching credential data}

# download teaching credential data 2015-16
if(!file.exists(file.path("raw_data", 
                          "StaffCred15.txt"))) {
  
download.file(
  url = "ftp://ftp.cde.ca.gov/demo/staffclass/StaffCred15.txt", 
  destfile = 
    file.path(
      "raw_data", 
      "StaffCred15.txt"
    ), 
  method = 'libcurl'
  )
  
  Sys.sleep(time = 5)  # pause to try to avoid anti-scraping blocker

}


# download teaching credential data 2016-17
if(!file.exists(file.path("raw_data", 
                          "StaffCred16.txt"))) {
download.file(
  url = "ftp://ftp.cde.ca.gov/demo/staffclass/StaffCred16.txt", 
  destfile = 
    file.path(
      "raw_data", 
      "StaffCred16.txt"
    ), 
  method = 'libcurl'
  )
  
  Sys.sleep(time = 5)  # pause to try to avoid anti-scraping blocker
  
  # remove all objects from the environment
  rm(list = ls(all.names = TRUE))
}


# download teaching credential data 2016-17
if(!file.exists(file.path("raw_data", 
                          "StaffCred17.txt"))) {
download.file(
  url = "ftp://ftp.cde.ca.gov/demo/staffclass/StaffCred17.txt", 
  destfile = 
    file.path(
      "raw_data", 
      "StaffCred17.txt"
    ), 
  method = 'libcurl'
  )
  
  # remove all objects from the environment
  rm(list = ls(all.names = TRUE))
}

```


```{r download free and reduced price lunch data}

# download poverty data 2016-17
if(!file.exists(file.path("raw_data", 
                          "frpm1617.xls"))) {

  download.file(
    url = "https://www.cde.ca.gov/ds/sd/sd/documents/frpm1617.xls", 
    destfile = 
      file.path(
        "raw_data", 
        "frpm1617.xls"
      ), 
    mode = 'wb'
    )
  
  # remove all objects from the environment
  rm(list = ls(all.names = TRUE))

}

# download poverty data 2017-18
if(!file.exists(file.path("raw_data", 
                          "frpm1718.xlsx"))) {

  download.file(
    url = "https://www.cde.ca.gov/ds/sd/sd/documents/frpm1718.xlsx", 
    destfile = 
      file.path(
        "raw_data", 
        "frpm1718.xlsx"
      ), 
    mode = 'wb'
    )
  
  # remove all objects from the environment
  rm(list = ls(all.names = TRUE))

}

```



## monitors

```{r download carb monitor data for 2016}

if(!file.exists(file.path("raw_data", 
           "carb_2017_daily_monitors_raw.txt"))) {
  
  # universal entries into query
  download <- 
    "https://www.arb.ca.gov/aqmis2/display.php"
  
  # values for daily query
  values_daily_2017 <- 
    list(
      download = "y", 
      param = "PM25", 
      units = "001", 
      year = "2017", 
      county_name = "--COUNTY--", 
      basin = "--AIR BASIN--", 
      latitude = "--PART OF STATE--", 
      report = "SITELIST", 
      order = "basin,county_name,s.name", 
      submit = "Retrieve Data", 
      ptype = "aqd", 
      std15 = ""
    )
  
  # set cookies correctly for the daily 2017 monitors
  POST(download, 
       body = values_daily_2017)
  Sys.sleep(time = 2)
  GET(download, 
      query = values_daily_2017)
  Sys.sleep(time = 2)
  
  # actually get the 2017 daily monitor list
  carb_2017_daily_monitors_raw <- 
     GET(download,
         query = values_daily_2017)
  content(carb_2017_daily_monitors_raw, "raw")
  Sys.sleep(time = 2)
  
  # write the 2017 daily monitor list to disk
  writeBin(content(carb_2017_daily_monitors_raw, "raw"), 
           file.path(
             "raw_data", 
             "carb_2017_daily_monitors_raw.txt"
           ))
  
  # remove all objects from the environment
  rm(list = ls(all.names = TRUE))
  
}

if(!file.exists(file.path("raw_data", 
           "carb_2017_hourly_monitors_raw.txt"))) {
  
  # universal entries into query
  download <- 
    "https://www.arb.ca.gov/aqmis2/display.php"
  
  # values for hourly query
  values_hourly_2017 <- 
    list(
      download = "y", 
      param = "PM25HR", 
      units = "001", 
      year = "2017", 
      county_name = "--COUNTY--", 
      basin = "--AIR BASIN--", 
      latitude = "--PART OF STATE--", 
      report = "SITELIST", 
      order = "basin,county_name,s.name", 
      submit = "Retrieve Data", 
      ptype = "aqd", 
      std15 = ""
    )
  
  # set cookies correctly for the hourly 2017 monitors
  POST(download, body = values_hourly_2017)
  Sys.sleep(time = 2)
  GET(download, query = values_hourly_2017)
  Sys.sleep(time = 2)
  
  # actually get the 2017 hourly monitor list
  carb_2017_hourly_monitors_raw <- 
     GET(download, query = values_hourly_2017)
  
  # write the 2017 hourly monitor list to disk
  writeBin(content(carb_2017_hourly_monitors_raw, "raw"), 
           file.path(
             "raw_data", 
             "carb_2017_hourly_monitors_raw.txt"
           ))
  
  # remove all objects from the environment
  rm(list = ls(all.names = TRUE))
}

```


```{r download epa monitor data for 2017}

# download federal reference method (parameter code 88101) data

if(!file.exists(file.path("raw_data",
                          "daily_88101_2017.zip"))) {
  
  GET(
    url = 
      "https://aqs.epa.gov/aqsweb/airdata/daily_88101_2017.zip", 
    write_disk(
      file.path("raw_data", 
                "daily_88101_2017.zip"), 
    overwrite = TRUE
    )
  )
  
  # remove all objects from the environment
  rm(list = ls(all.names = TRUE))
  
}

# download non-federal-reference-method (parameter code 88502) data

if(!file.exists(file.path("raw_data",
                          "daily_88502_2017.zip"))) {
  
  GET(
    url = 
      "https://aqs.epa.gov/aqsweb/airdata/daily_88502_2017.zip", 
    write_disk(
      file.path("raw_data", 
                "daily_88502_2017.zip"),
      overwrite = TRUE
      )
  )
  
  # remove all objects from the environment
  rm(list = ls(all.names = TRUE))
  
}

# download speciation data

if(!file.exists(file.path("raw_data", 
                          "daily_SPEC_2017.zip"))) {

  GET(
    url = "https://aqs.epa.gov/aqsweb/airdata/daily_SPEC_2017.zip", 
    write_disk(
      file.path("raw_data", 
                "daily_SPEC_2017.zip"),
      overwrite = TRUE
      )
  )
  
  # remove all objects from the environment
  rm(list = ls(all.names = TRUE))
  
}

```


# data preparation

## pm2.5 data

```{r read modeled air pollution data into r}

# unzip the air pollution data file
unzip(
  zipfile = 
    file.path(
      "raw_data", 
      "GWRwSPEC.HEI_PM25_NA_201701_201712-RH35-NoNegs.asc.zip"
    ), 
  exdir = 
    file.path(
      "intermediate_data"
    )
  )

# check the file type
gdal_utils(
  util = "info",
  file.path(
      "raw_data", 
      "GWRwSPEC.HEI_PM25_NA_201701_201712-RH35-NoNegs.asc.zip"
    )
)  # yup, just one band

# create the working raster
asc_air_data <- 
  raster(
    file.path(
      "intermediate_data", 
      "GWRwSPEC.HEI_PM25_NA_201701_201712-RH35-NoNegs.asc"
    )
  )
  
# save air data raster
save(
  asc_air_data, 
  file = 
    file.path(
      "intermediate_data", 
      "asc_air_data.rdata"
    )
)

# remove all objects from the environment
rm(list = ls(all.names = TRUE))

```


## california shapefile

```{r pull just california shapefile}

# load us shapefiles
load(
  file = 
    file.path(
      "raw_data", 
      "state_shapefiles.rdata"
    )
)

# load asc air data
load(
  file = 
    file.path(
      "intermediate_data", 
      "asc_air_data.rdata"
    )
)

# isolate california shapefile
california_shapefile <- 
  state_shapefiles %>% 
  filter(STUSPS == "CA") %>%  # get just california
  st_transform(crs = proj4string(asc_air_data))  # convert to pm2.5 coordinates

save(
  california_shapefile, 
  file = 
    file.path(
      "intermediate_data", 
      "california_shapefile.rdata"
    )
)

# remove all objects from the environment
rm(list = ls(all.names = TRUE))

```


## california shapefile and pm2.5

```{r clip air pollution data to shapefile}

# load california shapefile
load(
  file = 
    file.path(
      "intermediate_data", 
      "california_shapefile.rdata"
    )
)

# convert ca shapefile to raster-package-understandable
# SpatialPolygonsDataFrame
california_sp <- 
  as(california_shapefile, 
     'Spatial')

# load asc air data
load(
  file = 
    file.path(
      "intermediate_data", 
      "asc_air_data.rdata"
    )
)

# crop air data
air_cropped <- 
  raster::crop(
    asc_air_data, 
    california_sp) %>%  # crop air data to the bounding box
  raster::mask(
    california_sp
  )

# save cropped air data
save(
  air_cropped,
  file = 
    file.path(
      "intermediate_data", 
      "air_cropped.rdata"
    )
)

# plot(air_cropped)

# remove all objects from the environment
rm(list = ls(all.names = TRUE))

```


## school data

```{r read school enrollment data and save as .rdata files}

# make function to read them all into r from tab-delimited files
school_enrollment_tab_reader <- 
  function(start_year_numeric){
    
    end_year_chr <- 
      as.character(start_year_numeric + 1) %>% 
      substr(3, 4)  # get only the last two digits
    
    start_year_chr <- 
      as.character(start_year_numeric)
    
    file_name <- 
      str_c("school_enrollment_", 
            start_year_chr, 
            "_", 
            end_year_chr, 
            "_raw.txt")
      
    variable_name <- 
      str_c("school_enrollment_", 
            start_year_chr, 
            "_", 
            end_year_chr, 
            "_working_from_raw")
    
    read_tibble <- 
      read_delim(
        file = 
          file.path(
            "raw_data", 
            file_name),
        delim = "\t", 
        col_types = cols(.default = "?", CDS_CODE = col_double())
        )
    
    assign(variable_name, 
           read_tibble)
    
    save(
      list = variable_name, 
      file = 
        file.path(
          "intermediate_data", 
          str_c(
            variable_name, 
            ".rdata"
          )
        )
    )
    
    return(read_tibble)
  }

# make vector of starting years 2009-2018
start_years <- 
  seq(
    from = 2016,
    to = 2017, 
    by = 1
  )

# use function on all files
map(
  .f = school_enrollment_tab_reader, 
  .x = start_years
)

# remove all objects from the environment
rm(list = ls(all.names = TRUE))
  
```


```{r read free and reduced price lunch data and save as .rdata files}

# read data 2017-18
free_red_lunch_2017_18 <-
  readxl::read_excel(
    file.path(
      "raw_data", 
      "frpm1718.xlsx"
    )
  )

# save .rdata file to intermediate data
save(
  free_red_lunch_2017_18,
  file = 
    file.path(
      "intermediate_data", 
      "free_red_lunch_2017_18.rdata"
    )
)
  
# read data 2016-17
free_red_lunch_2016_17 <-
  readxl::read_xls(
    file.path(
      "raw_data", 
      "frpm1617.xls"
    )
  )

# save .rdata file to intermediate data
save(
  free_red_lunch_2016_17,
  file = 
    file.path(
      "intermediate_data", 
      "free_red_lunch_2016_17.rdata"
    )
)

# remove all objects from the environment
rm(list = ls(all.names = TRUE))

  
```



```{r read and save school addresses}

# fix parsing error in school addresses
public_school_directory_raw_text  <- 
  readLines(
    con =  
      file.path(
        "raw_data",
        "public_school_directory.txt"
        )
    )

public_school_directory_fixed_quotes <- 
  gsub(pattern = "\"350", 
       replace = "350", 
       x = public_school_directory_raw_text)

writeLines(
  public_school_directory_fixed_quotes, 
  con = 
    file.path(
      "raw_data",
      "public_school_directory_fixed.txt"
      )
  )

# read school addresses into tibble
public_school_directory_working_from_raw <-
  read_delim(file =
               file.path(
                 "raw_data",
                 "public_school_directory_fixed.txt"
                 ),
             delim = "\t", 
             col_types = cols(.default = "c", 
                              CDSCode = col_double())
             )

# save school addresses
save(
  public_school_directory_working_from_raw, 
  file = 
    file.path(
      "intermediate_data", 
      "public_school_directory_working_from_raw.rdata"
    )
)

# remove all objects from the environment
rm(list = ls(all.names = TRUE))

```


```{r make tibbles for 2016-2017}

# load relevant files
# 2016-17 enrollment
load(
  file = 
    file.path(
      "intermediate_data", 
      "school_enrollment_2016_17_working_from_raw.rdata"
    )
)

# school addresses
load(
  file = 
    file.path(
      "intermediate_data", 
      "public_school_directory_working_from_raw.rdata"
    )
)

# load function to make variable names snake case
load(
  file = 
    file.path(
      "functions", 
      "column_name_fixer.rdata"
    )
)

# load air data file
load(
  file = 
    file.path(
      "intermediate_data", 
      "air_cropped.rdata"
    )
)

# fix variable names
school_enrollment_2016_17_working <- 
  column_name_fixer(school_enrollment_2016_17_working_from_raw) %>%
   mutate(cds_school_code = str_sub(cds_code,  
                                   start = -7))

# public school directory
public_school_directory_working <- 
  column_name_fixer(public_school_directory_working_from_raw) %>% 
  rename(cds_code = cdscode) %>% 
  # this isolates the individual school code
  mutate(cds_school_code = str_sub(cds_code,  
                                   start = -7))

# get unique schools in 2016-17
unique_schools_2016_17 <- 
  school_enrollment_2016_17_working %>% 
  select(cds_code, county, district, school) %>% 
  mutate(cds_school_code = str_sub(cds_code,  # this isolates the individual school code
                                   start = -7)) %>% 
  unique()  # each of these has a unique cds code

# get lat/longs from public school directory
school_directory_lat_long <- 
  public_school_directory_working %>% 
  dplyr::select(
    cds_code_dir = cds_code, 
    cds_school_code_dir = cds_school_code, 
    county_dir = county,
    district_dir = district,
    school_dir = school,
    latitude = latitude, 
    longitude = longitude)

# # check duplicates in cds code
# sum(table(school_directory_lat_long$cds_code_dir) - 1)  # should be 0
# sum(table(school_directory_lat_long$cds_school_code_dir) - 1)  # should be 0
# # note, this is not zero!  beware!

# get cds codes for schools 2016-17
schools_2016_17 <- 
  school_enrollment_2016_17_working %>% 
  dplyr::select(
    cds_code_16_17 = cds_code, 
    cds_school_code_16_17 = cds_school_code, 
    county_16_17 = county,
    district_16_17 = district,
    school_16_17 = school) %>% 
  unique()

# merge two
school_lat_long_2016_17_first_try <- 
  schools_2016_17 %>% 
  left_join(school_directory_lat_long, 
            by = c("cds_code_16_17" =  "cds_code_dir"))

# filter missing latitudes and longitudes
school_lat_long_2016_17_no_missing <- 
  school_lat_long_2016_17_first_try %>% 
  filter(!is.na(latitude)) %>% 
  filter(!is.na(longitude)) %>% 
  filter(longitude != "No Data") %>% 
  mutate(latitude = as.numeric(latitude), 
         longitude = as.numeric(longitude))

# get cds codes of schools in 2016-17 dataset without lat_longs
cds_codes_no_lat_long <- 
  setdiff(
    unique(schools_2016_17$cds_code_16_17), 
    unique(school_lat_long_2016_17_no_missing$cds_code_16_17)
  )

# isolate schools with no lat/long (421 remain!)
# (cds_code, cds_school_code, county, district, school)
schools_2016_17_lat_long_missing <- 
  schools_2016_17 %>% 
  filter(cds_code_16_17 %in% cds_codes_no_lat_long) 

# exclude nonpublic and nonsectarian schools (24 remain!)
schools_2016_17_lat_long_missing_no_nonsec <- 
  schools_2016_17_lat_long_missing %>% 
  filter(!(school_16_17 == "Nonpublic, Nonsectarian Schools"))

# find which schools missing lat/longs are in the directory
school_directory_no_lat_long <- 
  public_school_directory_working %>% 
  filter(cds_code %in% cds_codes_no_lat_long)  # (two, these will be geocoded)

# match on cds code
schools_no_lat_long_matched_by_cds_school_code_only <- 
  left_join(
    schools_2016_17_lat_long_missing_no_nonsec, 
    school_directory_lat_long, 
    by = c("cds_school_code_16_17" = "cds_school_code_dir")
    ) 

# pull long valley longitude
long_valley_longitude <- 
  schools_no_lat_long_matched_by_cds_school_code_only %>% 
  filter(cds_school_code_16_17 == "6010763" &  # remove no-data long view record
           !(longitude == "No Data")) %>% 
  pull(longitude)

# pull long valley latitude
long_valley_latitude <- 
  schools_no_lat_long_matched_by_cds_school_code_only %>% 
  filter(cds_school_code_16_17 == "6010763" &  # remove no-data long view record
           !(latitude == "No Data")) %>% 
  pull(latitude)
  
# make the replacement
schools_2016_17_lat_long_missing_long_valley_fix <- 
  schools_no_lat_long_matched_by_cds_school_code_only %>% 
  # remove long view record that doesn't correspond to 16-17 data
  filter(!(cds_school_code_16_17 == "6010763" &  
             latitude != "No Data")) %>% 
  mutate(longitude = ifelse(cds_school_code_16_17 == "6010763", 
                            long_valley_longitude,
                            longitude), 
         latitude = ifelse(cds_school_code_16_17 == "6010763", 
                            long_valley_latitude,
                            latitude))

# pull cds codes for the directory from the two remaining w/o lat/longs
cds_codes_last_two <- 
  schools_2016_17_lat_long_missing_long_valley_fix %>% 
  filter(latitude == "No Data") %>%  # filter to those w/o lat/longs
  pull(cds_code_dir)  # pull the cds code in the directory

# pull directory records for last two for annoyingly manual geocoding
addresses_last_two <- 
  public_school_directory_working %>% 
  filter(cds_code %in% cds_codes_last_two) %>% 
  select(cds_code_dir = cds_code,  # change name of cds_code 
         school, 
         street, 
         city, 
         state, 
         zip) %>% 
  mutate(address_geocode =  # put entire address in one column for google
           str_c(street, 
                 ", ", 
                 city, 
                 ", ", 
                 state, 
                 " ", 
                 zip)
  )

# geocoding time!
geocoded_lat_longs_last_two <-
  addresses_last_two %>%
  mutate_geocode(
    location = address_geocode
  ) 

# make limited dataset
geocoded_simple <- 
  geocoded_lat_longs_last_two %>% 
  select(school_geo = school,  # rename variables to show that they came from the geo dataset
         cds_code_geo = cds_code_dir, 
         lon_geo = lon, 
         lat_geo = lat)

# add geocoded lat/longs
schools_missing_lat_long_fixed <- 
  schools_2016_17_lat_long_missing_long_valley_fix %>% 
  left_join(geocoded_simple, 
            by = c("cds_code_16_17" = "cds_code_geo")) %>%  
  mutate(geo_source = "small_cds",  # add column to tibble for lat-long source
         geo_source = replace(geo_source, !is.na(lon_geo), "google"), 
         longitude = ifelse(longitude == "No Data", lon_geo, longitude), 
         latitude = ifelse(latitude == "No Data", lat_geo, latitude)
  ) %>% 
  select(man_cds_code = cds_code_16_17, 
         man_longitude = longitude, 
         man_latitude = latitude, 
         geo_source)


# # remove all objects from the environment
# rm(list = ls(all.names = TRUE))

```

```{r}

# make spatial point files

# # make school locations into spatial point files
# school_lat_long_2016_17_sf <-
#   st_as_sf(
#     school_lat_long_2016_17,
#     coords =
#       c("longitude",
#         "latitude"),
#     crs = st_crs(air_cropped)
#   )
# 
# school_lat_long_2017_18_sf <-
#   st_as_sf(
#     school_lat_long_2017_18,
#     coords =
#       c("longitude",
#         "latitude"),
#     crs = st_crs(air_cropped)
#   )
# 
# # join school files with rest of data
# school_data_2016_17_sf <- 
#   right_join(
#     school_lat_long_2016_17_sf, 
#     school_enrollment_2016_17_working, 
#     by = "cds_code"
#   )
# 
# school_data_2017_18_sf <- 
#   right_join(
#     school_lat_long_2017_18_sf, 
#     school_enrollment_2017_18_working, 
#     by = "cds_code"
#   )

```



## monitor data

```{r read monitor data from california air resources board}

# load air data for reference
load(
  file = 
    file.path(
      "intermediate_data", 
      "air_cropped.rdata"
    )
)

# make function to read monitor files into r from csv
monitor_csv_to_tibble_reader <- 
  function(object_name){
    monitor_tibble <- 
      read_csv(
        file = 
          file.path(
            "raw_data", 
            str_c(
              object_name,
              "_raw.txt")
            )
      )
    
    assign(object_name, 
           monitor_tibble)
    
    save(
      list = object_name, 
      file = 
        file.path(
          "intermediate_data", 
          str_c(
            object_name, 
            "_working.rdata"
          )
        )
    )
    
    return(monitor_tibble)
  }

# read raw monitors
carb_2017_daily_monitors_working <- 
  monitor_csv_to_tibble_reader("carb_2017_daily_monitors")
carb_2017_hourly_monitors_working <- 
  monitor_csv_to_tibble_reader("carb_2017_hourly_monitors")

# get number of rows with text signaling the end of the data
data_end_daily_2017 <- 
  which(
    grepl(
      "Quality Flag Definition", 
      carb_2017_daily_monitors_working$basin
    )
  ) - 1

data_end_hourly_2017 <- 
  which(
    grepl(
      "Quality Flag Definition", 
      carb_2017_hourly_monitors_working$basin
    )
  ) - 1

# remove those rows
carb_2017_daily_monitors_working <- 
  carb_2017_daily_monitors_working %>% 
  slice(1:data_end_daily_2017)

carb_2017_hourly_monitors_working <- 
  carb_2017_hourly_monitors_working %>% 
  slice(1:data_end_hourly_2017)

# find monitors in common
intersect(carb_2017_daily_monitors_working$aqs_id, 
          carb_2017_hourly_monitors_working$aqs_id)

# fix the monitors for putting together hourlies and dailies
carb_2017_daily_monitors_working <-
  carb_2017_daily_monitors_working %>% 
  mutate(observations = pm25daily2017obs, 
         frequency = "daily") %>% 
  dplyr::select(-pm25daily2017obs)

carb_2017_hourly_monitors_working <- 
  carb_2017_hourly_monitors_working %>% 
  mutate(observations = pm25hourly2017obs, 
         frequency = "hourly") %>% 
  dplyr::select(-pm25hourly2017obs)

colnames(carb_2017_daily_monitors_working) ==
  colnames(carb_2017_hourly_monitors_working)

carb_pm_25_monitors_overall <- 
  carb_2017_daily_monitors_working %>% 
  bind_rows(carb_2017_hourly_monitors_working)

save(
  carb_pm_25_monitors_overall, 
  file = 
    file.path(
      "intermediate_data", 
      "carb_pm_25_monitors_overall.rdata"
    )
)

# isolate monitors by latitude and longitude
carb_pm_25_lat_long <- 
  carb_pm_25_monitors_overall %>% 
  select(aqs_id, site, latitude, longitude) %>% 
  distinct()

# make into sf (FIND DATUM)
carb_pm25_sf <- 
  carb_pm_25_lat_long %>% 
  st_as_sf(
    coords = 
      c("longitude",
        "latitude"),
    crs = st_crs(air_cropped)
  ) %>% 
  left_join(carb_pm_25_lat_long,  # add lat/long back
            by = c("aqs_id", "site"))

# plot(carb_pm25_sf)

save(
  carb_pm25_sf, 
  file = 
    file.path(
      "intermediate_data", 
      "carb_pm25_sf.rdata"
    )
)

# remove all objects from the environment
rm(list = ls(all.names = TRUE))

```


```{r process monitor data from epa}

# daily frm (88101) files
if(!file.exists(file.path("intermediate_data",
                          "epa_daily_88101_2017_working.rdata"))) {

  # unzip
  unzip(
    zipfile = 
      file.path("raw_data", 
                "daily_88101_2017.zip"), 
    exdir = 
      file.path(
        "intermediate_data"
      )
    )
  
  # read into a tibble
  epa_daily_88101_2017_working <- 
    read_csv(
      file = 
        file.path(
          "intermediate_data",
          "daily_88101_2017.csv"
        )
      )
  
  # save on disk
  save(
    epa_daily_88101_2017_working, 
    file = 
      file.path(
        "intermediate_data", 
        "epa_daily_88101_2017_working.rdata"
      )
  )

}


# daily non-frm (88502) files
if(!file.exists(file.path("intermediate_data",
                          "epa_daily_88502_2017_working.rdata"))) {

  # unzip
  unzip(
    zipfile = 
      file.path("raw_data", 
                "daily_88502_2017.zip"), 
    exdir = 
      file.path(
        "intermediate_data"
      )
    )
  
  # read files into r
  epa_daily_88502_2017_working <- 
    read_csv(
      file = 
        file.path(
          "intermediate_data",
          "daily_88502_2017.csv"
        )
      )
  
  # save to disk
  save(
    epa_daily_88502_2017_working, 
    file = 
      file.path(
        "intermediate_data", 
        "epa_daily_88502_2017_working.rdata"
      )
  )
}


# speciation (SPEC) files
if(!file.exists(file.path("intermediate_data",
                          "epa_daily_SPEC_2017_working.rdata"))) {

  unzip(
    zipfile = 
      file.path("raw_data", 
                "daily_SPEC_2017.zip"), 
    exdir = 
      file.path(
        "intermediate_data"
      )
  )
    
  epa_daily_SPEC_2017_working <- 
    read_csv(
      file = file.path(
        "intermediate_data", 
        "daily_SPEC_2017.csv"
      )
  )
  
  save(
    epa_daily_SPEC_2017_working, 
    file = 
      file.path(
        "intermediate_data", 
        "epa_daily_SPEC_2017_working.rdata"
      )
  )
  
}

# remove all objects from the environment

rm(list = ls(all.names = TRUE))

```


```{r isolate individual monitors in carb and epa}

# load 88101 file
load(
  file = 
    file.path(
      "intermediate_data", 
      "epa_daily_88101_2017_working.rdata"
    )
)

# load function to fix variable names
load(
  file = 
    file.path(
      "functions", 
      "column_name_fixer.rdata"
    )
)

# fix column names
epa_daily_88101_2017_working <-
  epa_daily_88101_2017_working %>% 
  column_name_fixer()

# limit to california
cal_epa_88101_2017_working <-
  epa_daily_88101_2017_working %>% 
  filter(state_code == "06")

# remove big file 
rm(epa_daily_88101_2017_working)
invisible(gc())

# create monitor ids
cal_epa_88101_2017_working <- 
  cal_epa_88101_2017_working %>% 
  mutate(
    monitor_id = 
      paste(
        state_code, 
        county_code, 
        site_num, 
        sep="-"), 
    poc = str_pad(poc, 2, pad = "0"),
    monitor_id_poc = str_c(monitor_id, "-", poc)
    )

# make tibble with monitors only
cal_epa_88101_2017_monitors <- 
  cal_epa_88101_2017_working %>% 
  select(
    monitor_id_poc,
    monitor_id,
    poc,
    state_code,
    county_code, 
    site_num, 
    latitude, 
    longitude, 
    datum, 
    local_site_name, 
    address, 
    state_name, 
    county_name, 
    city_name, 
    cbsa_name) %>% 
  distinct()

# unqiue monitors based solely on latitude and longitude
cal_epa_88101_2017_monitor_locations <- 
  cal_epa_88101_2017_monitors %>% 
  select(
    monitor_id_poc, 
    latitude, 
    longitude, 
    datum) %>% 
  distinct()

# View(table(cal_epa_88101_monitor_unique_locations$monitor_id))

# load state shapefile
load(
  file = 
    file.path(
      "intermediate_data", 
      "california_shapefile.rdata"
    )
)

# load cropped air pollution data
load(
  file = 
    file.path(
      "intermediate_data", 
      "air_cropped.rdata"
    )
)

# get datums of state shapefile and air data (both WGS84)
crs(california_shapefile)
crs(air_cropped)

# isolate NAD83 monitors
cal_epa_monitors_nad83 <- 
  cal_epa_88101_2017_monitor_locations %>% 
  filter(datum == "NAD83")

# save NAD83 monitors
save(
  cal_epa_monitors_nad83,
  file = 
    file.path(
      "intermediate_data",
      "cal_epa_monitors_nad83.rdata"
      )
  )
  
# isolate WGS84 monitors
cal_epa_monitors_wgs84 <- 
  cal_epa_88101_2017_monitor_locations %>% 
  filter(datum == "WGS84")

# save WGS84 monitors
save(
  cal_epa_monitors_wgs84,
  file = 
    file.path(
      "intermediate_data",
      "cal_epa_monitors_wgs84.rdata"
      )
  )

# make monitor locations into spatial point files
cal_epa_monitors_wgs84_sf <-
  st_as_sf(
    cal_epa_monitors_wgs84,
    coords =
      c("longitude",
        "latitude"),
    crs = st_crs(air_cropped)
  )

cal_epa_monitors_nad83_sf <-
  st_as_sf(
    cal_epa_monitors_nad83,
    coords =
      c("longitude",
        "latitude"),
    crs = 4269  # epsg code for NAD83
  )

# convert nad83-datum monitors to wgs84
cal_epa_monitors_nad83_to_wgs84_sf <- 
  st_transform(
    cal_epa_monitors_nad83_sf, 
    crs = st_crs(air_cropped))

# join transformed monitors together
cal_epa_monitors_sf <- 
  rbind(
    cal_epa_monitors_wgs84_sf,
    cal_epa_monitors_nad83_to_wgs84_sf
  ) %>% 
  mutate(transformed_datum = "wgs84", 
         original_datum = datum) %>%  # add new datum
  select(monitor_id_poc, transformed_datum, original_datum) %>% 
  left_join(cal_epa_88101_2017_monitors,  # join back to the rest of the data
            by = "monitor_id_poc") %>% 
  select(-datum) %>%   # remove "datum" column
  mutate(aqs_id = gsub('-', '', monitor_id))  # remove dashes from aqs_id

save(
  cal_epa_monitors_sf, 
  file = 
    file.path(
      "intermediate_data", 
      "cal_epa_monitors_sf.rdata"
    )
)

# plot(cal_epa_monitors_sf)

# remove all objects from the environment
rm(list = ls(all.names = TRUE))

```


```{r compare and compile epa and carb monitors into one unique monitor dat}

load(
  file = 
    file.path(
      "intermediate_data", 
      "carb_pm25_sf.rdata"
    )
)

load(
  file = 
    file.path(
      "intermediate_data", 
      "cal_epa_monitors_sf.rdata"
    )
)

load(
  file = 
    file.path(
      "intermediate_data", 
      "air_cropped.rdata"
    )
)

cal_epa_monitors_no_geom <- 
  cal_epa_monitors_sf %>% 
  st_drop_geometry()

carb_monitors_no_geom <- 
  carb_pm25_sf %>% 
  st_drop_geometry()

carb_but_not_epa_monitors <- 
  anti_join(
  carb_monitors_no_geom, 
  cal_epa_monitors_no_geom, 
  by = "aqs_id")

epa_but_not_carb_monitors <- 
  anti_join(
  cal_epa_monitors_no_geom, 
  carb_monitors_no_geom, 
  by = "aqs_id"
)

# simplify the epa_monitors
epa_pm25_sf_to_join <- 
  cal_epa_monitors_sf %>% 
  select(aqs_id, site = monitor_id_poc, latitude, longitude) %>% 
  mutate(source = "epa")
  
# reorder the carb monitors
carb_pm25_sf_to_join <- 
  carb_pm25_sf %>% 
  select(aqs_id, site, latitude, longitude) %>% 
  mutate(source = "carb", 
         site = as.character(site))

# make sf of all monitors at once
all_monitors_sf <- 
    do.call(rbind, 
            list(carb_pm25_sf_to_join, 
                 epa_pm25_sf_to_join
                 )
    )

# drop duplicates
all_monitors_no_duplicates_sf <- 
  arrange(all_monitors_sf,  # sort by source putting epa first
          source, aqs_id) %>%  # then by aqs_id
  distinct(aqs_id, .keep_all = TRUE)  # remove dups by aqs_id, keep other vars

# extract values of air data at monitors
all_monitors_no_duplicates_sf$modeled_air_at_monitor <- 
  raster::extract(
    air_cropped, 
    all_monitors_no_duplicates_sf
  )

# # plot monitors on top of air pollution levels
# plot(air_cropped)
# plot(all_monitors_no_duplicates_sf, pch=16, cex=0.5, col="red", add=TRUE)
# plot(all_monitors_no_duplicates_sf["modeled_air_at_monitor"], pch=16, cex=1)

# save carb not epa monitors
save(
  carb_but_not_epa_monitors, 
  file = 
    file.path(
      "intermediate_data", 
      "carb_but_not_epa_monitors.rdata"
    )
)

# save epa not carb monitors
save(
  carb_but_not_epa_monitors, 
  file = 
    file.path(
      "intermediate_data", 
      "epa_but_not_carb_monitors.rdata"
    )
)

# save monitors sans duplicates
save(all_monitors_no_duplicates_sf,
     file = 
       file.path(
         "intermediate_data", 
         "all_monitors_no_duplicates_sf.rdata"
       )
     )



# remove all objects from the environment
rm(list = ls(all.names = TRUE))

```


```{r extract air values at schools}

# load monitors
load(
  file = 
    file.path(
      "intermediate_data", 
      "all_monitors_no_duplicates_sf.rdata"
    )
)

# load pm2.5 estimates
load(
  file = 
    file.path(
      "intermediate_data", 
      "air_cropped.rdata"
    )
)

# # load schools
# load(
#   file = 
#     file.path(
#       "intermediate_data", 
#       "school_lat_long_2016_17_sf.rdata")
#     )
# 
# # rename school thing
# school_air_2016_17_sf <- 
#   school_lat_long_2016_17_sf
# 
# # extract values of air data at schools
# school_air_2016_17_sf$modeled_air_at_school <- 
#   raster::extract(
#     air_cropped,
#     school_air_2016_17_sf
#   )
# 
# # save new sf with air at school
# save(
#   school_air_2016_17_sf, 
#   file = 
#        file.path(
#          "intermediate_data", 
#          "school_air_2016_17_sf.rdata"
#        )
#      )

# # make plot
# plot(school_air_2016_17_sf["modeled_air_at_school"], pch=16, cex=0.4)

# # plot all 3!
# plot(air_cropped)
# plot(school_air_2016_17_sf, pch=16, cex=0.2, alpha = 0.8, col="black", add=TRUE)
# plot(all_monitors_no_duplicates_sf, pch=16, cex=0.2, col="red", add=TRUE)

# remove all objects from the environment
rm(list = ls(all.names = TRUE))

```
